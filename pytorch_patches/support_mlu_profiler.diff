diff --git a/torch/_C/_autograd.pyi b/torch/_C/_autograd.pyi
index 4b62950fe85..4245f12d594 100644
--- a/torch/_C/_autograd.pyi
+++ b/torch/_C/_autograd.pyi
@@ -53,7 +53,9 @@ class _KinetoEvent:
     def name(self) -> str: ...
     def device_index(self) -> int: ...
     def start_us(self) -> int: ...
+    def start_ns(self) -> int: ...
     def duration_us(self) -> int: ...
+    def duration_ns(self) -> int: ...
     def is_async(self) -> bool: ...
     def linked_correlation_id(self) -> int: ...
     def shapes(self) -> List[List[int]]: ...
@@ -70,6 +72,7 @@ class _KinetoEvent:
     def flops(self) -> int: ...
     def cuda_elapsed_us(self) -> int: ...
     def privateuse1_elapsed_us(self) -> int: ...
+    def is_user_annotation(self) -> bool: ...
 
 class _ProfilerResult:
     def events(self) -> List[_KinetoEvent]: ...
@@ -77,6 +80,7 @@ class _ProfilerResult:
     def save(self, path: str) -> None: ...
     def experimental_event_tree(self) -> List[_ProfilerEvent]: ...
     def trace_start_us(self) -> int: ...
+    def trace_start_ns(self) -> int: ...
 
 class SavedTensor: ...
 
diff --git a/torch/autograd/profiler.py b/torch/autograd/profiler.py
index e2e3d36e33b..b9d46f5e660 100644
--- a/torch/autograd/profiler.py
+++ b/torch/autograd/profiler.py
@@ -188,6 +188,7 @@ class profile:
         use_cpu=True,
         use_mtia=False,
         experimental_config=None,
+        use_mlu=False,
     ):
         self.enabled: bool = enabled
         if not self.enabled:
@@ -206,6 +207,7 @@ class profile:
         self.with_modules = with_modules
         self.use_cpu = use_cpu
         self.use_mtia = use_mtia
+        self.use_mlu = use_mlu
         if experimental_config is None:
             experimental_config = _ExperimentalConfig()
         self.experimental_config = experimental_config
@@ -228,6 +230,10 @@ class profile:
             warn("CUDA is not available, disabling CUDA profiling")
             self.use_cuda = False
 
+        if self.use_mlu and not torch.is_mlu_available():
+            warn("MLU is not available, disabling MLU profiling")
+            self.use_mlu = False
+
         self.kineto_activities = set()
         if self.use_cpu:
             self.kineto_activities.add(ProfilerActivity.CPU)
@@ -241,6 +247,8 @@ class profile:
                 self.profiler_kind = ProfilerState.KINETO_GPU_FALLBACK
             else:
                 self.kineto_activities.add(ProfilerActivity.CUDA)
+        if self.use_mlu:
+            self.kineto_activities.add(ProfilerActivity.MLU)
 
         if self.use_device:
             if (
@@ -281,6 +289,9 @@ class profile:
 
     def _prepare_trace(self):
         self.entered = True
+        if self.use_mlu:
+            from torch_mlu.profiler.profile_mlu import _enable_mlu_profiler
+            _enable_mlu_profiler()
         _prepare_profiler(self.config(), self.kineto_activities)
 
     def _start_trace(self):
@@ -293,12 +304,14 @@ class profile:
             return
         if self.use_cuda:
             torch.cuda.synchronize()
+        if self.use_mlu:
+            torch.mlu.synchronize()
         self.kineto_results = _disable_profiler()
         _run_on_profiler_stop()
         parsed_results = self._parse_kineto_results(self.kineto_results)
         self.function_events = EventList(
             parsed_results,
-            use_cuda=self.use_cuda,
+            use_cuda=self.use_cuda or self.use_mlu,
             use_device=self.use_device,
             profile_memory=self.profile_memory,
             with_flops=self.with_flops,
@@ -386,7 +399,7 @@ class profile:
     def _parse_kineto_results(self, result: _ProfilerResult):
         # result.events() has most of the events - PyTorch op-level and device-level events
 
-        trace_start_us = result.trace_start_us()
+        trace_start_ns = result.trace_start_ns()
         mem_records = [
             [evt, False] for evt in result.events() if evt.name() == MEMORY_EVENT_NAME
         ]
@@ -417,6 +430,13 @@ class profile:
                 else 0
             )
 
+        def _mlu_memory_usage(mem_record):
+            return (
+                mem_record.nbytes() 
+                if mem_record.device_type() in [DeviceType.PrivateUse1]
+                else 0
+            )
+
         # Create and return FunctionEvent list
         function_events = []
         device_corr_map: Dict[int, List[FunctionEvent]] = {}
@@ -424,20 +444,22 @@ class profile:
         for kineto_event in result.events():
             if _filter_name(kineto_event.name()):
                 continue
-            rel_start_us = kineto_event.start_us() - trace_start_us
-            rel_end_us = rel_start_us + kineto_event.duration_us()
-            abs_end_us = kineto_event.start_us() + kineto_event.duration_us()
+            rel_start_ns = kineto_event.start_ns() - trace_start_ns
+            rel_end_ns = rel_start_ns + kineto_event.duration_ns()
+            abs_end_ns = kineto_event.start_ns() + kineto_event.duration_ns()
 
             cpu_memory_usage = 0
             cuda_memory_usage = 0
+            mlu_memory_usage = 0
             privateuse1_memory_usage = 0
             if kineto_event.device_type() == DeviceType.CPU:
                 # find the corresponding memory allocation events
                 for mem_record in mem_records_acc.in_interval(
-                    kineto_event.start_us(), abs_end_us
+                    kineto_event.start_ns() / 1000, abs_end_ns / 1000
                 ):
                     cpu_memory_usage += _cpu_memory_usage(mem_record[0])
                     cuda_memory_usage += _cuda_memory_usage(mem_record[0])
+                    mlu_memory_usage += _mlu_memory_usage(mem_record[0])
                     privateuse1_memory_usage += _privateuse1_memory_usage(mem_record[0])
                     mem_record[1] = True
 
@@ -450,8 +472,8 @@ class profile:
                 name=_rewrite_name(name=kineto_event.name(), with_wildcard=True),
                 trace_name=_rewrite_name(name=kineto_event.name(), with_wildcard=False),
                 thread=kineto_event.start_thread_id(),
-                start_us=rel_start_us,
-                end_us=rel_end_us,
+                start_us=rel_start_ns / 1000,
+                end_us=rel_end_ns / 1000,
                 fwd_thread=kineto_event.fwd_thread_id(),
                 input_shapes=kineto_event.shapes(),
                 concrete_inputs=kineto_event.concrete_inputs(),
@@ -463,13 +485,14 @@ class profile:
                 scope=kineto_event.scope(),
                 use_device=self.use_device,
                 cpu_memory_usage=cpu_memory_usage,
-                cuda_memory_usage=cuda_memory_usage,
+                cuda_memory_usage=mlu_memory_usage if self.use_mlu else cuda_memory_usage,
                 privateuse1_memory_usage=privateuse1_memory_usage,
                 is_async=is_async,
                 sequence_nr=kineto_event.sequence_nr(),
                 device_type=kineto_event.device_type(),
                 device_index=kineto_event.device_index(),
                 flops=kineto_event.flops(),
+                is_user_annotation=kineto_event.is_user_annotation(),
             )
             max_evt_id = max(max_evt_id, fe.id)
             if fe.device_type == DeviceType.CPU and not fe.is_async:
@@ -490,16 +513,18 @@ class profile:
                 if corr_id not in device_corr_map:
                     device_corr_map[corr_id] = []
                 device_corr_map[corr_id].append(fe)
+                fe.linked = True
 
         # associate CUDA kernels and CUDA runtime (CPU) with CPU events
         for fe in function_events:
             if (
                 fe.device_type == DeviceType.CPU
                 and not fe.is_async
+                and not fe.linked
                 and fe.id in device_corr_map
             ):
                 for f_evt in device_corr_map[fe.id]:
-                    if f_evt.device_type == DeviceType.CUDA:
+                    if f_evt.device_type == DeviceType.CUDA or f_evt.device_type == DeviceType.PrivateUse1:
                         fe.append_kernel(
                             f_evt.name,
                             f_evt.device_index,
@@ -512,21 +537,22 @@ class profile:
                         f_evt.thread = fe.thread
 
         def createFunctionEventForMemoryEvents(evt):
-            rel_start_us = evt.start_us() - trace_start_us
+            rel_start_ns = evt.start_ns() - trace_start_ns
             fe = FunctionEvent(
                 id=max_evt_id,
                 name=evt.name(),
                 trace_name=None,  # not outputting in the trace
                 thread=evt.start_thread_id(),
-                start_us=rel_start_us,
-                end_us=rel_start_us,  # no duration
+                start_us=rel_start_ns / 1000,
+                end_us=rel_start_ns / 1000,  # no duration
                 fwd_thread=evt.start_thread_id(),
                 input_shapes=[],
                 stack=[],
                 scope=0,  # RecordScope::FUNCTION
                 use_device=self.use_device,
                 cpu_memory_usage=_cpu_memory_usage(evt),
-                cuda_memory_usage=_cuda_memory_usage(evt),
+                cuda_memory_usage=_mlu_memory_usage(evt) if self.use_mlu \
+                                   else _cuda_memory_usage(evt),
                 privateuse1_memory_usage=_privateuse1_memory_usage(evt),
                 is_async=False,
                 sequence_nr=-1,
diff --git a/torch/autograd/profiler_util.py b/torch/autograd/profiler_util.py
index 0d7388b1316..bdf991fca91 100644
--- a/torch/autograd/profiler_util.py
+++ b/torch/autograd/profiler_util.py
@@ -313,6 +313,7 @@ class EventList(list):
                 str(event.node_id),
                 str(event.device_type),
                 str(event.is_legacy),
+                str(event.is_user_annotation),
             ]
             if group_by_input_shapes:
                 key.append(str(event.input_shapes))
@@ -460,6 +461,7 @@ class FunctionEvent(FormattedTimesMixin):
         flops=None,
         trace_name=None,
         concrete_inputs=None,
+        is_user_annotation=False,
     ):
         self.id: int = id
         self.node_id: int = node_id
@@ -487,6 +489,8 @@ class FunctionEvent(FormattedTimesMixin):
         self.device_index: int = device_index
         self.is_legacy: bool = is_legacy
         self.flops: Optional[int] = flops
+        self.linked: bool = False
+        self.is_user_annotation: bool = is_user_annotation
 
     def append_kernel(self, name, device, duration):
         assert self.device_type == DeviceType.CPU
@@ -704,6 +708,7 @@ class FunctionEventAvg(FormattedTimesMixin):
             self.device_type = other.device_type
             self.is_legacy = other.is_legacy
             self.use_device = other.use_device
+            self.is_user_annotation = other.is_user_annotation
 
         assert isinstance(other, (FunctionEvent, FunctionEventAvg))
         assert other.key == self.key
@@ -777,18 +782,18 @@ class MemRecordsAcc:
 
     def __init__(self, mem_records):
         self._mem_records = mem_records
-        self._start_uses: List[int] = []
+        self._start_nses: List[int] = []
         self._indices: List[int] = []
         if len(mem_records) > 0:
-            tmp = sorted([(r[0].start_us(), i) for i, r in enumerate(mem_records)])
-            self._start_uses, self._indices = zip(*tmp)  # type: ignore[assignment]
+            tmp = sorted([(r[0].start_ns(), i) for i, r in enumerate(mem_records)])
+            self._start_nses, self._indices = zip(*tmp)  # type: ignore[assignment]
 
     def in_interval(self, start_us, end_us):
         r"""
         Return all records in the given interval
         """
-        start_idx = bisect.bisect_left(self._start_uses, start_us)
-        end_idx = bisect.bisect_right(self._start_uses, end_us)
+        start_idx = bisect.bisect_left(self._start_nses, start_us * 1000)
+        end_idx = bisect.bisect_right(self._start_nses, end_us * 1000)
         for i in range(start_idx, end_idx):
             yield self._mem_records[self._indices[i]]
 
@@ -1036,10 +1041,10 @@ def _build_table(
                     sum_self_cuda_time_total += evt.self_cuda_time_total
                 else:
                     sum_self_privateuse1_time_total += evt.self_privateuse1_time_total
-        elif evt.device_type == DeviceType.CUDA:
+        elif evt.device_type == DeviceType.CUDA and not evt.is_user_annotation:
             # in kineto profiler, there're events with the correct device type (e.g. CUDA)
             sum_self_cuda_time_total += evt.self_cuda_time_total
-        elif evt.device_type == DeviceType.PrivateUse1:
+        elif evt.device_type == DeviceType.PrivateUse1 and not evt.is_user_annotation:
             sum_self_privateuse1_time_total += evt.self_privateuse1_time_total
 
     # Actual printing
diff --git a/torch/csrc/autograd/init.cpp b/torch/csrc/autograd/init.cpp
index 2e5e5aa40db..a0369dda360 100644
--- a/torch/csrc/autograd/init.cpp
+++ b/torch/csrc/autograd/init.cpp
@@ -202,9 +202,13 @@ PyObject* THPAutograd_initExtension(PyObject* _unused, PyObject* unused) {
       // the forward op
       .def("sequence_nr", [](const KinetoEvent& e) { return e.sequenceNr(); })
       // absolute start time (since unix epoch) in us
-      .def("start_us", [](const KinetoEvent& e) { return e.startUs(); })
+      .def("start_us", [](const KinetoEvent& e) { return e.startNs() / 1000; })
+      // absolute start time (since unix epoch) in ns
+      .def("start_ns", [](const KinetoEvent& e) { return e.startNs(); })
       // duration in us
-      .def("duration_us", [](const KinetoEvent& e) { return e.durationUs(); })
+      .def("duration_us", [](const KinetoEvent& e) { return e.durationNs() / 1000; })
+      // duration in ns
+      .def("duration_ns", [](const KinetoEvent& e) { return e.durationNs(); })
       // used for correlation between high-level PyTorch events
       // and low-level device events
       .def(
@@ -249,6 +253,16 @@ PyObject* THPAutograd_initExtension(PyObject* _unused, PyObject* unused) {
       .def("is_async", [](const KinetoEvent& e) { return e.isAsync(); })
       .def("cuda_elapsed_us", &KinetoEvent::cudaElapsedUs)
       .def("privateuse1_elapsed_us", &KinetoEvent::privateuse1ElapsedUs)
+      .def(
+          "is_user_annotation",
+          [](const KinetoEvent& e) {
+            return e.activityType() ==
+                (uint8_t)libkineto::ActivityType::USER_ANNOTATION ||
+                e.activityType() ==
+                (uint8_t)libkineto::ActivityType::GPU_USER_ANNOTATION ||
+                e.activityType() ==
+                (uint8_t)libkineto::ActivityType::MLU_USER_ANNOTATION;
+          })
       .def("nbytes", [](const KinetoEvent& e) { return e.nBytes(); });
 
   m.def("_soft_assert_raises", &setSoftAssertRaises);
@@ -256,6 +270,7 @@ PyObject* THPAutograd_initExtension(PyObject* _unused, PyObject* unused) {
 
   py::class_<ProfilerResult>(m, "_ProfilerResult")
       .def("trace_start_us", &ProfilerResult::trace_start_us)
+      .def("trace_start_ns", &ProfilerResult::trace_start_ns)
       .def("events", &ProfilerResult::events)
       .def("experimental_event_tree", &ProfilerResult::event_tree)
 #ifdef USE_KINETO
diff --git a/torch/csrc/autograd/profiler_kineto.cpp b/torch/csrc/autograd/profiler_kineto.cpp
index 350db179427..99fa6e52334 100644
--- a/torch/csrc/autograd/profiler_kineto.cpp
+++ b/torch/csrc/autograd/profiler_kineto.cpp
@@ -50,11 +51,11 @@ namespace autograd {
 namespace profiler {
 
 namespace {
-inline int64_t getTimeUs() {
+inline int64_t getTimeNs() {
 #ifdef USE_KINETO
   return libkineto::timeSinceEpoch(std::chrono::system_clock::now());
 #else
-  return c10::getTime() / 1000;
+  return c10::getTime();
 #endif // USE_KINETO
 }
 
@@ -305,7 +306,7 @@ struct KinetoThreadLocalState : public ProfilerStateBase {
       const ProfilerConfig& config,
       std::set<torch::profiler::impl::ActivityType> activities)
       : ProfilerStateBase(config),
-        start_time_(getTimeUs()),
+        start_time_(getTimeNs()),
         record_queue_(config, std::move(activities)) {}
   ~KinetoThreadLocalState() override = default;
 
@@ -372,7 +373,7 @@ struct KinetoThreadLocalState : public ProfilerStateBase {
 
   std::unique_ptr<torch::profiler::impl::kineto::ActivityTraceWrapper>
   finalizeTrace() {
-    auto end_time = getTimeUs();
+    auto end_time = getTimeNs();
     record_queue_.stop();
 
     std::lock_guard<std::mutex> guard(state_mutex_);
@@ -770,8 +777,8 @@ const c10::ArrayRef<std::string> KinetoEvent::moduleHierarchy() const {
   return {};
 }
 
-uint64_t KinetoEvent::durationUs() const {
-  return (result_->endTimeNS() - result_->start_time_ns_) / 1000;
+uint64_t KinetoEvent::durationNs() const {
+  return result_->endTimeNS() - result_->start_time_ns_;
 }
 
 int64_t KinetoEvent::debugHandle() const {
@@ -852,7 +859,7 @@ FORWARD_FROM_RESULT(endThreadId, endTID())
 FORWARD_FROM_RESULT(activityType, kinetoType())
 FORWARD_FROM_RESULT(name, name())
 FORWARD_FROM_RESULT(deviceType, deviceType())
-FORWARD_FROM_RESULT(startUs, start_time_ns_ / 1000)
+FORWARD_FROM_RESULT(startNs, start_time_ns_)
 FORWARD_FROM_RESULT(correlationId, correlationID())
 FORWARD_FROM_RESULT(deviceResourceId, kineto_info_.resource)
 #undef FORWARD_FROM_RESULT
@@ -904,7 +911,7 @@ ProfilerResult::ProfilerResult(
     std::unique_ptr<torch::profiler::impl::kineto::ActivityTraceWrapper>&&
         trace,
     std::vector<experimental_event_t>&& event_tree)
-    : trace_start_us_(start_time),
+    : trace_start_ns_(start_time),
       events_(std::move(events)),
       trace_(std::move(trace)),
       event_tree_(std::move(event_tree)) {}
diff --git a/torch/csrc/autograd/profiler_kineto.h b/torch/csrc/autograd/profiler_kineto.h
index 6ea7cf63d6a..5f2d661c22e 100644
--- a/torch/csrc/autograd/profiler_kineto.h
+++ b/torch/csrc/autograd/profiler_kineto.h
@@ -48,8 +48,8 @@ struct TORCH_API KinetoEvent {
   c10::DeviceType deviceType() const;
   int deviceIndex() const;
   int64_t nBytes() const;
-  uint64_t startUs() const;
-  uint64_t durationUs() const;
+  uint64_t startNs() const;
+  uint64_t durationNs() const;
   bool isAsync() const;
   uint64_t correlationId() const;
   uint64_t linkedCorrelationId() const;
@@ -88,7 +88,11 @@ struct TORCH_API ProfilerResult {
   ~ProfilerResult();
 
   uint64_t trace_start_us() const {
-    return trace_start_us_;
+    return trace_start_ns_ / 1000;
+  }
+
+  uint64_t trace_start_ns() const {
+    return trace_start_ns_;
   }
 
   const std::vector<KinetoEvent>& events() const {
@@ -102,7 +106,7 @@ struct TORCH_API ProfilerResult {
   void save(const std::string& path);
 
  private:
-  uint64_t trace_start_us_ = 0;
+  uint64_t trace_start_ns_ = 0;
   std::vector<KinetoEvent> events_;
   std::unique_ptr<torch::profiler::impl::kineto::ActivityTraceWrapper> trace_;
   std::vector<experimental_event_t> event_tree_;
diff --git a/torch/csrc/profiler/collection.cpp b/torch/csrc/profiler/collection.cpp
index 6df5e0d6fcd..7915d6acee4 100644
--- a/torch/csrc/profiler/collection.cpp
+++ b/torch/csrc/profiler/collection.cpp
@@ -591,7 +591,7 @@ int64_t Result::endTimeNS() const {
           Vulkan, start_time_ns_ + (e.in_tree_building_ ? 0 : e.duration_ns_)),
       ATTRIBUTE(Allocation, start_time_ns_),
       ATTRIBUTE(OutOfMemory, start_time_ns_),
-      ATTRIBUTE(Kineto, start_time_ns_ + e.duration_us_ * 1000),
+      ATTRIBUTE(Kineto, start_time_ns_ + e.duration_ns_),
       [&](const auto& e) -> int64_t { return e.end_time_ns_; }));
 
   // In rare cases we're willing to tolerate ops which are missing an end time
@@ -802,12 +802,12 @@ static constexpr const char* indexKey = "Ev Idx";
 
 void passEventsToKineto(
     const std::vector<std::shared_ptr<Result>>& results,
-    uint64_t start_time_us,
-    uint64_t end_time_us,
+    uint64_t start_time_ns,
+    uint64_t end_time_ns,
     const ProfilerConfig& config) {
   using namespace torch::profiler::impl::kineto;
   TraceWrapper cpu_trace(
-      static_cast<int64_t>(start_time_us), "PyTorch Profiler");
+      static_cast<int64_t>(start_time_ns), "PyTorch Profiler");
 
   // Generate Kineto events for each event recorded by the PyTorch profiler.
   for (const auto i : c10::irange(results.size())) {
@@ -817,8 +817,8 @@ void passEventsToKineto(
         e->kinetoType(),
         e->kineto_info_,
         e->correlationID(),
-        e->start_time_ns_ / 1000,
-        e->endTimeNS() / 1000);
+        e->start_time_ns_,
+        e->endTimeNS());
 
     TORCH_INTERNAL_ASSERT(activity || !kKinetoAvailable);
     if (activity) {
@@ -841,7 +841,7 @@ void passEventsToKineto(
   }
 
   // Kineto adds the events that it collected.
-  cpu_trace.transferCpuTrace(static_cast<int64_t>(end_time_us));
+  cpu_trace.transferCpuTrace(static_cast<int64_t>(end_time_ns));
 }
 
 #ifdef USE_KINETO
@@ -950,7 +950,7 @@ class TransferEvents {
         static_cast<int32_t>(activity->resourceId())};
 
     auto event = Result::create(
-        activity->timestamp() * 1000,
+        activity->timestamp(),
         noTID, // Placeholder
         device_and_resource,
         ExtraFields<EventType::Kineto>{
@@ -1097,11 +1097,11 @@ class TransferEvents {
 
 trace_ptr_t addKinetoEvents(
     std::vector<std::shared_ptr<Result>>& results,
-    uint64_t start_time_us,
-    uint64_t end_time_us,
+    uint64_t start_time_ns,
+    uint64_t end_time_ns,
     const ProfilerConfig& config) {
   using namespace torch::profiler::impl::kineto;
-  passEventsToKineto(results, start_time_us, end_time_us, config);
+  passEventsToKineto(results, start_time_ns, end_time_ns, config);
 
   // In on demand mode kineto is directly controlled by other machinery.
   if (config.global()) {
@@ -1352,8 +1352,8 @@ std::pair<
     std::unique_ptr<torch::profiler::impl::kineto::ActivityTraceWrapper>>
 RecordQueue::getRecords(
     std::function<c10::time_t(c10::approx_time_t)> time_converter,
-    uint64_t start_time_us,
-    uint64_t end_time_us) {
+    uint64_t start_time_ns,
+    uint64_t end_time_ns) {
   auto converter = [&](c10::approx_time_t t) {
     return t == std::numeric_limits<c10::approx_time_t>::min()
         ? std::numeric_limits<c10::time_t>::min()
@@ -1406,7 +1406,7 @@ RecordQueue::getRecords(
     for (const auto& i : python_tracer_->getEvents(
              converter,
              python_enters,
-             static_cast<c10::time_t>(end_time_us * 1000))) {
+             static_cast<c10::time_t>(end_time_ns))) {
       out.push_back(i);
     }
     python_tracer_.reset();
@@ -1426,7 +1426,7 @@ RecordQueue::getRecords(
     }
   }
 
-  auto trace = addKinetoEvents(out, start_time_us, end_time_us, config_);
+  auto trace = addKinetoEvents(out, start_time_ns, end_time_ns, config_);
 
   std::stable_sort(out.begin(), out.end(), [](const auto& a, const auto& b) {
     return a->start_time_ns_ < b->start_time_ns_;
diff --git a/torch/csrc/profiler/collection.h b/torch/csrc/profiler/collection.h
index 3678e04bfbd..c3de621f2c0 100644
--- a/torch/csrc/profiler/collection.h
+++ b/torch/csrc/profiler/collection.h
@@ -339,7 +339,7 @@ struct ExtraFields<EventType::Kineto> {
   };
 
   std::string name_;
-  int64_t duration_us_{0};
+  int64_t duration_ns_{0};
   uint64_t correlation_id_{0};
   libkineto::ActivityType activity_type_;
   Flow flow;
@@ -631,8 +631,8 @@ class TORCH_API RecordQueue {
       std::unique_ptr<torch::profiler::impl::kineto::ActivityTraceWrapper>>
   getRecords(
       std::function<c10::time_t(c10::approx_time_t)> time_converter,
-      uint64_t start_time_us,
-      uint64_t end_time_us);
+      uint64_t start_time_ns,
+      uint64_t end_time_ns);
 
  private:
   uint32_t id_;
diff --git a/torch/csrc/profiler/kineto_shim.cpp b/torch/csrc/profiler/kineto_shim.cpp
index 56b089592b1..07036f6e4c5 100644
--- a/torch/csrc/profiler/kineto_shim.cpp
+++ b/torch/csrc/profiler/kineto_shim.cpp
@@ -327,10 +327,17 @@ c10::DeviceType deviceTypeFromActivity(libkineto::ActivityType activity_type) {
     // TODO: T151322015
     case libkineto::ActivityType::MTIA_CCP_EVENTS:
       return c10::DeviceType::CUDA;
+    case libkineto::ActivityType::MLU_MEMCPY:
+    case libkineto::ActivityType::MLU_MEMSET:
+    case libkineto::ActivityType::MLU_CONCURRENT_KERNEL:
+    case libkineto::ActivityType::MLU_USER_ANNOTATION:
+    case libkineto::ActivityType::MLU_PROFILER_RANGE:
+      return c10::DeviceType::PrivateUse1;
     case libkineto::ActivityType::CPU_OP:
     case libkineto::ActivityType::USER_ANNOTATION:
     case libkineto::ActivityType::EXTERNAL_CORRELATION:
     case libkineto::ActivityType::CUDA_RUNTIME:
+    case libkineto::ActivityType::MLU_RUNTIME:
     case libkineto::ActivityType::CPU_INSTANT_EVENT:
     case libkineto::ActivityType::GLOW_RUNTIME:
     case libkineto::ActivityType::MTIA_RUNTIME:
diff --git a/torch/csrc/profiler/orchestration/observer.h b/torch/csrc/profiler/orchestration/observer.h
index da675e0f3da..2faea23328b 100644
--- a/torch/csrc/profiler/orchestration/observer.h
+++ b/torch/csrc/profiler/orchestration/observer.h
@@ -16,6 +16,7 @@ enum class C10_API_ENUM ActivityType {
   CPU = 0,
   XPU, // XPU kernels, runtime
   CUDA, // CUDA kernels, runtime
+  MLU, // MLU kernels, runtime
   MTIA, // MTIA kernels, runtime
   NUM_KINETO_ACTIVITIES, // must be the last one
 };
diff --git a/torch/csrc/profiler/python/init.cpp b/torch/csrc/profiler/python/init.cpp
index e6254c323a6..6b8dc8e0781 100644
--- a/torch/csrc/profiler/python/init.cpp
+++ b/torch/csrc/profiler/python/init.cpp
@@ -260,7 +261,8 @@ void initPythonBindings(PyObject* module) {
       .value("CPU", ActivityType::CPU)
       .value("XPU", ActivityType::XPU)
       .value("MTIA", ActivityType::MTIA)
-      .value("CUDA", ActivityType::CUDA);
+      .value("CUDA", ActivityType::CUDA)
+      .value("MLU", ActivityType::MLU);
 
   py::class_<ExperimentalConfig>(m, "_ExperimentalConfig")
       .def(
diff --git a/torch/profiler/_utils.py b/torch/profiler/_utils.py
index 783a69ea89a..07bfada25a2 100644
--- a/torch/profiler/_utils.py
+++ b/torch/profiler/_utils.py
@@ -147,15 +147,15 @@ class BasicEvaluation:
 
         cuda_launch_events = sorted(
             (e for e in cuda_event_list if is_cuda_launch_kernel(e)),
-            key=lambda x: x.start_us(),
+            key=lambda x: x.start_ns(),
         )
         cuda_kernel_events = sorted(
             (e for e in cuda_event_list if is_cuda_kernel(e)),
-            key=lambda x: x.start_us(),
+            key=lambda x: x.start_ns(),
         )
 
         self.cuda_events = sorted(
-            cuda_launch_events + cuda_kernel_events, key=lambda x: x.start_us()
+            cuda_launch_events + cuda_kernel_events, key=lambda x: x.start_ns()
         )
 
         kernel_mapping: Dict[_KinetoEvent, int] = {}
@@ -178,6 +178,8 @@ class BasicEvaluation:
         def new_old_event_comparator(event):
             if hasattr(event, "start_us"):
                 return event.start_us() * 1000
+            if hasattr(event, "start_ns"):
+                return event.start_ns()
             if hasattr(event, "start_time_ns"):
                 return event.start_time_ns
             raise Exception("Unknown Event Type")
@@ -192,20 +194,26 @@ class BasicEvaluation:
                 # Find current spawned cuda kernel event
                 if event in kernel_mapping and kernel_mapping[event] is not None:
                     spawned_kernel_index = kernel_mapping[event]
+            elif hasattr(event, "start_ns"):
+                start_time = event.start_ns()
+                end_time = event.start_ns() + event.duration_ns()
+                # Find current spawned cuda kernel event
+                if event in kernel_mapping and kernel_mapping[event] is not None:
+                    spawned_kernel_index = kernel_mapping[event]
             elif hasattr(event, "start_time_ns"):
                 start_time = event.start_time_ns  # type: ignore[attr-defined]
                 end_time = event.end_time_ns  # type: ignore[attr-defined]
 
             while (
                 current_kernel_index < len(cuda_kernel_events)
-                and (cuda_kernel_events[current_kernel_index].start_us()) * 1000
+                and (cuda_kernel_events[current_kernel_index].start_ns())
                 <= start_time  # type: ignore[possibly-undefined]
             ):
                 current_kernel_index += 1
             current_queue_depth = spawned_kernel_index - current_kernel_index + 1
             current_queue_depth = max(current_queue_depth, 0)
 
-            if hasattr(event, "start_us"):
+            if hasattr(event, "start_us") or hasattr(event, "start_ns"):
                 queue_depth_list.append(
                     Interval(start_time, end_time, current_queue_depth)  # type: ignore[possibly-undefined]
                 )
diff --git a/torch/profiler/profiler.py b/torch/profiler/profiler.py
index b1b611b4ec2..a743ad12fb9 100644
--- a/torch/profiler/profiler.py
+++ b/torch/profiler/profiler.py
@@ -151,6 +151,7 @@ class _KinetoProfile:
             with_modules=self.with_modules,
             use_kineto=True,
             experimental_config=self.experimental_config,
+            use_mlu=(ProfilerActivity.MLU in self.activities),
         )
         self.profiler._prepare_trace()
 
@@ -593,6 +594,7 @@ class profile(_KinetoProfile):
         execution_trace_observer: Optional[_ITraceObserver] = None,
         # deprecated:
         use_cuda: Optional[bool] = None,
+        use_mlu: Optional[bool] = None,
     ):
         activities_set = set(activities) if activities else supported_activities()
         if use_cuda is not None:
@@ -603,6 +605,14 @@ class profile(_KinetoProfile):
                 activities_set.remove(ProfilerActivity.CUDA)
         assert len(activities_set) > 0, "No valid profiler activities found"
 
+        if use_mlu is not None:
+            warn("use_mlu is deprecated, use activities argument instead")
+            if use_mlu:
+                activities_set.add(ProfilerActivity.MLU)
+            elif ProfilerActivity.MLU in activities_set:
+                activities_set.remove(ProfilerActivity.MLU)
+        assert len(activities_set) > 0, "No valid profiler activities found"
+
         super().__init__(
             activities=activities,
             record_shapes=record_shapes,
