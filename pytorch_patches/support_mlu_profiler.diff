diff --git a/torch/_C/_autograd.pyi b/torch/_C/_autograd.pyi
index 0a92d31b323..8bd2d4a0821 100644
--- a/torch/_C/_autograd.pyi
+++ b/torch/_C/_autograd.pyi
@@ -53,7 +53,9 @@ class _KinetoEvent:
     def name(self) -> str: ...
     def device_index(self) -> int: ...
     def start_us(self) -> int: ...
+    def start_ns(self) -> int: ...
     def duration_us(self) -> int: ...
+    def duration_ns(self) -> int: ...
     def is_async(self) -> bool: ...
     def linked_correlation_id(self) -> int: ...
     def shapes(self) -> List[List[int]]: ...
@@ -70,6 +72,7 @@ class _KinetoEvent:
     def flops(self) -> int: ...
     def cuda_elapsed_us(self) -> int: ...
     def privateuse1_elapsed_us(self) -> int: ...
+    def is_user_annotation(self) -> bool: ...
 
 class _ProfilerResult:
     def events(self) -> List[_KinetoEvent]: ...
@@ -77,6 +80,7 @@ class _ProfilerResult:
     def save(self, path: str) -> None: ...
     def experimental_event_tree(self) -> List[_ProfilerEvent]: ...
     def trace_start_us(self) -> int: ...
+    def trace_start_ns(self) -> int: ...
 
 class SavedTensor: ...
 
diff --git a/torch/autograd/profiler.py b/torch/autograd/profiler.py
index 650546fac4c..bfc338d0311 100644
--- a/torch/autograd/profiler.py
+++ b/torch/autograd/profiler.py
@@ -215,6 +215,7 @@ class profile:
         use_cpu=True,
         use_mtia=False,
         experimental_config=None,
+        use_mlu=False,
     ):
         self.enabled: bool = enabled
         if not self.enabled:
@@ -233,6 +234,7 @@ class profile:
         self.with_modules = with_modules
         self.use_cpu = use_cpu
         self.use_mtia = use_mtia
+        self.use_mlu = use_mlu
         if experimental_config is None:
             experimental_config = _ExperimentalConfig()
         self.experimental_config = experimental_config
@@ -255,6 +257,10 @@ class profile:
             warn("CUDA is not available, disabling CUDA profiling")
             self.use_cuda = False
 
+        if self.use_mlu and not torch.is_mlu_available():
+            warn("MLU is not available, disabling MLU profiling")
+            self.use_mlu = False
+
         self.kineto_activities = set()
         if self.use_cpu:
             self.kineto_activities.add(ProfilerActivity.CPU)
@@ -268,6 +274,8 @@ class profile:
                 self.profiler_kind = ProfilerState.KINETO_GPU_FALLBACK
             else:
                 self.kineto_activities.add(ProfilerActivity.CUDA)
+        if self.use_mlu:
+            self.kineto_activities.add(ProfilerActivity.MLU)
 
         if self.use_device:
             if (
@@ -308,6 +316,9 @@ class profile:
 
     def _prepare_trace(self):
         self.entered = True
+        if self.use_mlu:
+            from torch_mlu.profiler.profile_mlu import _enable_mlu_profiler
+            _enable_mlu_profiler()
         _prepare_profiler(self.config(), self.kineto_activities)
 
     def _start_trace(self):
@@ -320,12 +331,14 @@ class profile:
             return
         if self.use_cuda:
             torch.cuda.synchronize()
+        if self.use_mlu:
+            torch.mlu.synchronize()
         self.kineto_results = _disable_profiler()
         _run_on_profiler_stop()
         parsed_results = self._parse_kineto_results(self.kineto_results)
         self.function_events = EventList(
             parsed_results,
-            use_cuda=self.use_cuda,
+            use_cuda=self.use_cuda or self.use_mlu,
             use_device=self.use_device,
             profile_memory=self.profile_memory,
             with_flops=self.with_flops,
@@ -412,7 +425,7 @@ class profile:
     def _parse_kineto_results(self, result: _ProfilerResult):
         # result.events() has most of the events - PyTorch op-level and device-level events
 
-        trace_start_us = result.trace_start_us()
+        trace_start_ns = result.trace_start_ns()
         mem_records = [
             [evt, False] for evt in result.events() if evt.name() == MEMORY_EVENT_NAME
         ]
@@ -443,6 +456,13 @@ class profile:
                 else 0
             )
 
+        def _mlu_memory_usage(mem_record):
+            return (
+                mem_record.nbytes() 
+                if mem_record.device_type() in [DeviceType.PrivateUse1]
+                else 0
+            )
+
         # Create and return FunctionEvent list
         function_events = []
         device_corr_map: Dict[int, List[FunctionEvent]] = {}
@@ -450,20 +470,27 @@ class profile:
         for kineto_event in result.events():
             if _filter_name(kineto_event.name()):
                 continue
-            rel_start_us = kineto_event.start_us() - trace_start_us
-            rel_end_us = rel_start_us + kineto_event.duration_us()
-            abs_end_us = kineto_event.start_us() + kineto_event.duration_us()
+            if (
+                kineto_event.device_type() != DeviceType.CPU
+                and kineto_event.is_user_annotation()
+            ):
+                continue
+            rel_start_ns = kineto_event.start_ns() - trace_start_ns
+            rel_end_ns = rel_start_ns + kineto_event.duration_ns()
+            abs_end_ns = kineto_event.start_ns() + kineto_event.duration_ns()
 
             cpu_memory_usage = 0
             cuda_memory_usage = 0
+            mlu_memory_usage = 0
             privateuse1_memory_usage = 0
             if kineto_event.device_type() == DeviceType.CPU:
                 # find the corresponding memory allocation events
                 for mem_record in mem_records_acc.in_interval(
-                    kineto_event.start_us(), abs_end_us
+                    kineto_event.start_ns() / 1000, abs_end_ns / 1000
                 ):
                     cpu_memory_usage += _cpu_memory_usage(mem_record[0])
                     cuda_memory_usage += _cuda_memory_usage(mem_record[0])
+                    mlu_memory_usage += _mlu_memory_usage(mem_record[0])
                     privateuse1_memory_usage += _privateuse1_memory_usage(mem_record[0])
                     mem_record[1] = True
 
@@ -476,8 +503,8 @@ class profile:
                 name=_rewrite_name(name=kineto_event.name(), with_wildcard=True),
                 trace_name=_rewrite_name(name=kineto_event.name(), with_wildcard=False),
                 thread=kineto_event.start_thread_id(),
-                start_us=rel_start_us,
-                end_us=rel_end_us,
+                start_us=rel_start_ns / 1000,
+                end_us=rel_end_ns / 1000,
                 fwd_thread=kineto_event.fwd_thread_id(),
                 input_shapes=kineto_event.shapes(),
                 concrete_inputs=kineto_event.concrete_inputs(),
@@ -489,7 +516,7 @@ class profile:
                 scope=kineto_event.scope(),
                 use_device=self.use_device,
                 cpu_memory_usage=cpu_memory_usage,
-                cuda_memory_usage=cuda_memory_usage,
+                cuda_memory_usage=mlu_memory_usage if self.use_mlu else cuda_memory_usage,
                 privateuse1_memory_usage=privateuse1_memory_usage,
                 is_async=is_async,
                 sequence_nr=kineto_event.sequence_nr(),
@@ -516,16 +543,18 @@ class profile:
                 if corr_id not in device_corr_map:
                     device_corr_map[corr_id] = []
                 device_corr_map[corr_id].append(fe)
+                fe.linked = True
 
         # associate CUDA kernels and CUDA runtime (CPU) with CPU events
         for fe in function_events:
             if (
                 fe.device_type == DeviceType.CPU
                 and not fe.is_async
+                and not fe.linked
                 and fe.id in device_corr_map
             ):
                 for f_evt in device_corr_map[fe.id]:
-                    if f_evt.device_type == DeviceType.CUDA:
+                    if f_evt.device_type == DeviceType.CUDA or f_evt.device_type == DeviceType.PrivateUse1:
                         fe.append_kernel(
                             f_evt.name,
                             f_evt.device_index,
@@ -538,21 +567,22 @@ class profile:
                         f_evt.thread = fe.thread
 
         def createFunctionEventForMemoryEvents(evt):
-            rel_start_us = evt.start_us() - trace_start_us
+            rel_start_ns = evt.start_ns() - trace_start_ns
             fe = FunctionEvent(
                 id=max_evt_id,
                 name=evt.name(),
                 trace_name=None,  # not outputting in the trace
                 thread=evt.start_thread_id(),
-                start_us=rel_start_us,
-                end_us=rel_start_us,  # no duration
+                start_us=rel_start_ns / 1000,
+                end_us=rel_start_ns / 1000,  # no duration
                 fwd_thread=evt.start_thread_id(),
                 input_shapes=[],
                 stack=[],
                 scope=0,  # RecordScope::FUNCTION
                 use_device=self.use_device,
                 cpu_memory_usage=_cpu_memory_usage(evt),
-                cuda_memory_usage=_cuda_memory_usage(evt),
+                cuda_memory_usage=_mlu_memory_usage(evt) if self.use_mlu \
+                                   else _cuda_memory_usage(evt),
                 privateuse1_memory_usage=_privateuse1_memory_usage(evt),
                 is_async=False,
                 sequence_nr=-1,
diff --git a/torch/autograd/profiler_util.py b/torch/autograd/profiler_util.py
index de330f10a4f..07ac6a302bb 100644
--- a/torch/autograd/profiler_util.py
+++ b/torch/autograd/profiler_util.py
@@ -484,6 +484,7 @@ class FunctionEvent(FormattedTimesMixin):
         self.device_index: int = device_index
         self.is_legacy: bool = is_legacy
         self.flops: Optional[int] = flops
+        self.linked: bool = False
 
     def append_kernel(self, name, device, duration):
         assert self.device_type == DeviceType.CPU
@@ -774,15 +775,15 @@ class MemRecordsAcc:
 
     def __init__(self, mem_records):
         self._mem_records = mem_records
-        self._start_uses = []
+        self._start_nses = []
         self._indices = []
         if len(mem_records) > 0:
-            tmp = sorted([(r[0].start_us(), i) for i, r in enumerate(mem_records)])
-            self._start_uses, self._indices = zip(*tmp)
+            tmp = sorted([(r[0].start_ns(), i) for i, r in enumerate(mem_records)])
+            self._start_nses, self._indices = zip(*tmp)
 
     def in_interval(self, start_us, end_us):
-        start_idx = bisect.bisect_left(self._start_uses, start_us)
-        end_idx = bisect.bisect_right(self._start_uses, end_us)
+        start_idx = bisect.bisect_left(self._start_nses, start_us * 1000)
+        end_idx = bisect.bisect_right(self._start_nses, end_us * 1000)
         for i in range(start_idx, end_idx):
             yield self._mem_records[self._indices[i]]
 
diff --git a/torch/csrc/autograd/init.cpp b/torch/csrc/autograd/init.cpp
index 746695eb714..3fb79a113a5 100644
--- a/torch/csrc/autograd/init.cpp
+++ b/torch/csrc/autograd/init.cpp
@@ -193,9 +193,13 @@ PyObject* THPAutograd_initExtension(PyObject* _unused, PyObject* unused) {
       // the forward op
       .def("sequence_nr", [](const KinetoEvent& e) { return e.sequenceNr(); })
       // absolute start time (since unix epoch) in us
-      .def("start_us", [](const KinetoEvent& e) { return e.startUs(); })
+      .def("start_us", [](const KinetoEvent& e) { return e.startNs() / 1000; })
+      // absolute start time (since unix epoch) in ns
+      .def("start_ns", [](const KinetoEvent& e) { return e.startNs(); })
       // duration in us
-      .def("duration_us", [](const KinetoEvent& e) { return e.durationUs(); })
+      .def("duration_us", [](const KinetoEvent& e) { return e.durationNs() / 1000; })
+      // duration in ns
+      .def("duration_ns", [](const KinetoEvent& e) { return e.durationNs(); })
       // used for correlation between high-level PyTorch events
       // and low-level device events
       .def(
@@ -240,6 +244,16 @@ PyObject* THPAutograd_initExtension(PyObject* _unused, PyObject* unused) {
       .def("is_async", [](const KinetoEvent& e) { return e.isAsync(); })
       .def("cuda_elapsed_us", &KinetoEvent::cudaElapsedUs)
       .def("privateuse1_elapsed_us", &KinetoEvent::privateuse1ElapsedUs)
+      .def(
+          "is_user_annotation",
+          [](const KinetoEvent& e) {
+            return e.activityType() ==
+                (uint8_t)libkineto::ActivityType::USER_ANNOTATION ||
+                e.activityType() ==
+                (uint8_t)libkineto::ActivityType::MLU_USER_ANNOTATION ||
+                e.activityType() ==
+                (uint8_t)libkineto::ActivityType::GPU_USER_ANNOTATION;
+          })
       .def("nbytes", [](const KinetoEvent& e) { return e.nBytes(); });
 
   m.def("_soft_assert_raises", &setSoftAssertRaises);
@@ -247,6 +261,7 @@ PyObject* THPAutograd_initExtension(PyObject* _unused, PyObject* unused) {
 
   py::class_<ProfilerResult>(m, "_ProfilerResult")
       .def("trace_start_us", &ProfilerResult::trace_start_us)
+      .def("trace_start_ns", &ProfilerResult::trace_start_ns)
       .def("events", &ProfilerResult::events)
       .def("experimental_event_tree", &ProfilerResult::event_tree)
 #ifdef USE_KINETO
diff --git a/torch/csrc/autograd/profiler_kineto.cpp b/torch/csrc/autograd/profiler_kineto.cpp
index e59abc859f7..68419f1fa4d 100644
--- a/torch/csrc/autograd/profiler_kineto.cpp
+++ b/torch/csrc/autograd/profiler_kineto.cpp
@@ -54,11 +55,11 @@ namespace autograd {
 namespace profiler {
 
 namespace {
-inline int64_t getTimeUs() {
+inline int64_t getTimeNs() {
 #ifdef USE_KINETO
   return libkineto::timeSinceEpoch(std::chrono::system_clock::now());
 #else
-  return torch::profiler::impl::getTime() / 1000;
+  return torch::profiler::impl::getTime();
 #endif // USE_KINETO
 }
 
@@ -280,7 +281,7 @@ struct KinetoThreadLocalState : public ProfilerStateBase {
       const ProfilerConfig& config,
       std::set<torch::profiler::impl::ActivityType> activities)
       : ProfilerStateBase(config),
-        start_time_(getTimeUs()),
+        start_time_(getTimeNs()),
         record_queue_(config, std::move(activities)) {}
   ~KinetoThreadLocalState() override = default;
 
@@ -347,7 +348,7 @@ struct KinetoThreadLocalState : public ProfilerStateBase {
 
   std::unique_ptr<torch::profiler::impl::kineto::ActivityTraceWrapper>
   finalizeTrace() {
-    auto end_time = getTimeUs();
+    auto end_time = getTimeNs();
     record_queue_.stop();
 
     std::lock_guard<std::mutex> guard(state_mutex_);
@@ -741,8 +748,8 @@ const c10::ArrayRef<std::string> KinetoEvent::moduleHierarchy() const {
       [](const auto&) -> const c10::ArrayRef<std::string> { return {}; }));
 }
 
-uint64_t KinetoEvent::durationUs() const {
-  return (result_->endTimeNS() - result_->start_time_ns_) / 1000;
+uint64_t KinetoEvent::durationNs() const {
+  return result_->endTimeNS() - result_->start_time_ns_;
 }
 
 int64_t KinetoEvent::debugHandle() const {
@@ -823,7 +830,7 @@ FORWARD_FROM_RESULT(endThreadId, endTID())
 FORWARD_FROM_RESULT(activityType, kinetoType())
 FORWARD_FROM_RESULT(name, name())
 FORWARD_FROM_RESULT(deviceType, deviceType())
-FORWARD_FROM_RESULT(startUs, start_time_ns_ / 1000)
+FORWARD_FROM_RESULT(startNs, start_time_ns_)
 FORWARD_FROM_RESULT(correlationId, correlationID())
 FORWARD_FROM_RESULT(deviceResourceId, kineto_info_.resource)
 #undef FORWARD_FROM_RESULT
@@ -872,7 +879,7 @@ ProfilerResult::ProfilerResult(
     std::unique_ptr<torch::profiler::impl::kineto::ActivityTraceWrapper>&&
         trace,
     std::vector<experimental_event_t>&& event_tree)
-    : trace_start_us_(start_time),
+    : trace_start_ns_(start_time),
       events_(std::move(events)),
       trace_(std::move(trace)),
       event_tree_(std::move(event_tree)) {}
diff --git a/torch/csrc/autograd/profiler_kineto.h b/torch/csrc/autograd/profiler_kineto.h
index 6d059498496..6eb229b2792 100644
--- a/torch/csrc/autograd/profiler_kineto.h
+++ b/torch/csrc/autograd/profiler_kineto.h
@@ -48,8 +48,8 @@ struct TORCH_API KinetoEvent {
   c10::DeviceType deviceType() const;
   uint8_t deviceIndex() const;
   int64_t nBytes() const;
-  uint64_t startUs() const;
-  uint64_t durationUs() const;
+  uint64_t startNs() const;
+  uint64_t durationNs() const;
   bool isAsync() const;
   uint64_t correlationId() const;
   uint64_t linkedCorrelationId() const;
@@ -87,7 +87,11 @@ struct TORCH_API ProfilerResult {
   ~ProfilerResult();
 
   uint64_t trace_start_us() const {
-    return trace_start_us_;
+    return trace_start_ns_ / 1000;
+  }
+
+  uint64_t trace_start_ns() const {
+    return trace_start_ns_;
   }
 
   const std::vector<KinetoEvent>& events() const {
@@ -101,7 +105,7 @@ struct TORCH_API ProfilerResult {
   void save(const std::string& path);
 
  private:
-  uint64_t trace_start_us_ = 0;
+  uint64_t trace_start_ns_ = 0;
   std::vector<KinetoEvent> events_;
   std::unique_ptr<torch::profiler::impl::kineto::ActivityTraceWrapper> trace_;
   std::vector<experimental_event_t> event_tree_;
diff --git a/torch/csrc/autograd/profiler_python.cpp b/torch/csrc/autograd/profiler_python.cpp
index 70d9b7c38cc..03297138e5c 100644
--- a/torch/csrc/autograd/profiler_python.cpp
+++ b/torch/csrc/autograd/profiler_python.cpp
@@ -871,7 +871,7 @@ void PythonTracer::recordCCall(
     ThreadLocalResults& tls,
     PyFrameObject* frame,
     PyObject* arg) {
-  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(Py_TYPE(arg) == &PyCFunction_Type);
+  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(PyCFunction_Check(arg));
   auto fn = reinterpret_cast<PyCFunctionObject*>(arg);
 
   // NB: For C calls a new frame is not created, so we use `frame` rather than
diff --git a/torch/csrc/profiler/collection.cpp b/torch/csrc/profiler/collection.cpp
index c13e2c494e4..e3677eff0a2 100644
--- a/torch/csrc/profiler/collection.cpp
+++ b/torch/csrc/profiler/collection.cpp
@@ -587,7 +589,7 @@ int64_t Result::endTimeNS() const {
           Vulkan, start_time_ns_ + (e.in_tree_building_ ? 0 : e.duration_ns_)),
       ATTRIBUTE(Allocation, start_time_ns_),
       ATTRIBUTE(OutOfMemory, start_time_ns_),
-      ATTRIBUTE(Kineto, start_time_ns_ + e.duration_us_ * 1000),
+      ATTRIBUTE(Kineto, start_time_ns_ + e.duration_ns_),
       [&](const auto& e) -> int64_t { return e.end_time_ns_; }));
 
   // In rare cases we're willing to tolerate ops which are missing an end time
@@ -796,11 +798,11 @@ static constexpr const char* indexKey = "Ev Idx";
 
 void passEventsToKineto(
     const std::vector<std::shared_ptr<Result>>& results,
-    uint64_t start_time_us,
-    uint64_t end_time_us,
+    uint64_t start_time_ns,
+    uint64_t end_time_ns,
     const ProfilerConfig& config) {
   using namespace torch::profiler::impl::kineto;
-  TraceWrapper cpu_trace(start_time_us, "PyTorch Profiler");
+  TraceWrapper cpu_trace(start_time_ns, "PyTorch Profiler");
 
   // Generate Kineto events for each event recorded by the PyTorch profiler.
   for (const auto i : c10::irange(results.size())) {
@@ -810,8 +812,8 @@ void passEventsToKineto(
         e->kinetoType(),
         e->kineto_info_,
         e->correlationID(),
-        e->start_time_ns_ / 1000,
-        e->endTimeNS() / 1000);
+        e->start_time_ns_,
+        e->endTimeNS());
 
     TORCH_INTERNAL_ASSERT(activity || !kKinetoAvailable);
     if (activity) {
@@ -834,7 +836,7 @@ void passEventsToKineto(
   }
 
   // Kineto adds the events that it collected.
-  cpu_trace.transferCpuTrace(end_time_us);
+  cpu_trace.transferCpuTrace(end_time_ns);
 }
 
 #ifdef USE_KINETO
@@ -943,7 +945,7 @@ class TransferEvents {
         static_cast<int32_t>(activity->resourceId())};
 
     auto event = Result::create(
-        activity->timestamp() * 1000,
+        activity->timestamp(),
         noTID, // Placeholder
         device_and_resource,
         ExtraFields<EventType::Kineto>{
@@ -1090,11 +1092,11 @@ class TransferEvents {
 
 trace_ptr_t addKinetoEvents(
     std::vector<std::shared_ptr<Result>>& results,
-    uint64_t start_time_us,
-    uint64_t end_time_us,
+    uint64_t start_time_ns,
+    uint64_t end_time_ns,
     const ProfilerConfig& config) {
   using namespace torch::profiler::impl::kineto;
-  passEventsToKineto(results, start_time_us, end_time_us, config);
+  passEventsToKineto(results, start_time_ns, end_time_ns, config);
 
   // In on demand mode kineto is directly controlled by other machinery.
   if (config.global()) {
@@ -1345,8 +1347,8 @@ std::pair<
     std::unique_ptr<torch::profiler::impl::kineto::ActivityTraceWrapper>>
 RecordQueue::getRecords(
     std::function<time_t(approx_time_t)> time_converter,
-    uint64_t start_time_us,
-    uint64_t end_time_us) {
+    uint64_t start_time_ns,
+    uint64_t end_time_ns) {
   auto converter = [&](approx_time_t t) {
     return t == std::numeric_limits<approx_time_t>::min()
         ? std::numeric_limits<time_t>::min()
@@ -1397,7 +1399,7 @@ RecordQueue::getRecords(
 
   if (python_tracer_) {
     for (const auto& i : python_tracer_->getEvents(
-             converter, python_enters, end_time_us * 1000)) {
+             converter, python_enters, end_time_ns)) {
       out.push_back(i);
     }
     python_tracer_.reset();
@@ -1417,7 +1419,7 @@ RecordQueue::getRecords(
     }
   }
 
-  auto trace = addKinetoEvents(out, start_time_us, end_time_us, config_);
+  auto trace = addKinetoEvents(out, start_time_ns, end_time_ns, config_);
 
   std::stable_sort(out.begin(), out.end(), [](const auto& a, const auto& b) {
     return a->start_time_ns_ < b->start_time_ns_;
diff --git a/torch/csrc/profiler/collection.h b/torch/csrc/profiler/collection.h
index 20ef0c85836..919110f73d1 100644
--- a/torch/csrc/profiler/collection.h
+++ b/torch/csrc/profiler/collection.h
@@ -330,7 +333,7 @@ struct ExtraFields<EventType::Kineto> {
   };
 
   std::string name_;
-  int64_t duration_us_{0};
+  int64_t duration_ns_{0};
   uint64_t correlation_id_{0};
   libkineto::ActivityType activity_type_;
   Flow flow;
@@ -616,8 +620,8 @@ class TORCH_API RecordQueue {
       std::unique_ptr<torch::profiler::impl::kineto::ActivityTraceWrapper>>
   getRecords(
       std::function<time_t(approx_time_t)> time_converter,
-      uint64_t start_time_us,
-      uint64_t end_time_us);
+      uint64_t start_time_ns,
+      uint64_t end_time_ns);
 
  private:
   uint32_t id_;
diff --git a/torch/csrc/profiler/kineto_shim.cpp b/torch/csrc/profiler/kineto_shim.cpp
index 59e721eec41..0cb545d5b48 100644
--- a/torch/csrc/profiler/kineto_shim.cpp
+++ b/torch/csrc/profiler/kineto_shim.cpp
@@ -331,10 +331,18 @@ c10::DeviceType deviceTypeFromActivity(libkineto::ActivityType activity_type) {
     // TODO: T151322015
     case libkineto::ActivityType::MTIA_CCP_EVENTS:
       return c10::DeviceType::CUDA;
+    case libkineto::ActivityType::MLU_MEMCPY:
+    case libkineto::ActivityType::MLU_MEMSET:
+    case libkineto::ActivityType::MLU_CONCURRENT_KERNEL:
+    case libkineto::ActivityType::MLU_USER_ANNOTATION:
+    case libkineto::ActivityType::MLU_PROFILER_RANGE:
+    case libkineto::ActivityType::MLU_NOTIFIER:
+      return c10::DeviceType::PrivateUse1;
     case libkineto::ActivityType::CPU_OP:
     case libkineto::ActivityType::USER_ANNOTATION:
     case libkineto::ActivityType::EXTERNAL_CORRELATION:
     case libkineto::ActivityType::CUDA_RUNTIME:
+    case libkineto::ActivityType::MLU_RUNTIME:
     case libkineto::ActivityType::CPU_INSTANT_EVENT:
     case libkineto::ActivityType::GLOW_RUNTIME:
     case libkineto::ActivityType::MTIA_RUNTIME:
diff --git a/torch/csrc/profiler/orchestration/observer.h b/torch/csrc/profiler/orchestration/observer.h
index 5d42f9234c3..af7fe5866fa 100644
--- a/torch/csrc/profiler/orchestration/observer.h
+++ b/torch/csrc/profiler/orchestration/observer.h
@@ -16,6 +16,7 @@ enum class C10_API_ENUM ActivityType {
   CPU = 0,
   XPU, // XPU kernels, runtime
   CUDA, // CUDA kernels, runtime
+  MLU, // MLU kernels, runtime
   MTIA, // MTIA kernels, runtime
   NUM_KINETO_ACTIVITIES, // must be the last one
 };
diff --git a/torch/csrc/profiler/python/init.cpp b/torch/csrc/profiler/python/init.cpp
index 14eb91c64f8..6411bf93348 100644
--- a/torch/csrc/profiler/python/init.cpp
+++ b/torch/csrc/profiler/python/init.cpp
@@ -236,6 +236,7 @@ void initPythonBindings(PyObject* module) {
       .value("CPU", ActivityType::CPU)
       .value("XPU", ActivityType::XPU)
       .value("MTIA", ActivityType::MTIA)
-      .value("CUDA", ActivityType::CUDA);
+      .value("CUDA", ActivityType::CUDA)
+      .value("MLU", ActivityType::MLU);
 
   py::class_<ExperimentalConfig>(m, "_ExperimentalConfig")
       .def(
diff --git a/torch/profiler/_utils.py b/torch/profiler/_utils.py
index cb9469e4c98..c57006d5de6 100644
--- a/torch/profiler/_utils.py
+++ b/torch/profiler/_utils.py
@@ -147,15 +147,15 @@ class BasicEvaluation:
 
         cuda_launch_events = sorted(
             (e for e in cuda_event_list if is_cuda_launch_kernel(e)),
-            key=lambda x: x.start_us(),
+            key=lambda x: x.start_ns(),
         )
         cuda_kernel_events = sorted(
             (e for e in cuda_event_list if is_cuda_kernel(e)),
-            key=lambda x: x.start_us(),
+            key=lambda x: x.start_ns(),
         )
 
         self.cuda_events = sorted(
-            cuda_launch_events + cuda_kernel_events, key=lambda x: x.start_us()
+            cuda_launch_events + cuda_kernel_events, key=lambda x: x.start_ns()
         )
 
         kernel_mapping: Dict[_KinetoEvent, int] = {}
@@ -178,6 +178,8 @@ class BasicEvaluation:
         def new_old_event_comparator(event):
             if hasattr(event, "start_us"):
                 return event.start_us() * 1000
+            if hasattr(event, "start_ns"):
+                return event.start_ns()
             if hasattr(event, "start_time_ns"):
                 return event.start_time_ns
             raise Exception("Unknown Event Type")
@@ -192,20 +194,26 @@ class BasicEvaluation:
                 # Find current spawned cuda kernel event
                 if event in kernel_mapping and kernel_mapping[event] is not None:
                     spawned_kernel_index = kernel_mapping[event]
+            elif hasattr(event, "start_ns"):
+                start_time = event.start_ns()
+                end_time = event.start_ns() + event.duration_ns()
+                # Find current spawned cuda kernel event
+                if event in kernel_mapping and kernel_mapping[event] is not None:
+                    spawned_kernel_index = kernel_mapping[event]
             elif hasattr(event, "start_time_ns"):
                 start_time = event.start_time_ns  # type: ignore[attr-defined]
                 end_time = event.end_time_ns  # type: ignore[attr-defined]
 
             while (
                 current_kernel_index < len(cuda_kernel_events)
-                and (cuda_kernel_events[current_kernel_index].start_us()) * 1000
+                and (cuda_kernel_events[current_kernel_index].start_ns())
                 <= start_time
             ):
                 current_kernel_index += 1
             current_queue_depth = spawned_kernel_index - current_kernel_index + 1
             current_queue_depth = max(current_queue_depth, 0)
 
-            if hasattr(event, "start_us"):
+            if hasattr(event, "start_us") or hasattr(event, "start_ns"):
                 queue_depth_list.append(
                     Interval(start_time, end_time, current_queue_depth)
                 )
diff --git a/torch/profiler/profiler.py b/torch/profiler/profiler.py
index fce2e988471..6e10aafad8d 100644
--- a/torch/profiler/profiler.py
+++ b/torch/profiler/profiler.py
@@ -122,6 +122,7 @@ class _KinetoProfile:
             with_modules=self.with_modules,
             use_kineto=True,
             experimental_config=self.experimental_config,
+            use_mlu=(ProfilerActivity.MLU in self.activities),
         )
         self.profiler._prepare_trace()
 
@@ -504,6 +505,7 @@ class profile(_KinetoProfile):
         experimental_config: Optional[_ExperimentalConfig] = None,
         # deprecated:
         use_cuda: Optional[bool] = None,
+        use_mlu: Optional[bool] = None,
     ):
         activities_set = set(activities) if activities else supported_activities()
         if use_cuda is not None:
@@ -514,6 +516,14 @@ class profile(_KinetoProfile):
                 activities_set.remove(ProfilerActivity.CUDA)
         assert len(activities_set) > 0, "No valid profiler activities found"
 
+        if use_mlu is not None:
+            warn("use_mlu is deprecated, use activities argument instead")
+            if use_mlu:
+                activities_set.add(ProfilerActivity.MLU)
+            elif ProfilerActivity.MLU in activities_set:
+                activities_set.remove(ProfilerActivity.MLU)
+        assert len(activities_set) > 0, "No valid profiler activities found"
+
         super().__init__(
             activities=activities,
             record_shapes=record_shapes,
