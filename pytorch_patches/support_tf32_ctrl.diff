diff --git a/aten/src/ATen/Context.cpp b/aten/src/ATen/Context.cpp
index 1ec545dfc0..f99ace3574 100644
--- a/aten/src/ATen/Context.cpp
+++ b/aten/src/ATen/Context.cpp
@@ -183,6 +183,15 @@ void Context::setAllowTF32CuBLAS(bool b) {
   float32_matmul_precision = b ? at::Float32MatmulPrecision::HIGH : at::Float32MatmulPrecision::HIGHEST;
 }
 
+bool Context::allowTF32CnMatMul() const {
+  static bool allow_tf32_cnmatmul_override = c10::utils::check_env("TORCH_ALLOW_TF32_CNMATMUL_OVERRIDE") == true;
+  return allow_tf32_cnmatmul_override || float32_matmul_precision != at::Float32MatmulPrecision::HIGHEST;
+}
+
+void Context::setAllowTF32CnMatMul(bool b) {
+  setAllowTF32CuBLAS(b);
+}
+
 Float32MatmulPrecision Context::float32MatmulPrecision() const {
   return float32_matmul_precision;
 }
diff --git a/torch/csrc/Module.cpp b/torch/csrc/Module.cpp
index 503eb8a8aa..0f6b8088a0 100644
--- a/torch/csrc/Module.cpp
+++ b/torch/csrc/Module.cpp
@@ -752,6 +752,24 @@ PyObject* THPModule_allowTF32CuBLAS(PyObject* _unused, PyObject* noargs) {
   Py_RETURN_FALSE;
 }
 
+PyObject* THPModule_setAllowTF32CnMatMul(PyObject* _unused, PyObject* arg) {
+  THPUtils_assert(
+      PyBool_Check(arg),
+      "set_allow_tf32_cnmatmul expects a bool, "
+      "but got %s",
+      THPUtils_typename(arg));
+  at::globalContext().setAllowTF32CnMatMul(arg == Py_True);
+  Py_RETURN_NONE;
+}
+
+PyObject* THPModule_allowTF32CnMatMul(PyObject* _unused, PyObject* noargs) {
+  if (at::globalContext().allowTF32CnMatMul()) {
+    Py_RETURN_TRUE;
+  }
+  Py_RETURN_FALSE;
+}
+
+
 PyObject* THPModule_setAllowFP16ReductionCuBLAS(
     PyObject* _unused,
     PyObject* arg) {
@@ -1124,6 +1142,8 @@ static PyMethodDef TorchMethods[] = { // NOLINT
     {"_warn_deprecation", THPModule_warnDeprecation, METH_NOARGS, nullptr},
     {"_get_cublas_allow_tf32", THPModule_allowTF32CuBLAS, METH_NOARGS, nullptr},
     {"_set_cublas_allow_tf32", THPModule_setAllowTF32CuBLAS, METH_O, nullptr},
+    {"_get_cnmatmul_allow_tf32", THPModule_allowTF32CnMatMul, METH_NOARGS, nullptr},
+    {"_set_cnmatmul_allow_tf32", THPModule_setAllowTF32CnMatMul, METH_O, nullptr},
     {"_get_float32_matmul_precision",
      THPModule_float32MatmulPrecision,
      METH_NOARGS,
diff --git a/torch/csrc/profiler/collection.cpp b/torch/csrc/profiler/collection.cpp
index c13e2c494e..6567268e56 100644
--- a/torch/csrc/profiler/collection.cpp
+++ b/torch/csrc/profiler/collection.cpp
@@ -360,6 +360,7 @@ std::unique_ptr<KinetoObserverContext> ThreadLocalSubqueue::begin_op(
 
   event->start_time_ = torch::profiler::impl::getApproximateTime();
   event->allow_tf32_cublas_ = at::globalContext().allowTF32CuBLAS();
+  event->allow_tf32_cnmatmul_ = at::globalContext().allowTF32CnMatMul();
   if (!config_.experimental_config.performance_events.empty()) {
     const size_t n = config_.experimental_config.performance_events.size();
     event->counters_ = std::make_unique<perf_counters_t>(n, 0);
@@ -450,6 +451,7 @@ void ThreadLocalSubqueue::TorchOpStorage::materialize(
         extra_args(),
         gpu_fallback(),
         event->allow_tf32_cublas_,
+        event->allow_tf32_cnmatmul_,
         std::move(event->counters_)};
 
     out.emplace_back(Result::create(
diff --git a/torch/csrc/profiler/collection.h b/torch/csrc/profiler/collection.h
index 20ef0c8583..600b1894a8 100644
--- a/torch/csrc/profiler/collection.h
+++ b/torch/csrc/profiler/collection.h
@@ -133,6 +133,7 @@ struct ExtraFields<EventType::TorchOp> : TorchOpBasicFields {
       extra_args_t&& extra_args,
       FallbackPair&& device_fallback,
       bool allow_tf32_cublas,
+      bool allow_tf32_cnmatmul,
       std::unique_ptr<perf_counters_t>&& perf_event_counters)
       : TorchOpBasicFields(std::move(f)),
         correlation_id_{correlation_id},
@@ -144,6 +145,7 @@ struct ExtraFields<EventType::TorchOp> : TorchOpBasicFields {
         extra_args_{std::move(extra_args)},
         device_fallback_{std::move(device_fallback)},
         allow_tf32_cublas_{allow_tf32_cublas},
+        allow_tf32_cnmatmul_{allow_tf32_cnmatmul},
         perf_event_counters_{std::move(perf_event_counters)} {}
   uint64_t correlation_id_;
   time_t end_time_ns_;
@@ -154,6 +156,7 @@ struct ExtraFields<EventType::TorchOp> : TorchOpBasicFields {
   extra_args_t extra_args_;
   FallbackPair device_fallback_;
   bool allow_tf32_cublas_;
+  bool allow_tf32_cnmatmul_;
   std::unique_ptr<perf_counters_t> perf_event_counters_;
 };
 
@@ -423,6 +426,7 @@ struct KinetoObserverContext : public at::ObserverContext {
     approx_time_t end_time_{std::numeric_limits<approx_time_t>::min()};
 
     bool allow_tf32_cublas_;
+    bool allow_tf32_cnmatmul_;
     std::unique_ptr<perf_counters_t> counters_;
   };
 
diff --git a/torch/csrc/profiler/python/init.cpp b/torch/csrc/profiler/python/init.cpp
index 14eb91c64f..8dc8379549 100644
--- a/torch/csrc/profiler/python/init.cpp
+++ b/torch/csrc/profiler/python/init.cpp
@@ -396,7 +396,8 @@ void initPythonBindings(PyObject* module) {
           })
       .def_readonly("scope", &torch_op_t::scope_)
       .def_readonly("sequence_number", &torch_op_t::sequence_number_)
-      .def_readonly("allow_tf32_cublas", &torch_op_t::allow_tf32_cublas_);
+      .def_readonly("allow_tf32_cublas", &torch_op_t::allow_tf32_cublas_)
+      .def_readonly("allow_tf32_cnmatmul", &torch_op_t::allow_tf32_cnmatmul_);
 
   py::class_<ExtraFields<EventType::Backend>>(m, "_ExtraFields_Backend");
   py::class_<ExtraFields<EventType::Vulkan>>(m, "_ExtraFields_Vulkan");
