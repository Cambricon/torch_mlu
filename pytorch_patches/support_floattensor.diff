diff --git a/tools/autograd/templates/VariableType.h b/tools/autograd/templates/VariableType.h
index abad2f9338..327e9373c0 100644
--- a/tools/autograd/templates/VariableType.h
+++ b/tools/autograd/templates/VariableType.h
@@ -46,6 +46,7 @@ using c10::optional;
 
 namespace VariableType {
   TORCH_API std::vector<at::DeprecatedTypeProperties*> allCUDATypes();
+  TORCH_API std::vector<at::DeprecatedTypeProperties*> allMLUTypes();
   TORCH_API std::vector<at::DeprecatedTypeProperties*> allXPUTypes();
   TORCH_API std::vector<at::DeprecatedTypeProperties*> allCPUTypes();
 
diff --git a/torch/csrc/autograd/VariableTypeManual.cpp b/torch/csrc/autograd/VariableTypeManual.cpp
index b42d22d0fa..5a477f8453 100644
--- a/torch/csrc/autograd/VariableTypeManual.cpp
+++ b/torch/csrc/autograd/VariableTypeManual.cpp
@@ -47,6 +47,10 @@ std::vector<at::DeprecatedTypeProperties*> allCUDATypes() {
   return allTypesForBackends({Backend::CUDA, Backend::SparseCUDA});
 }
 
+std::vector<at::DeprecatedTypeProperties*> allMLUTypes() {
+  return allTypesForBackends({Backend::PrivateUse1});
+}
+
 std::vector<at::DeprecatedTypeProperties*> allXPUTypes() {
   return allTypesForBackends({Backend::XPU, Backend::SparseXPU});
 }
diff --git a/torch/csrc/utils/tensor_types.cpp b/torch/csrc/utils/tensor_types.cpp
index 5cc5c722ef..0b10e0813a 100644
--- a/torch/csrc/utils/tensor_types.cpp
+++ b/torch/csrc/utils/tensor_types.cpp
@@ -78,11 +78,14 @@ std::string type_to_string(const at::DeprecatedTypeProperties& type) {
 at::TensorOptions options_from_string(const std::string& str) {
   static std::string cuda_prefix("torch.cuda.");
   static std::string xpu_prefix("torch.xpu.");
+  static std::string mlu_prefix("torch.mlu.");
   static c10::once_flag cpu_once;
   static c10::once_flag cuda_once;
   static c10::once_flag xpu_once;
+  static c10::once_flag mlu_once;
   static std::unordered_map<std::string, at::DeprecatedTypeProperties*> cpu_map;
   static std::unordered_map<std::string, at::DeprecatedTypeProperties*> xpu_map;
+  static std::unordered_map<std::string, at::DeprecatedTypeProperties*> mlu_map;
   static std::unordered_map<std::string, at::DeprecatedTypeProperties*>
       cuda_map;
 
@@ -115,6 +117,16 @@ at::TensorOptions options_from_string(const std::string& str) {
       }
     });
     map = &xpu_map;
+  } else if (
+      std::mismatch(mlu_prefix.begin(), mlu_prefix.end(), str.begin()).first ==
+      mlu_prefix.end()) {
+    // torch.mlu. is prefix of str
+    c10::call_once(mlu_once, []() {
+      for (auto type : autograd::VariableType::allMLUTypes()) {
+        mlu_map.emplace(type_to_string(*type), type);
+      }
+    });
+    map = &mlu_map;
   } else {
     c10::call_once(cpu_once, []() {
       for (auto type : autograd::VariableType::allCPUTypes()) {
diff --git a/torch/csrc/utils/python_arg_parser.h b/torch/csrc/utils/python_arg_parser.h
index f752ee88a7..77399f13ec 100644
--- a/torch/csrc/utils/python_arg_parser.h
+++ b/torch/csrc/utils/python_arg_parser.h
@@ -268,8 +268,10 @@ struct PythonArgs {
   inline at::Layout layoutWithDefault(int i, at::Layout default_layout);
   inline c10::optional<at::Layout> layoutOptional(int i);
   inline at::Device device(int i);
+  inline at::Device device(int i, c10::DispatchKey dispatch_key);
   inline at::Device deviceWithDefault(int i, const at::Device& default_device);
   inline c10::optional<at::Device> deviceOptional(int i);
+  inline c10::optional<at::Device> deviceOptional(int i, c10::DispatchKey dispatch_key);
   inline at::Dimname dimname(int i);
   inline std::vector<at::Dimname> dimnamelist(int i);
   inline c10::optional<std::vector<at::Dimname>> toDimnameListOptional(int i);
@@ -798,8 +800,33 @@ inline at::Device toDevice(PyObject* obj) {
   if (THPUtils_checkLong(obj)) {
     const auto device_index = THPUtils_unpackLong(obj);
     TORCH_CHECK(device_index >= 0, "Device index must not be negative");
+#ifdef USE_CUDA
     return at::Device(
         c10::DeviceType::CUDA, static_cast<c10::DeviceIndex>(device_index));
+#else
+    return at::Device(
+        c10::DeviceType::PrivateUse1, static_cast<c10::DeviceIndex>(device_index));
+#endif
+  }
+  const std::string& device_str = THPUtils_unpackString(obj);
+  return at::Device(device_str);
+}
+
+inline at::Device toDevice(PyObject* obj, c10::DispatchKey dispatch_key) {
+  if (THPDevice_Check(obj)) {
+    const auto device = reinterpret_cast<THPDevice*>(obj);
+    return device->device;
+  }
+  if (THPUtils_checkLong(obj)) {
+    const auto device_index = THPUtils_unpackLong(obj);
+    TORCH_CHECK(device_index >= 0, "Device index must not be negative");
+    if (dispatch_key != c10::DispatchKey::PrivateUse1) {
+      return at::Device(
+          c10::DeviceType::CUDA, static_cast<c10::DeviceIndex>(device_index));
+    } else {
+      return at::Device(
+          c10::DeviceType::PrivateUse1, static_cast<c10::DeviceIndex>(device_index));
+    }
   }
   const std::string& device_str = THPUtils_unpackString(obj);
   return at::Device(device_str);
@@ -812,6 +839,13 @@ inline at::Device PythonArgs::device(int i) {
   return toDevice(args[i]);
 }
 
+inline at::Device PythonArgs::device(int i, c10::DispatchKey dispatch_key) {
+  if (!args[i]) {
+    return torch::tensors::get_default_device();
+  }
+  return toDevice(args[i], dispatch_key);
+}
+
 inline at::Device PythonArgs::deviceWithDefault(
     int i,
     const at::Device& default_device) {
@@ -826,6 +860,12 @@ inline c10::optional<at::Device> PythonArgs::deviceOptional(int i) {
   return device(i);
 }
 
+inline c10::optional<at::Device> PythonArgs::deviceOptional(int i, c10::DispatchKey dispatch_key) {
+  if (!args[i])
+    return c10::nullopt;
+  return device(i, dispatch_key);
+}
+
 inline at::Dimname PythonArgs::dimname(int i) {
   TORCH_INTERNAL_ASSERT(args[i] != nullptr);
   return THPDimname_parse(args[i]);
diff --git a/torch/csrc/utils/tensor_new.cpp b/torch/csrc/utils/tensor_new.cpp
index 820212459b..849c495dec 100644
--- a/torch/csrc/utils/tensor_new.cpp
+++ b/torch/csrc/utils/tensor_new.cpp
@@ -689,7 +689,7 @@ Tensor legacy_tensor_generic_ctor_new(
   ParsedArgs<2> parsed_args;
   auto r = parser.parse(args, kwargs, parsed_args);
   if (r.idx == 0) {
-    auto deviceOptional = r.deviceOptional(0);
+    auto deviceOptional = r.deviceOptional(0, dispatch_key);
     check_legacy_ctor_device(dispatch_key, deviceOptional);
     at::OptionalDeviceGuard device_guard(deviceOptional);
     return at::empty({0}, build_options(options, scalar_type));
@@ -740,7 +740,7 @@ Tensor legacy_tensor_generic_ctor_new(
     }
   } else if (r.idx == 5) {
     PyObject* arg = r.pyobject(0);
-    auto deviceOptional = r.deviceOptional(1);
+    auto deviceOptional = r.deviceOptional(1, dispatch_key);
     check_legacy_ctor_device(dispatch_key, deviceOptional);
     if (!THPSize_Check(arg) && PyTuple_GET_SIZE(args) >= 1 &&
         arg == PyTuple_GET_ITEM(args, 0)) {
@@ -750,9 +750,9 @@ Tensor legacy_tensor_generic_ctor_new(
           options, scalar_type, deviceOptional, r.pyobject(0));
     }
     return new_with_sizes(
-        options, scalar_type, r.deviceOptional(1), r.symintlist(0));
+        options, scalar_type, r.deviceOptional(1, dispatch_key), r.symintlist(0));
   } else if (r.idx == 6) {
-    auto deviceOptional = r.deviceOptional(1);
+    auto deviceOptional = r.deviceOptional(1, dispatch_key);
     check_legacy_ctor_device(dispatch_key, deviceOptional);
     return legacy_new_from_sequence(
         options, scalar_type, deviceOptional, r.pyobject(0));
