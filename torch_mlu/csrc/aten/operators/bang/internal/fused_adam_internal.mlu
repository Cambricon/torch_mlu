/*************************************************************************
 * Copyright (C) [2019-2023] by Cambricon, Inc.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/

#include "bang_internal.h"
#include "aten/operators/bang/internal/multi_tensor_apply.h"

namespace torch_mlu {
namespace ops {

#define ALIGN_SIZE 128

// Use half of the max nram size for ping-pong
__nram__ char total_nram[MAX_NRAM_SIZE];

// split nram to grad_nram,m_nram,v_nram,variable_nram
inline int get_char_element_num(const int& nram_size,
                                const int& split_num) {
  return ((nram_size - reserve_stack_size) / split_num) / ALIGN_SIZE * ALIGN_SIZE;
}

template<typename T, typename FULL_T = float,
         torch_mlu::ops::internal::ADAM_MODE mode = torch_mlu::ops::internal::ADAM_MODE::adamw,
         std::enable_if_t<std::is_same_v<FULL_T, float>, int> = 1,
         std::enable_if_t<std::disjunction_v<std::is_same<float, T>,
                                             std::is_same<bfloat16_t, T>,
                                             std::is_same<half, T>>, int> = 1>
struct AdamFunctor {
  __mlu_func__ inline void operator()(const int& num_align,
                                      T* variable_nram,
                                      T* grad_nram,
                                      FULL_T* m_nram,
                                      FULL_T* v_nram,
                                      const float& beta1,
                                      const float& beta2,
                                      const float& beta1_minus,
                                      const float& beta2_minus,
                                      const float& decay,
                                      const float& learning_rate_correction,
                                      const float& epsilon_correction,
                                      const float& decay_correction,
                                      const int& load_offset) {
    FULL_T* grad_nram_f = (FULL_T*)grad_nram;
    FULL_T* variable_nram_f = (FULL_T*)variable_nram;
    if constexpr (std::is_same_v<T, bfloat16_t>) {
      __bang_bfloat162float(grad_nram_f, grad_nram + load_offset, num_align);
      __bang_bfloat162float(variable_nram_f, variable_nram + load_offset, num_align);
    } else if constexpr (std::is_same_v<T, half>) {
      __bang_half2float(grad_nram_f, grad_nram + load_offset, num_align);
      __bang_half2float(variable_nram_f, variable_nram + load_offset, num_align);
    }
    if constexpr (mode == torch_mlu::ops::internal::ADAM_MODE::adam) {
      // scaled_grad = scaled_grad + decay * variable
      __bang_fusion(FUSION_FMA, grad_nram_f, variable_nram_f, decay, grad_nram_f, num_align, num_align);
    }
    // mt = beta1 * mt-1 + (1 - beta1) * grad
    __bang_mul_scalar(m_nram, m_nram, beta1, num_align);
    __bang_fusion(FUSION_FMA, m_nram, grad_nram_f, beta1_minus, m_nram, num_align, num_align);
    // vt = beta2 * vt-1 + (1 - beta2) * grad ^ 2
    __bang_mul(grad_nram_f, grad_nram_f, grad_nram_f, num_align);
    __bang_mul_scalar(grad_nram_f, grad_nram_f, beta2_minus, num_align);
    __bang_fusion(FUSION_FMA, v_nram, v_nram, beta2, grad_nram_f, num_align, num_align);
    // demo = exp_avg  / (sqrtf(exp_avg_sq) + epsilon_correction)
    // epsilon_correction = eps * sqrtf(bias_correction2)
    // use grad_nram as temp buffer
    __bang_sqrt(grad_nram_f, v_nram, num_align);
    __bang_add_scalar(grad_nram_f, grad_nram_f, epsilon_correction, num_align);
    __bang_recip(grad_nram_f, grad_nram_f, num_align);
    __bang_mul(grad_nram_f, m_nram, grad_nram_f, num_align);
    if constexpr (mode == torch_mlu::ops::internal::ADAM_MODE::adamw) {
      // demo = demo + decay_correction * param
      // decay_correction = bias_correction1 / sqrtf(bias_correction2) * decay
      __bang_fusion(FUSION_FMA, grad_nram_f, variable_nram_f, decay_correction, grad_nram_f,
                    num_align, num_align);
    }
    // param = param + learning_rate_correction * demo
    // learning_rate_correction = -1 * lr*sqrtf(bias_correction2)/bias_correction1
    __bang_fusion(FUSION_FMA, variable_nram_f, grad_nram_f, learning_rate_correction,
                  variable_nram_f, num_align, num_align);
    if constexpr (std::is_same_v<T, bfloat16_t>) {
      __bang_float2bfloat16_rn(variable_nram, (FULL_T*)variable_nram_f, num_align);
    } else if constexpr (std::is_same_v<T, half>) {
      __bang_float2half_rn(variable_nram, (FULL_T*)variable_nram_f, num_align);
    }
  }
};  // struct AdamFunctor

template<typename tupleTypeList, int maxBlockNum, int depth>
struct ApplyAdam {
template<typename AdamFunctor>
__mlu_func__ static void const call_internal(const torch_mlu::bangcommon::BlockInfoContainer<maxBlockNum, depth>& block_info_container,
                                             const float& beta1,
                                             const float& beta1_minus,
                                             const float& beta2,
                                             const float& beta2_minus,
                                             const float& epsilon_correction,
                                             const float& learning_rate_correction,
                                             AdamFunctor&& compute_func,
                                             const float& decay,
                                             const float& decay_correction) {
  using T = std::tuple_element_t<0, tupleTypeList>;
  using FULL_T = std::tuple_element_t<2, tupleTypeList>;
  torch_mlu::bangcommon::MemoryPolicy<tupleTypeList, maxBlockNum, depth>
    data_handler(block_info_container);
  const int circle_num = data_handler.get_circle_num();
  const int element_num = data_handler.get_each_block_element_num();
  const int char_element_num = block_info_container.block_size * sizeof(float);
  // params--grads--exp_avg--exp_avg_sq
  T* param_nram_start = (T*)total_nram;
  T* param_nram_load_start = param_nram_start;
  T* grad_nram_start = (T*)(total_nram + 2 * char_element_num);
  T* grad_nram_load_start = grad_nram_start;
  // load_offset for half and bfloat16, and copy data to last part of nram,
  // for using same nram space to convert from half/bfloat16 to float.
  // This is not need for store data back to gdram.
  // char_element_num size: |-----------------|-----------------|
  //                    float start    half/bfloat16 start
  const int load_offset = sizeof(T) == 2 ? char_element_num / sizeof(FULL_T) : 0;
  if constexpr (sizeof(T) == 2) {
    param_nram_load_start = param_nram_start + load_offset;
    grad_nram_load_start = grad_nram_start + load_offset;
  }
  FULL_T* exp_avg_nram_start = (FULL_T*)(total_nram + 4 * char_element_num);
  FULL_T* exp_avg_sq_nram_start = (FULL_T*)(total_nram + 6 * char_element_num);
  const int full_t_ping_pong_size = element_num;
  const int t_ping_pong_size = sizeof(T) == 2 ? element_num * 2 : element_num;
  int ping_size = 0;
  int pong_size = 0;
  void* load_ping_nram_array[depth] = {
    (void*)param_nram_load_start, (void*)grad_nram_load_start,
    (void*)exp_avg_nram_start, (void*)exp_avg_sq_nram_start
  };
  void* load_pong_nram_array[depth] = {
    (void*)(param_nram_load_start + t_ping_pong_size),
    (void*)(grad_nram_load_start + t_ping_pong_size),
    (void*)(exp_avg_nram_start + full_t_ping_pong_size),
    (void*)(exp_avg_sq_nram_start + full_t_ping_pong_size)
  };
  void* store_ping_nram_array[depth] = {
    (void*)param_nram_start, nullptr,
    (void*)exp_avg_nram_start, (void*)exp_avg_sq_nram_start
  };
  void* store_pong_nram_array[depth] = {
    (void*)(param_nram_start + t_ping_pong_size),
    nullptr,
    (void*)(exp_avg_nram_start + full_t_ping_pong_size),
    (void*)(exp_avg_sq_nram_start + full_t_ping_pong_size)
  };

  if (circle_num > 0) {
    data_handler.multi_data_load_same_size(0, load_ping_nram_array, ping_size);
    __sync_io();
  }
  if (circle_num > 1) {
    // pong and compute
    data_handler.multi_data_load_same_size(1, load_pong_nram_array, pong_size);
    // compute ping
    std::forward<AdamFunctor>(compute_func)(ping_size, param_nram_start, grad_nram_start, exp_avg_nram_start,
                 exp_avg_sq_nram_start, beta1, beta2, beta1_minus, beta2_minus, decay,
                 learning_rate_correction, epsilon_correction, decay_correction, load_offset);
    __sync_io_move_compute(false, false, true, true, false, false);
    __sync_io_move_compute(true, false, false, false, false, true);
  }
  int load_index = 0;
  int store_index = 0;
  for (int i = 0; i < circle_num - 2; i++) {
    // store data
    load_index = i + 2;
    store_index = i;
    int ping_pong = i % 2;
    data_handler.multi_data_store_same_size(store_index, ping_pong ? store_pong_nram_array : store_ping_nram_array,
                                            ping_pong ? pong_size : ping_size);
    data_handler.multi_data_load_same_size(load_index, ping_pong ? load_pong_nram_array : load_ping_nram_array,
                                            ping_pong ? pong_size : ping_size);
    // compute data date order is different with load and store data.
    ping_pong = (i + 1) % 2;
    std::forward<AdamFunctor>(compute_func)(ping_pong ? pong_size : ping_size,
                 param_nram_start + ping_pong * t_ping_pong_size,
                 grad_nram_start + ping_pong * t_ping_pong_size,
                 exp_avg_nram_start + ping_pong * full_t_ping_pong_size,
                 exp_avg_sq_nram_start + ping_pong * full_t_ping_pong_size,
                 beta1, beta2, beta1_minus, beta2_minus,
                 decay, learning_rate_correction, epsilon_correction,
                 decay_correction, load_offset);
    __sync_io_move_compute(false, false, true, true, false, false);
    __sync_io_move_compute(true, false, false, false, false, true);
  }

  // store circle - 1 data
  int ping_pong = circle_num % 2;
  if (circle_num > 1) {
    store_index = circle_num - 2;
    data_handler.multi_data_store_same_size(store_index,
                                            ping_pong ? store_pong_nram_array : store_ping_nram_array,
                                            ping_pong ? pong_size : ping_size);
  }

  ping_pong = (circle_num + 1) % 2;
  if (circle_num > 0) {
    // compute last circle
    std::forward<AdamFunctor>(compute_func)(ping_pong ? pong_size : ping_size,
                 param_nram_start + ping_pong * t_ping_pong_size,
                 grad_nram_start + ping_pong * t_ping_pong_size,
                 exp_avg_nram_start + ping_pong * full_t_ping_pong_size,
                 exp_avg_sq_nram_start + ping_pong * full_t_ping_pong_size,
                 beta1, beta2, beta1_minus, beta2_minus,
                 decay, learning_rate_correction, epsilon_correction,
                 decay_correction, load_offset);
    __sync_compute();
    // store last circle data
    store_index = circle_num - 1;
    data_handler.multi_data_store_same_size(store_index,
                                            ping_pong ? store_pong_nram_array : store_ping_nram_array,
                                            ping_pong ? pong_size : ping_size);
  }
}

__mlu_global__ static void call(const torch_mlu::bangcommon::BlockInfoContainer<maxBlockNum, depth> block_info_container,
                                float beta1,
                                float beta1_minus,
                                float beta2,
                                float beta2_minus,
                                float epsilon_correction,
                                float learning_rate_correction,
                                internal::ADAM_MODE adam_mode,
                                float decay,
                                float decay_correction) {
  if (__is_mpu()) return;
  using T = std::tuple_element_t<0, tupleTypeList>;
  using FULL_T = std::tuple_element_t<2, tupleTypeList>;
  switch (adam_mode) {
    case torch_mlu::ops::internal::ADAM_MODE::adam:
      call_internal(block_info_container, beta1, beta1_minus, beta2, beta2_minus,
                    epsilon_correction, learning_rate_correction,
                    AdamFunctor<T, FULL_T, torch_mlu::ops::internal::ADAM_MODE::adam>(),
                    decay, decay_correction);
      break;
    case torch_mlu::ops::internal::ADAM_MODE::adamw:
      call_internal(block_info_container, beta1, beta1_minus, beta2, beta2_minus,
                    epsilon_correction, learning_rate_correction,
                    AdamFunctor<T, FULL_T, torch_mlu::ops::internal::ADAM_MODE::adamw>(),
                    decay, decay_correction);
      break;
    default:
      break;
  }
}

};  // struct ApplyAdam

template<cnrtDataType_V2_t value, int depth>
void apex_fused_adam_internal(const std::vector<std::array<void*, depth>>& data_ptr_list,
                              const std::vector<int64_t>& sizes,
                              float beta1,
                              float beta1_minus,
                              float beta2,
                              float beta2_minus,
                              float epsilon_correction,
                              float learning_rate_correction,
                              internal::ADAM_MODE mode,
                              float decay,
                              float decay_correction,
                              cnrtQueue_t queue,
                              cnrtFunctionType_t k_type,
                              cnrtDim3_t k_dim,
                              const int nram_size) {
  // Load fix num data to nram each time per tensor.
  // Data order in nram:
  // variable_nram, grad_nram, m_nram, v_nram
  // and grad_nram space is reused as temp buffer, so can't restore
  // grad_nram data to gdram. After each iterator, m_nram, v_nram,
  // variable_nram data will be saved back to gdram.
  // Restrict and Note:
  // 1) Exp_avg_ptr and exp_avg_sq_ptr are float type, grad_ptr,
  //    param_ptr are support bfloat16, float16 and float32 type;
  // 2) All data on nram compute are float type, so need to convert
  //    grad_ptr and param_ptr to float type before compute;
  // 3) Grad_nram space is reused as temp buffer, so can't restore
  //    grad_nram data to gdram;
  // 4) Load variable_nram, grad_nram, m_nram, v_nram to nram each time per block,
  //    After compute, variable_nram, m_nram, v_nram data will be saved back to gdram;
  // 5) Each block contains element_num is less or equal to get_char_element_num(nram_size) / sizeof(float);
  const int element_num = get_char_element_num(nram_size, 8) / sizeof(float);
  constexpr int maxBlockNum = torch_mlu::bangcommon::depth_to_max_blockinfo[depth - 1];
  using T = CNRTTypeValueToBangcCppType_t<value>;
  using FULL_T = float;
  using tupleTypeList = std::tuple<T, T, FULL_T, FULL_T>;
  torch_mlu::bangcommon::multi_tensor_apply<maxBlockNum, depth, tupleTypeList>(
              data_ptr_list, sizes, element_num, queue, k_type, k_dim,
              ApplyAdam<tupleTypeList, maxBlockNum, depth>(),
              beta1, beta1_minus, beta2, beta2_minus, epsilon_correction,
              learning_rate_correction, mode, decay, decay_correction);
}

// add explicit instantiation for float, half, bfloat16_t
#define APEX_FUSED_ADAM_INTERNAL_DEFINE(cnrt_type, depth)       \
template void apex_fused_adam_internal<cnrt_type, depth>(       \
    const std::vector<std::array<void*, depth>>& data_ptr_list, \
    const std::vector<int64_t>& sizes,                          \
    float beta1,                                                \
    float beta1_minus,                                          \
    float beta2,                                                \
    float beta2_minus,                                          \
    float epsilon_correction,                                   \
    float learning_rate_correction,                             \
    internal::ADAM_MODE mode,                                   \
    float decay,                                                \
    float decay_correction,                                     \
    cnrtQueue_t queue,                                          \
    cnrtFunctionType_t k_type,                                  \
    cnrtDim3_t k_dim,                                           \
    const int nram_size);

APEX_FUSED_ADAM_INTERNAL_DEFINE(cnrtDataType_V2_t::cnrtFloat, 4)
APEX_FUSED_ADAM_INTERNAL_DEFINE(cnrtDataType_V2_t::cnrtHalf, 4)
APEX_FUSED_ADAM_INTERNAL_DEFINE(cnrtDataType_V2_t::cnrtBfloat, 4)

#undef APEX_FUSED_ADAM_INTERNAL_DEFINE

} // namespace ops
} // namespace torch_mlu
