/*************************************************************************
 * Copyright (C) [2019-2023] by Cambricon, Inc.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/

#include <cmath>
#include "bang_internal.h"

namespace torch_mlu {
namespace ops {

#define SIZE_PER_REGION_L2NORM (MAX_NRAM_SIZE - 32 * 4)

__nram__ float temp_nram[32];
__nram__ char total_nram[SIZE_PER_REGION_L2NORM];

inline __mlu_func__ void ComputeInternal(float* tensor_nram, int num_align) {
  __bang_square(tensor_nram, tensor_nram, num_align);
  __bang_sumpool(
      temp_nram,
      tensor_nram,
      /*channel*/ 32,
      /*height*/ 1,
      /*width*/ num_align / 32,
      /*kernel_height*/ 1,
      /*kernel_width*/ num_align / 32,
      /*stride_height*/ 1,
      /*stride_width*/ 1);
  __bang_reduce_sum(temp_nram, temp_nram, 32);
}

template <typename T>
inline __mlu_func__ void Compute(T* tensor_nram, int num_align) {
  ComputeInternal((float*)tensor_nram, num_align);
}

template <>
inline __mlu_func__ void Compute(half* tensor_nram, int num_align) {
  __bang_half2float(
      (float*)tensor_nram, tensor_nram + SIZE_PER_REGION_L2NORM / 4, num_align);
  ComputeInternal((float*)tensor_nram, num_align);
}

template <>
inline __mlu_func__ void Compute(bfloat16_t* tensor_nram,
                                 int num_align) {
  __bang_bfloat162float((float*)tensor_nram, tensor_nram + SIZE_PER_REGION_L2NORM / 4, num_align);
  ComputeInternal((float*)tensor_nram, num_align);
}

template <typename T>
__mlu_func__ void ApplyL2norm(
    const AddressList& tensors,
    const SizeList& sizes,
    float* output_buffer,
    float* output_buffer_per_tensor,
    int tensor_num,
    bool per_tensor,
    int32_t* overflow) {
  // Data
  int num_per_region =
      SIZE_PER_REGION_L2NORM / sizeof(float); // compute type is fixed as float
  T* tensor_nram = (T*)total_nram;
  int load_offset = sizeof(T) == 2 ? SIZE_PER_REGION_L2NORM / 4 : 0;

  int remains_chunck_num = 0; // assign each task average chuncks as possible
  // avoid int overflow for tensor_size near to 2 ^ 31(2G)
  int64_t tensor_size, chunck_num, last_chunck_size;
  int repeat_per_task, last_loop_chunck_num;
  int chunck_id; // chunck_id maybe minus
  int element_num;
  float per_core_sum = 0;
  float temp_partial_sum = 0.0;
  for (int tensor_id = 0; tensor_id < tensor_num; ++tensor_id) {
    tensor_size = sizes.sizes[tensor_id];

    chunck_num = ALIGN_UP_DIV(tensor_size, num_per_region);
    last_chunck_size = (tensor_size - 1) % num_per_region + 1;

    repeat_per_task = ALIGN_UP_DIV(chunck_num + remains_chunck_num, taskDim);
    last_loop_chunck_num = chunck_num % taskDim;

    T* tensor_gdram = (T*)tensors.addresses[tensor_id];
    for (int iter = 0; iter < repeat_per_task; ++iter) {
      chunck_id = iter * taskDim + taskId - remains_chunck_num;

      if (chunck_id > -1 && chunck_id < chunck_num) {
        element_num =
            chunck_id == chunck_num - 1 ? last_chunck_size : num_per_region;
        // bang_squre requires n * 128 bytes
        // __bang_half2float requires n * 64 elements
        // take 64 as the intersection
        int num_align = ALIGN_UP(element_num, 64);

        // Load
        if (num_align != element_num) {
          if (sizeof(T) == 2) {
            __bang_write_value((uint16_t *)tensor_nram + load_offset, num_align, (uint16_t)0);
          } else {
            __bang_write_value((uint32_t *)tensor_nram + load_offset, num_align, (uint32_t)0);
          }
        }
        __memcpy(
            tensor_nram + load_offset,
            tensor_gdram + chunck_id * num_per_region,
            element_num * sizeof(T),
            GDRAM2NRAM);

        Compute(tensor_nram, num_align);

        temp_partial_sum = *temp_nram;
        per_core_sum += temp_partial_sum;
        if (per_tensor) {
          __bang_atomic_add(
              temp_nram + 1,
              output_buffer_per_tensor + tensor_id * taskDim + taskId,
              temp_partial_sum);
        }
      }
    }
    remains_chunck_num = (remains_chunck_num + last_loop_chunck_num) % taskDim;
  }
  __bang_atomic_add(temp_nram + 1, output_buffer + taskId, per_core_sum);
} 

__mlu_global__ void MLUMultiTensorL2Norm(AddressList tensors, SizeList sizes,
                                         float* output_buffer,
                                         float* output_buffer_per_tensor,
                                         int tensor_num, bool per_tensor,
                                         bool amp_opt, int32_t* overflow,
                                         cnrtDataType_V2_t cnrt_type) {
  if (amp_opt && *overflow) {
    return;
  }
  switch (cnrt_type) {
    case (cnrtDataType_V2_t)cnrtFloat:
      ApplyL2norm<float>(tensors, sizes, output_buffer,
                   output_buffer_per_tensor, tensor_num, per_tensor,
                   overflow);
      break;
    case (cnrtDataType_V2_t)cnrtHalf:
      ApplyL2norm<half>(tensors, sizes, output_buffer, output_buffer_per_tensor,
                  tensor_num, per_tensor, overflow);
      break;
    case (cnrtDataType_V2_t)cnrtBfloat:
      ApplyL2norm<bfloat16_t>(tensors, sizes, output_buffer, output_buffer_per_tensor,
                  tensor_num, per_tensor, overflow);
      break;
    default:
      break;
  }
}

__mlu_func__ void ReduceSumPerTensor(
    float* output_per_tensor,
    float* output_buffer_per_tensor_ptr,
    int tensor_num) {
  // divide tensor_num equally by cores except for the last core
  // NOTE [taskDim assumption]: the following codes make the assumption that
  // taskDims for cleanup kernel and ApplyL2norm kernel are the same,
  // please be careful to break that assumption.
  const int batch_num = ALIGN_UP_DIV(tensor_num, taskDim);
  for (int batch = 0; batch < batch_num; batch++) {
    int tensor_id = batch * taskDim + taskId;
    if (tensor_id < tensor_num) {
      float temp_sum = 0.0f;
      float* tensor_nram = (float*)total_nram;
      __memcpy(
          tensor_nram,
          output_buffer_per_tensor_ptr + tensor_id * taskDim,
          taskDim * sizeof(float),
          GDRAM2NRAM);
      for (int chunk_id = 0; chunk_id < taskDim; chunk_id++) {
        temp_sum += tensor_nram[chunk_id];
      }
      output_per_tensor[tensor_id] = sqrtf(temp_sum);
    }
  }
}

__mlu_global__ void MLUCleanUp(
    float* output,
    float* output_per_tensor,
    float* output_buffer_ptr,
    float* output_buffer_per_tensor_ptr,
    bool per_tensor,
    int tensor_num,
    bool amp_opt,
    int* overflow) {
  if (coreId == 0x80) {
    return;
  }

  if (amp_opt && *overflow) {
    return;
  }

  // all cores will write to the same place with the same value
  // see NOTE [taskDim assumption]
  float final_value = 0.0f;
  float* tensor_nram = (float*)total_nram;
  __memcpy(tensor_nram, output_buffer_ptr, taskDim * sizeof(float), GDRAM2NRAM);
  for (int i = 0; i < taskDim; i++) {
    final_value += tensor_nram[i];
  }

  if (std::isnan(final_value) || std::isinf(final_value)) {
    *overflow = 1;
  }

  // according to the behavior of multi_tensor_l2norm_mp, if abnormal value
  // is detected, just return. And here we take advantage of the feature
  // that abnormal values can be saved in the output.
  if (amp_opt && *overflow) {
    return;
  }

  *output = sqrtf(final_value);

  if (per_tensor) {
    ReduceSumPerTensor(
        output_per_tensor, output_buffer_per_tensor_ptr, tensor_num);
  }
}

void bang_fused_l2_norm_internal(AddressList tensors, SizeList sizes,
                                 float* output_buffer_ptr,
                                 float* output_buffer_per_tensor_ptr,
                                 int tensor_num, bool per_tensor,
                                 int32_t* overflow, cnrtDim3_t k_dim,
                                 cnrtFunctionType_t k_type, cnrtQueue_t queue,
                                 cnrtDataType_V2_t cnrt_type, bool amp_opt) {
  MLUMultiTensorL2Norm<<<k_dim, k_type, queue>>>(
      tensors,
      sizes,
      output_buffer_ptr,
      output_buffer_per_tensor_ptr,
      tensor_num,
      per_tensor,
      amp_opt,
      overflow,
      cnrt_type);
}

void bang_fused_l2_norm_clean_internal(
    float* output_ptr,
    float* output_per_tensor_ptr,
    float* output_buffer_ptr,
    float* output_buffer_per_tensor_ptr,
    bool per_tensor,
    int tensor_num,
    int* overflow,
    cnrtDim3_t k_dim,
    cnrtFunctionType_t k_type,
    cnrtQueue_t queue,
    bool amp_opt) {
  MLUCleanUp<<<k_dim, k_type, queue>>>(
      output_ptr,
      output_per_tensor_ptr,
      output_buffer_ptr,
      output_buffer_per_tensor_ptr,
      per_tensor,
      tensor_num,
      amp_opt,
      overflow);
}

} // namespace ops
} // namespace torch_mlu
