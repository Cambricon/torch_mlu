{"steps": {"columns": [{"type": "string", "name": "Step"}, {"type": "number", "name": "Kernel"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Memcpy"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Memset"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Runtime"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "DataLoader"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "CPU Exec"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Other"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}], "rows": [["2", 89610.5302734375, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 223836.5068359375us<br><b>Kernel: 89610.5302734375us</b><br>Percentage: 40.03%</div>", 4047.69482421875, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 223836.5068359375us<br><b>Memcpy: 4047.69482421875us</b><br>Percentage: 1.81%</div>", 106.1728515625, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 223836.5068359375us<br><b>Memset: 106.1728515625us</b><br>Percentage: 0.05%</div>", 550.173828125, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 223836.5068359375us<br><b>Runtime: 550.173828125us</b><br>Percentage: 0.25%</div>", 109079.11767578125, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 223836.5068359375us<br><b>DataLoader: 109079.11767578125us</b><br>Percentage: 48.73%</div>", 10642.8310546875, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 223836.5068359375us<br><b>CPU Exec: 10642.8310546875us</b><br>Percentage: 4.75%</div>", 9799.986328125, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 223836.5068359375us<br><b>Other: 9799.986328125us</b><br>Percentage: 4.38%</div>"], ["2", 0, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 0.0us<br><b>Kernel: 0us</b><br>Percentage: 0.0%</div>", 0, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 0.0us<br><b>Memcpy: 0us</b><br>Percentage: 0.0%</div>", 0, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 0.0us<br><b>Memset: 0us</b><br>Percentage: 0.0%</div>", 0, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 0.0us<br><b>Runtime: 0us</b><br>Percentage: 0.0%</div>", 0.0, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 0.0us<br><b>DataLoader: 0.0us</b><br>Percentage: 0.0%</div>", 0, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 0.0us<br><b>CPU Exec: 0us</b><br>Percentage: 0.0%</div>", 0, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 0.0us<br><b>Other: 0us</b><br>Percentage: 0.0%</div>"], ["3", 89682.69140625, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 3<br>Total: 105888.43505859375us<br><b>Kernel: 89682.69140625us</b><br>Percentage: 84.7%</div>", 4035.47021484375, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 3<br>Total: 105888.43505859375us<br><b>Memcpy: 4035.47021484375us</b><br>Percentage: 3.81%</div>", 105.1181640625, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 3<br>Total: 105888.43505859375us<br><b>Memset: 105.1181640625us</b><br>Percentage: 0.1%</div>", 638.443359375, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 3<br>Total: 105888.43505859375us<br><b>Runtime: 638.443359375us</b><br>Percentage: 0.6%</div>", 10626.11083984375, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 3<br>Total: 105888.43505859375us<br><b>DataLoader: 10626.11083984375us</b><br>Percentage: 10.04%</div>", 544.93359375, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 3<br>Total: 105888.43505859375us<br><b>CPU Exec: 544.93359375us</b><br>Percentage: 0.51%</div>", 255.66748046875, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 3<br>Total: 105888.43505859375us<br><b>Other: 255.66748046875us</b><br>Percentage: 0.24%</div>"]]}, "performance": [{"name": "Average Step Time", "description": "", "value": 109908, "extra": 100, "children": [{"name": "Kernel", "description": "", "value": 59764, "extra": 54.38}, {"name": "Memcpy", "description": "", "value": 2694, "extra": 2.45}, {"name": "Memset", "description": "", "value": 70, "extra": 0.06}, {"name": "Runtime", "description": "", "value": 396, "extra": 0.36}, {"name": "DataLoader", "description": "", "value": 39902, "extra": 36.3}, {"name": "CPU Exec", "description": "", "value": 3729, "extra": 3.39}, {"name": "Other", "description": "", "value": 3352, "extra": 3.05}]}], "recommendations": "<ul><li>This run has high time cost on input data loading. 36.3% of the step time is in DataLoader. You could try to set num_workers on DataLoader's construction and <a href=\"https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading\" target=\"_blank\">enable multi-processes on data loading</a>.</li><li>Kernels with 20% time are launched by Tensor Cores eligible operators. You could enable <a href=\"https://pytorch.org/docs/stable/amp.html\" target=\"_blank\">Automatic Mixed Precision</a> to speedup by using FP16.</li></ul>", "environments": [{"title": "Number of Worker(s)", "value": "1"}, {"title": "Device Type", "value": "GPU"}], "gpu_metrics": {"title": "GPU Summary", "data": [{"title": "GPU 0:", "value": ""}, {"title": "Name", "value": "Tesla V100-SXM2-16GB"}, {"title": "Memory", "value": "15.77 GB"}, {"title": "Compute Capability", "value": "7.0"}, {"title": "GPU Utilization", "value": "54.38 %"}, {"title": "Est. SM Efficiency", "value": "53.7 %"}, {"title": "Est. Achieved Occupancy", "value": "46.79 %"}, {"title": "Kernel Time using Tensor Cores", "value": "0.0 %"}], "tooltip": "The GPU usage metrics:\n\nGPU Utilization:\nGPU busy time / All steps time. The higher, the better. GPU busy time is the time during which there is at least one GPU kernel running on it. All steps time is the total time of all profiler steps(or called as iterations).\n\nEst. SM Efficiency:\nEstimated Stream Multiprocessor Efficiency. The higher, the better. This metric of a kernel, SM_Eff_K = min(blocks of this kernel / SM number of this GPU, 100%). This overall number is the sum of all kernels' SM_Eff_K weighted by kernel's execution duration, divided by all steps time.\n\nEst. Achieved Occupancy:\nFor most cases such as memory bandwidth bounded kernels, the higher the better. Occupancy is the ratio of active warps on an SM to the maximum number of active warps supported by the SM. The theoretical occupancy of a kernel is upper limit occupancy of this kernel, limited by multiple factors such as kernel shape, kernel used resource, and the GPU compute capability.\nEst. Achieved Occupancy of a kernel, OCC_K = min(threads of the kernel / SM number / max threads per SM, theoretical occupancy of the kernel). This overall number is the weighted average of all kernels' OCC_K using kernel's execution duration as weight. It shows fine-grained low-level GPU utilization.\n\nKernel using Tensor Cores:\nTotal GPU Time for Tensor Core kernels / Total GPU Time for all kernels.\n"}}
{"device_total_time": {"title": "Device Total Time (us)", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["autograd::engine::evaluate_function: ConvolutionBackward0", 87021.599609375], ["aten::convolution_backward", 81946.64404296875], ["ConvolutionBackward0", 81946.64404296875], ["aten::cudnn_convolution", 35263.22998046875], ["aten::_convolution", 35263.22998046875], ["aten::convolution", 35263.22998046875], ["aten::conv2d", 35263.22998046875], ["aten::cudnn_batch_norm_backward", 20720.8076171875], ["CudnnBatchNormBackward0", 20720.8076171875], ["autograd::engine::evaluate_function: CudnnBatchNormBackward0", 20720.8076171875], ["aten::cudnn_batch_norm", 11207.10888671875], ["aten::_batch_norm_impl_index", 11207.10888671875], ["aten::batch_norm", 11207.10888671875], ["aten::add_", 10330.02783203125], ["aten::threshold_backward", 8803.47607421875], ["ReluBackward0", 8803.47607421875], ["autograd::engine::evaluate_function: ReluBackward0", 8803.47607421875], ["aten::copy_", 8083.1650390625], ["aten::_to_copy", 8083.1650390625], ["aten::to", 8083.1650390625], ["aten::clamp_min_", 5917.359375], ["aten::relu_", 5917.359375], ["aten::max_pool2d_with_indices_backward", 2411.06103515625], ["MaxPool2DWithIndicesBackward0", 2411.06103515625], ["autograd::engine::evaluate_function: MaxPool2DWithIndicesBackward0", 2411.06103515625], ["aten::_foreach_add_", 1557.40087890625], ["aten::_foreach_mul_", 561.853515625], ["aten::max_pool2d_with_indices", 458.654296875], ["aten::max_pool2d", 458.654296875], ["aten::fill_", 237.0869140625], ["aten::zero_", 234.3349609375], ["autograd::engine::evaluate_function: AddmmBackward0", 105.3056640625], ["aten::mm", 87.353515625], ["AddmmBackward0", 87.353515625], ["aten::mean", 73.34423828125], ["aten::adaptive_avg_pool2d", 73.34423828125], ["aten::addmm", 58.49609375], ["aten::linear", 58.49609375], ["aten::div", 39.72705078125], ["MeanBackward1", 39.72705078125], ["autograd::engine::evaluate_function: MeanBackward1", 39.72705078125], ["aten::cross_entropy_loss", 22.0478515625], ["aten::sum", 17.9521484375], ["aten::_log_softmax_backward_data", 17.05615234375], ["LogSoftmaxBackward0", 17.05615234375], ["autograd::engine::evaluate_function: LogSoftmaxBackward0", 17.05615234375], ["aten::_log_softmax", 15.48779296875], ["aten::log_softmax", 15.48779296875], ["aten::nll_loss_backward", 8.16015625], ["NllLossBackward0", 8.16015625], ["autograd::engine::evaluate_function: NllLossBackward0", 8.16015625], ["aten::nll_loss_forward", 6.56005859375], ["aten::nll_loss", 6.56005859375], ["aten::nll_loss_nd", 6.56005859375], ["aten::ones_like", 2.751953125]]}, "device_self_time": {"title": "Device Self Time (us)", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::convolution_backward", 81946.64404296875], ["aten::cudnn_convolution", 35263.22998046875], ["aten::cudnn_batch_norm_backward", 20720.8076171875], ["aten::cudnn_batch_norm", 11207.10888671875], ["aten::add_", 10330.02783203125], ["aten::threshold_backward", 8803.47607421875], ["aten::copy_", 8083.1650390625], ["aten::clamp_min_", 5917.359375], ["aten::max_pool2d_with_indices_backward", 2179.35009765625], ["aten::_foreach_add_", 1557.40087890625], ["aten::_foreach_mul_", 561.853515625], ["aten::max_pool2d_with_indices", 458.654296875], ["aten::fill_", 237.0869140625], ["aten::mm", 87.353515625], ["aten::mean", 73.34423828125], ["aten::addmm", 58.49609375], ["aten::div", 39.72705078125], ["aten::sum", 17.9521484375], ["aten::_log_softmax_backward_data", 17.05615234375], ["aten::_log_softmax", 15.48779296875], ["aten::nll_loss_forward", 6.56005859375], ["aten::nll_loss_backward", 5.5361328125]]}, "host_total_time": {"title": "Host Total Time (us)", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::contiguous", 21802.5419921875], ["aten::conv2d", 18152.740234375], ["aten::convolution", 17572.0810546875], ["aten::clone", 16721.7177734375], ["aten::_convolution", 16274.34716796875], ["aten::copy_", 15362.36962890625], ["autograd::engine::evaluate_function: ConvolutionBackward0", 15016.95458984375], ["aten::batch_norm", 14660.32763671875], ["aten::_batch_norm_impl_index", 14307.0595703125], ["aten::empty_like", 13759.90771484375], ["aten::to", 13709.76416015625], ["aten::cudnn_batch_norm", 13692.87841796875], ["aten::_to_copy", 13077.66845703125], ["ConvolutionBackward0", 12973.48828125], ["aten::convolution_backward", 12239.326171875], ["aten::stack", 10814.966796875], ["aten::cat", 10734.9541015625], ["autograd::engine::evaluate_function: CudnnBatchNormBackward0", 9008.92724609375], ["aten::empty", 8268.94384765625], ["aten::cudnn_convolution", 7921.34326171875], ["CudnnBatchNormBackward0", 7569.36083984375], ["aten::cudnn_batch_norm_backward", 6943.3857421875], ["aten::relu_", 6541.568359375], ["aten::div", 6328.94140625], ["aten::add_", 4212.48095703125], ["autograd::engine::evaluate_function: ReluBackward0", 3584.9521484375], ["autograd::engine::evaluate_function: torch::autograd::AccumulateGrad", 3434.05810546875], ["ReluBackward0", 2798.390625], ["aten::threshold_backward", 2417.58056640625], ["aten::clamp_min_", 2122.4033203125], ["torch::autograd::AccumulateGrad", 1756.365234375], ["aten::_foreach_add_", 1344.185546875], ["autograd::engine::evaluate_function: MeanBackward1", 1274.31103515625], ["MeanBackward1", 1259.048828125], ["aten::narrow", 1178.84521484375], ["aten::cross_entropy_loss", 1021.8701171875], ["aten::detach", 937.08203125], ["aten::adaptive_avg_pool2d", 934.802734375], ["aten::view", 836.53125], ["aten::permute", 746.59228515625], ["aten::_foreach_mul_", 723.09814453125], ["autograd::engine::evaluate_function: MaxPool2DWithIndicesBackward0", 717.5361328125], ["MaxPool2DWithIndicesBackward0", 694.48681640625], ["aten::empty_strided", 634.40869140625], ["detach", 548.41259765625], ["autograd::engine::evaluate_function: AddmmBackward0", 413.3330078125], ["AddmmBackward0", 284.56005859375], ["aten::linear", 253.2919921875], ["aten::result_type", 239.13232421875], ["autograd::engine::evaluate_function: NllLossBackward0", 233.60498046875], ["autograd::engine::evaluate_function: AddBackward0", 223.6767578125], ["NllLossBackward0", 192.2529296875], ["aten::addmm", 186.6640625], ["aten::as_strided", 186.65869140625], ["aten::mm", 182.35498046875], ["aten::fill_", 179.07861328125], ["aten::nll_loss_backward", 158.4736328125], ["aten::ones_like", 136.26708984375], ["aten::lift_fresh", 132.8720703125], ["aten::zero_", 131.1259765625], ["aten::max_pool2d", 130.923828125], ["aten::nll_loss_nd", 120.64208984375], ["aten::mean", 115.505859375], ["aten::max_pool2d_with_indices", 115.21484375], ["aten::max_pool2d_with_indices_backward", 109.04296875], ["aten::t", 107.46630859375], ["aten::nll_loss", 107.3837890625], ["aten::log_softmax", 106.30517578125], ["aten::nll_loss_forward", 94.23291015625], ["autograd::engine::evaluate_function: LogSoftmaxBackward0", 86.35498046875], ["aten::_log_softmax", 84.421875], ["aten::sum", 71.90673828125], ["LogSoftmaxBackward0", 67.04296875], ["aten::transpose", 57.49462890625], ["aten::_log_softmax_backward_data", 52.251953125], ["AddBackward0", 32.046875], ["aten::flatten", 31.62109375], ["autograd::engine::evaluate_function: ViewBackward0", 29.9716796875], ["autograd::engine::evaluate_function: TBackward0", 29.0859375], ["aten::slice", 23.97607421875], ["ViewBackward0", 18.4501953125], ["aten::expand", 18.162109375], ["aten::detach_", 17.51806640625], ["TBackward0", 17.2529296875], ["aten::reshape", 8.44384765625], ["detach_", 5.23291015625], ["aten::resize_", 1.9501953125]]}, "host_self_time": {"title": "Host Self Time (us)", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::empty_like", 12106.05419921875], ["aten::cat", 9556.10888671875], ["aten::_convolution", 8353.00390625], ["aten::empty", 8268.94384765625], ["aten::convolution_backward", 7203.7744140625], ["aten::copy_", 6631.078125], ["aten::cudnn_batch_norm", 6194.982421875], ["aten::cudnn_convolution", 5237.96875], ["aten::div", 5088.25439453125], ["aten::contiguous", 5080.82421875], ["aten::relu_", 4419.1650390625], ["aten::cudnn_batch_norm_backward", 3092.64404296875], ["aten::add_", 2742.14453125], ["autograd::engine::evaluate_function: torch::autograd::AccumulateGrad", 1677.69287109375], ["aten::threshold_backward", 1652.03125], ["autograd::engine::evaluate_function: CudnnBatchNormBackward0", 1439.56640625], ["autograd::engine::evaluate_function: ConvolutionBackward0", 1421.916015625], ["aten::_to_copy", 1365.7470703125], ["aten::convolution", 1297.73388671875], ["aten::clamp_min_", 1219.16845703125], ["aten::narrow", 1154.869140625], ["MeanBackward1", 1132.5], ["aten::_foreach_add_", 1089.2158203125], ["aten::view", 836.53125], ["aten::adaptive_avg_pool2d", 819.296875], ["torch::autograd::AccumulateGrad", 819.283203125], ["aten::cross_entropy_loss", 794.9228515625], ["autograd::engine::evaluate_function: ReluBackward0", 786.5615234375], ["ConvolutionBackward0", 734.162109375], ["aten::empty_strided", 634.40869140625], ["aten::to", 632.095703125], ["CudnnBatchNormBackward0", 625.97509765625], ["aten::_batch_norm_impl_index", 614.18115234375], ["aten::clone", 595.78515625], ["MaxPool2DWithIndicesBackward0", 585.44384765625], ["aten::permute", 585.06787109375], ["aten::conv2d", 580.6591796875], ["aten::_foreach_mul_", 562.51611328125], ["detach", 548.41259765625], ["aten::detach", 388.66943359375], ["ReluBackward0", 380.81005859375], ["aten::batch_norm", 353.26806640625], ["aten::result_type", 239.13232421875], ["autograd::engine::evaluate_function: AddBackward0", 191.6298828125], ["aten::as_strided", 186.65869140625], ["aten::addmm", 145.7021484375], ["aten::mm", 134.31884765625], ["aten::lift_fresh", 132.8720703125], ["aten::max_pool2d_with_indices", 88.27001953125], ["aten::mean", 87.5107421875], ["aten::fill_", 78.24951171875], ["aten::stack", 72.52197265625], ["aten::nll_loss_forward", 70.36572265625], ["aten::_log_softmax", 60.18994140625], ["aten::nll_loss_backward", 57.3466796875], ["aten::sum", 54.11962890625], ["autograd::engine::evaluate_function: AddmmBackward0", 50.27392578125], ["AddmmBackward0", 50.21875], ["aten::t", 49.9716796875], ["aten::max_pool2d_with_indices_backward", 47.86083984375], ["autograd::engine::evaluate_function: NllLossBackward0", 41.35205078125], ["aten::transpose", 39.4931640625], ["aten::_log_softmax_backward_data", 36.44580078125], ["NllLossBackward0", 33.779296875], ["AddBackward0", 32.046875], ["aten::linear", 24.47412109375], ["autograd::engine::evaluate_function: MaxPool2DWithIndicesBackward0", 23.04931640625], ["aten::zero_", 21.861328125], ["aten::log_softmax", 20.5673828125], ["aten::slice", 19.619140625], ["autograd::engine::evaluate_function: LogSoftmaxBackward0", 19.31201171875], ["aten::ones_like", 16.53125], ["aten::max_pool2d", 15.708984375], ["aten::expand", 15.38623046875], ["autograd::engine::evaluate_function: MeanBackward1", 15.26220703125], ["LogSoftmaxBackward0", 14.791015625], ["aten::nll_loss_nd", 13.25830078125], ["aten::nll_loss", 13.15087890625], ["aten::detach_", 12.28515625], ["autograd::engine::evaluate_function: TBackward0", 11.8330078125], ["autograd::engine::evaluate_function: ViewBackward0", 11.521484375], ["aten::flatten", 11.51513671875], ["ViewBackward0", 10.00634765625], ["detach_", 5.23291015625], ["TBackward0", 3.9267578125], ["aten::reshape", 3.9140625], ["aten::resize_", 1.9501953125]]}}
{"metadata": {"sort": "device_self_duration", "tooltips": {"tc_eligible": "Whether this operator is eligible to use Tensor Cores.", "tc_self_ratio": "Time of self-kernels with Tensor Cores / Time of self-kernels.", "tc_total_ratio": "Time of kernels with Tensor Cores / Time of kernels."}}, "data": [{"name": "aten::convolution_backward", "calls": 106, "device_self_duration": 81947, "device_total_duration": 81947, "host_self_duration": 7204, "host_total_duration": 12239, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::cudnn_convolution", "calls": 106, "device_self_duration": 35263, "device_total_duration": 35263, "host_self_duration": 5238, "host_total_duration": 7921, "tc_eligible": "Yes", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::cudnn_batch_norm_backward", "calls": 106, "device_self_duration": 20721, "device_total_duration": 20721, "host_self_duration": 3093, "host_total_duration": 6943, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::cudnn_batch_norm", "calls": 106, "device_self_duration": 11207, "device_total_duration": 11207, "host_self_duration": 6195, "host_total_duration": 13693, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::add_", "calls": 170, "device_self_duration": 10330, "device_total_duration": 10330, "host_self_duration": 2742, "host_total_duration": 4212, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::threshold_backward", "calls": 98, "device_self_duration": 8803, "device_total_duration": 8803, "host_self_duration": 1652, "host_total_duration": 2418, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::copy_", "calls": 196, "device_self_duration": 8083, "device_total_duration": 8083, "host_self_duration": 6631, "host_total_duration": 15362, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::clamp_min_", "calls": 98, "device_self_duration": 5917, "device_total_duration": 5917, "host_self_duration": 1219, "host_total_duration": 2122, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::max_pool2d_with_indices_backward", "calls": 2, "device_self_duration": 2179, "device_total_duration": 2411, "host_self_duration": 48, "host_total_duration": 109, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_foreach_add_", "calls": 4, "device_self_duration": 1557, "device_total_duration": 1557, "host_self_duration": 1089, "host_total_duration": 1344, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_foreach_mul_", "calls": 2, "device_self_duration": 562, "device_total_duration": 562, "host_self_duration": 563, "host_total_duration": 723, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::max_pool2d_with_indices", "calls": 2, "device_self_duration": 459, "device_total_duration": 459, "host_self_duration": 88, "host_total_duration": 115, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::fill_", "calls": 6, "device_self_duration": 237, "device_total_duration": 237, "host_self_duration": 78, "host_total_duration": 179, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::mm", "calls": 4, "device_self_duration": 87, "device_total_duration": 87, "host_self_duration": 134, "host_total_duration": 182, "tc_eligible": "Yes", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::mean", "calls": 2, "device_self_duration": 73, "device_total_duration": 73, "host_self_duration": 88, "host_total_duration": 116, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::addmm", "calls": 2, "device_self_duration": 58, "device_total_duration": 58, "host_self_duration": 146, "host_total_duration": 187, "tc_eligible": "Yes", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::div", "calls": 66, "device_self_duration": 40, "device_total_duration": 40, "host_self_duration": 5088, "host_total_duration": 6329, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::sum", "calls": 2, "device_self_duration": 18, "device_total_duration": 18, "host_self_duration": 54, "host_total_duration": 72, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_log_softmax_backward_data", "calls": 2, "device_self_duration": 17, "device_total_duration": 17, "host_self_duration": 36, "host_total_duration": 52, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_log_softmax", "calls": 2, "device_self_duration": 15, "device_total_duration": 15, "host_self_duration": 60, "host_total_duration": 84, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::nll_loss_forward", "calls": 2, "device_self_duration": 7, "device_total_duration": 7, "host_self_duration": 70, "host_total_duration": 94, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::nll_loss_backward", "calls": 2, "device_self_duration": 6, "device_total_duration": 8, "host_self_duration": 57, "host_total_duration": 158, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::lift_fresh", "calls": 66, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 133, "host_total_duration": 133, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::view", "calls": 284, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 837, "host_total_duration": 837, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::as_strided", "calls": 78, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 187, "host_total_duration": 187, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::permute", "calls": 64, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 585, "host_total_duration": 747, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::empty", "calls": 1126, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 8269, "host_total_duration": 8269, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::empty_like", "calls": 172, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 12106, "host_total_duration": 13760, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::clone", "calls": 64, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 596, "host_total_duration": 16722, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::contiguous", "calls": 64, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 5081, "host_total_duration": 21803, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::empty_strided", "calls": 134, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 634, "host_total_duration": 634, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::_to_copy", "calls": 132, "device_self_duration": 0, "device_total_duration": 8083, "host_self_duration": 1366, "host_total_duration": 13078, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::to", "calls": 242, "device_self_duration": 0, "device_total_duration": 8083, "host_self_duration": 632, "host_total_duration": 13710, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::slice", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 20, "host_total_duration": 24, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::narrow", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 1155, "host_total_duration": 1179, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::cat", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 9556, "host_total_duration": 10735, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::stack", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 73, "host_total_duration": 10815, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "detach_", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 5, "host_total_duration": 5, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::detach_", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 12, "host_total_duration": 18, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::_convolution", "calls": 106, "device_self_duration": 0, "device_total_duration": 35263, "host_self_duration": 8353, "host_total_duration": 16274, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::convolution", "calls": 106, "device_self_duration": 0, "device_total_duration": 35263, "host_self_duration": 1298, "host_total_duration": 17572, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::conv2d", "calls": 106, "device_self_duration": 0, "device_total_duration": 35263, "host_self_duration": 581, "host_total_duration": 18153, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_batch_norm_impl_index", "calls": 106, "device_self_duration": 0, "device_total_duration": 11207, "host_self_duration": 614, "host_total_duration": 14307, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::batch_norm", "calls": 106, "device_self_duration": 0, "device_total_duration": 11207, "host_self_duration": 353, "host_total_duration": 14660, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::relu_", "calls": 98, "device_self_duration": 0, "device_total_duration": 5917, "host_self_duration": 4419, "host_total_duration": 6542, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::max_pool2d", "calls": 2, "device_self_duration": 0, "device_total_duration": 459, "host_self_duration": 16, "host_total_duration": 131, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::adaptive_avg_pool2d", "calls": 2, "device_self_duration": 0, "device_total_duration": 73, "host_self_duration": 819, "host_total_duration": 935, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::flatten", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 12, "host_total_duration": 32, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::transpose", "calls": 10, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 39, "host_total_duration": 57, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::t", "calls": 10, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 50, "host_total_duration": 107, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::linear", "calls": 2, "device_self_duration": 0, "device_total_duration": 58, "host_self_duration": 24, "host_total_duration": 253, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::log_softmax", "calls": 2, "device_self_duration": 0, "device_total_duration": 15, "host_self_duration": 21, "host_total_duration": 106, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::resize_", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 2, "host_total_duration": 2, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::nll_loss", "calls": 2, "device_self_duration": 0, "device_total_duration": 7, "host_self_duration": 13, "host_total_duration": 107, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::nll_loss_nd", "calls": 2, "device_self_duration": 0, "device_total_duration": 7, "host_self_duration": 13, "host_total_duration": 121, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::cross_entropy_loss", "calls": 2, "device_self_duration": 0, "device_total_duration": 22, "host_self_duration": 795, "host_total_duration": 1022, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::ones_like", "calls": 2, "device_self_duration": 0, "device_total_duration": 3, "host_self_duration": 17, "host_total_duration": 136, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::result_type", "calls": 966, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 239, "host_total_duration": 239, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::zero_", "calls": 4, "device_self_duration": 0, "device_total_duration": 234, "host_self_duration": 22, "host_total_duration": 131, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "NllLossBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 8, "host_self_duration": 34, "host_total_duration": 192, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: NllLossBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 8, "host_self_duration": 41, "host_total_duration": 234, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "LogSoftmaxBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 17, "host_self_duration": 15, "host_total_duration": 67, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: LogSoftmaxBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 17, "host_self_duration": 19, "host_total_duration": 86, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "AddmmBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 87, "host_self_duration": 50, "host_total_duration": 285, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: AddmmBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 105, "host_self_duration": 50, "host_total_duration": 413, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "detach", "calls": 322, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 548, "host_total_duration": 548, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::detach", "calls": 322, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 389, "host_total_duration": 937, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "torch::autograd::AccumulateGrad", "calls": 322, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 819, "host_total_duration": 1756, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: torch::autograd::AccumulateGrad", "calls": 322, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 1678, "host_total_duration": 3434, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "TBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 4, "host_total_duration": 17, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: TBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 12, "host_total_duration": 29, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::reshape", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 4, "host_total_duration": 8, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "ViewBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 10, "host_total_duration": 18, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: ViewBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 12, "host_total_duration": 30, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::expand", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 15, "host_total_duration": 18, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "MeanBackward1", "calls": 2, "device_self_duration": 0, "device_total_duration": 40, "host_self_duration": 1132, "host_total_duration": 1259, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: MeanBackward1", "calls": 2, "device_self_duration": 0, "device_total_duration": 40, "host_self_duration": 15, "host_total_duration": 1274, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "ReluBackward0", "calls": 98, "device_self_duration": 0, "device_total_duration": 8803, "host_self_duration": 381, "host_total_duration": 2798, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: ReluBackward0", "calls": 98, "device_self_duration": 0, "device_total_duration": 8803, "host_self_duration": 787, "host_total_duration": 3585, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "AddBackward0", "calls": 32, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 32, "host_total_duration": 32, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: AddBackward0", "calls": 32, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 192, "host_total_duration": 224, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "CudnnBatchNormBackward0", "calls": 106, "device_self_duration": 0, "device_total_duration": 20721, "host_self_duration": 626, "host_total_duration": 7569, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: CudnnBatchNormBackward0", "calls": 106, "device_self_duration": 0, "device_total_duration": 20721, "host_self_duration": 1440, "host_total_duration": 9009, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "ConvolutionBackward0", "calls": 106, "device_self_duration": 0, "device_total_duration": 81947, "host_self_duration": 734, "host_total_duration": 12973, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: ConvolutionBackward0", "calls": 106, "device_self_duration": 0, "device_total_duration": 87022, "host_self_duration": 1422, "host_total_duration": 15017, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "MaxPool2DWithIndicesBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 2411, "host_self_duration": 585, "host_total_duration": 694, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: MaxPool2DWithIndicesBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 2411, "host_self_duration": 23, "host_total_duration": 718, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}]}
{"metadata": {"sort": "Total Duration (us)"}, "data": {"columns": [{"type": "string", "name": "Name"}, {"type": "string", "name": "Tensor Cores Used", "tooltip": "Whether this kernel uses Tensor Cores."}, {"type": "number", "name": "Calls"}, {"type": "number", "name": "Total Duration (us)"}, {"type": "number", "name": "Mean Duration (us)"}, {"type": "number", "name": "Max Duration (us)"}, {"type": "number", "name": "Min Duration (us)"}, {"type": "number", "name": "Mean Blocks Per SM", "tooltip": "Blocks Per SM = blocks of this kernel / SM number of this GPU.\nIf this number is less than 1, it indicates the GPU multiprocessors are not fully utilized.\n\"Mean Blocks per SM\" is the weighted average of all calls of this kernel, using each call's execution duration as weight."}, {"type": "number", "name": "Mean Est. Achieved Occupancy (%)", "tooltip": "Est. Achieved Occupancy:\nFor most cases such as memory bandwidth bounded kernels, the higher the better. Occupancy is the ratio of active warps on an SM to the maximum number of active warps supported by the SM. The theoretical occupancy of a kernel is upper limit occupancy of this kernel, limited by multiple factors such as kernel shape, kernel used resource, and the GPU compute capability.\nEst. Achieved Occupancy of a kernel, OCC_K = min(threads of the kernel / SM number / max threads per SM, theoretical occupancy of the kernel). This \"Mean\" number is the weighted average of all calls' OCC_K of the kernel, using each call's execution duration as weight. It shows fine-grained low-level GPU utilization."}], "rows": [["volta_sgemm_64x64_nt", "No", 50, 11044, 221, 474, 82, 24.17, 21.45], ["void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)", "No", 64, 10121, 158, 365, 47, 400.82, 100.0], ["void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 128, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)", "No", 22, 9276, 422, 763, 213, 2.42, 57.21], ["void cudnn::detail::dgrad_engine<float, 512, 6, 5, 3, 3, 3, false>(int, int, int, float const*, int, float const*, int, float*, kernel_grad_params, unsigned long long, int, unsigned long long, int, float, int, int, int)", "No", 12, 9196, 766, 980, 591, 75.58, 31.0], ["void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 32, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)", "No", 64, 8826, 138, 362, 46, 6.94, 70.01], ["void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)", "No", 98, 8803, 90, 365, 14, 326.96, 100.0], ["volta_sgemm_32x128_nt", "No", 30, 8536, 285, 519, 145, 19.64, 47.49], ["_5x_cudnn_volta_scudnn_128x128_stridedB_splitK_medium_nn_v1", "No", 18, 7886, 438, 653, 304, 6.63, 25.0], ["_5x_cudnn_volta_scudnn_128x64_stridedB_small_nn_v1", "No", 26, 7099, 273, 299, 248, 5.55, 17.1], ["volta_sgemm_32x128_tn", "No", 24, 7092, 295, 617, 263, 21.71, 48.26], ["_5x_cudnn_volta_scudnn_128x64_relu_medium_nn_v1", "No", 24, 7022, 293, 566, 248, 6.13, 21.01], ["volta_sgemm_64x64_nn", "No", 30, 6951, 232, 477, 87, 37.34, 24.67], ["void wgrad_alg0_engine<float, 128, 6, 7, 3, 3, 5, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", "No", 16, 6376, 399, 488, 192, 5.71, 37.82], ["void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)", "No", 98, 5917, 60, 251, 4, 334.46, 100.0], ["void wgrad_alg0_engine<float, 128, 6, 8, 3, 3, 5, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", "No", 12, 5461, 455, 655, 352, 4.72, 25.0], ["_5x_cudnn_volta_scudnn_128x128_relu_small_nn_v1", "No", 14, 5366, 383, 596, 246, 7.13, 25.0], ["void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 128, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)", "No", 22, 4917, 223, 404, 107, 2.43, 41.2], ["_5x_cudnn_volta_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1", "No", 12, 4006, 334, 338, 331, 22.4, 25.0], ["volta_sgemm_32x128_nn", "No", 14, 3530, 252, 501, 154, 16.0, 50.0], ["_5x_cudnn_volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1", "No", 8, 3396, 424, 619, 279, 3.31, 25.0], ["void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 32, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)", "No", 26, 3060, 118, 217, 50, 5.06, 47.67], ["void precomputed_convolve_sgemm<float, 512, 6, 7, 4, 3, 5, 1, false>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, bool, bool, float const*, float const*, int*)", "No", 8, 3051, 381, 537, 296, 4.96, 25.0], ["volta_sgemm_128x32_nt", "No", 8, 2680, 335, 447, 19, 0.98, 11.55], ["_5x_cudnn_volta_scudnn_128x64_stridedB_splitK_xregs_large_nn_v1", "No", 4, 2460, 615, 616, 613, 15.74, 19.0], ["void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)", "No", 56, 2349, 42, 85, 13, 12.48, 75.0], ["volta_sgemm_64x64_tn", "No", 4, 2310, 578, 582, 574, 1.6, 5.0], ["void at::native::(anonymous namespace)::max_pool_backward_nchw<float, float>(float const*, long const*, int, long, long, long, int, int, int, int, int, int, int, int, int, int, float*)", "No", 2, 2179, 1090, 1090, 1089, 1254.4, 100.0], ["void wgrad_alg0_engine<float, 128, 5, 5, 3, 3, 3, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", "No", 2, 1873, 937, 938, 935, 100.0, 31.0], ["void cudnn::winograd_nonfused::winogradForwardOutput4x4<float, float>(cudnn::winograd_nonfused::WinogradOutputParams<float, float>)", "No", 40, 1714, 43, 71, 20, 10.06, 63.0], ["void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 512, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)", "No", 2, 1679, 840, 841, 838, 0.8, 20.0], ["void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float)", "No", 12, 1557, 130, 251, 29, 2.88, 71.75], ["void cudnn::winograd_nonfused::winogradForwardData4x4<float, float>(cudnn::winograd_nonfused::WinogradDataParams<float, float>)", "No", 40, 1552, 39, 61, 19, 10.05, 50.0], ["void cudnn::winograd_nonfused::winogradWgradDelta4x4<float, float>(cudnn::winograd_nonfused::WinogradDeltaParams<float, float>)", "No", 26, 1551, 60, 130, 18, 15.8, 50.0], ["void cudnn::winograd_nonfused::winogradWgradData4x4<float, float>(cudnn::winograd_nonfused::WinogradDataParams<float, float>)", "No", 26, 1543, 59, 121, 20, 15.38, 38.0], ["_5x_cudnn_volta_scudnn_128x32_relu_medium_nn_v1", "No", 2, 1233, 617, 617, 616, 78.4, 25.0], ["void cudnn::cnn::reduce_wgrad_nchw_helper<float, float>(void*, void const*, float, int, int)", "No", 28, 1090, 39, 49, 4, 24.99, 97.76], ["void cudnn::bn_bw_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_bw_1C11_args<float>)", "No", 18, 939, 52, 89, 24, 20.7, 50.0], ["void cudnn::winograd_nonfused::winogradForwardFilter4x4<float, float>(cudnn::winograd_nonfused::WinogradFilterParams<float, float>)", "No", 40, 882, 22, 65, 5, 8.55, 72.34], ["void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 512, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)", "No", 2, 881, 441, 441, 440, 0.8, 20.0], ["void cudnn::engines_precompiled::scalePackedTensor_kernel<float, float>(long, float*, float)", "No", 12, 772, 64, 127, 21, 598.55, 100.0], ["void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<1>, at::native::(anonymous namespace)::BinaryOpScalarFunctor<float, 1, 1, 0>, std::multiplies<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<1>, at::native::(anonymous namespace)::BinaryOpScalarFunctor<float, 1, 1, 0>, std::multiplies<float>, float)", "No", 6, 562, 94, 196, 20, 3.32, 82.93], ["void cudnn::winograd_nonfused::winogradWgradOutput4x4<float, float>(cudnn::winograd_nonfused::WinogradWgradOutputParams<float, float>)", "No", 26, 484, 19, 65, 5, 7.93, 40.74], ["void at::native::(anonymous namespace)::max_pool_forward_nchw<float, float>(int, float const*, long, long, long, int, int, int, int, int, int, int, int, int, int, float*, long*)", "No", 2, 459, 229, 231, 228, 313.6, 100.0], ["void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)", "No", 12, 413, 34, 66, 9, 65.15, 100.0], ["void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)", "No", 6, 237, 40, 116, 1, 612.99, 97.79], ["void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)", "No", 106, 209, 2, 2, 2, 0.01, 0.0], ["void cask__5x_cudnn::computeOffsetsKernel<false, false>(cask__5x_cudnn::ComputeOffsetsParams)", "No", 48, 103, 2, 3, 2, 0.05, 0.66], ["void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)", "No", 8, 91, 11, 19, 8, 16.73, 100.0], ["void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MeanOps<float, float, float, float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MeanOps<float, float, float, float>, unsigned int, float, 4>)", "No", 2, 73, 37, 37, 37, 51.2, 100.0], ["void cask__5x_cudnn::computeOffsetsKernel<true, false>(cask__5x_cudnn::ComputeOffsetsParams)", "No", 26, 62, 2, 3, 2, 0.01, 0.0], ["cask__5x_cudnn::computeBOffsetsKernel(cask__5x_cudnn::ComputeBOffsetsParams)", "No", 26, 54, 2, 3, 2, 0.03, 0.0], ["void cudnn::winograd::generateWinogradTilesKernel<0, float, float>(cudnn::winograd::GenerateWinogradTilesParams<float, float>)", "No", 12, 51, 4, 5, 3, 0.4, 3.0], ["volta_sgemm_64x32_sliced1x4_tn", "No", 2, 49, 25, 25, 24, 1.0, 13.0], ["cask__5x_cudnn::computeWgradSplitKOffsetsKernel(cask__5x_cudnn::ComputeSplitKOffsetsParams)", "No", 22, 43, 2, 2, 2, 0.14, 1.57], ["cask__5x_cudnn::computeWgradBOffsetsKernel(cask__5x_cudnn::ComputeWgradBOffsetsParams)", "No", 22, 42, 2, 2, 2, 0.03, 0.0], ["volta_sgemm_64x32_sliced1x4_nn", "No", 2, 41, 21, 21, 20, 2.0, 25.0], ["void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})", "No", 2, 40, 20, 20, 20, 156.8, 100.0], ["void cudnn::cnn::kern_precompute_indices<false>(int*, int, int, int, int, int)", "No", 8, 20, 3, 3, 2, 8.52, 0.33], ["void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)", "No", 2, 18, 9, 9, 9, 0.03, 0.0], ["void (anonymous namespace)::softmax_warp_backward<float, float, float, 10, true, false>(float*, float const*, float const*, int, int, int, bool const*)", "No", 2, 17, 9, 9, 8, 0.1, 1.0], ["void (anonymous namespace)::softmax_warp_forward<float, float, float, 10, true, false>(float*, float const*, int, int, int, bool const*, int, bool)", "No", 2, 15, 8, 8, 8, 0.1, 1.0], ["void splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)", "No", 2, 9, 5, 5, 5, 0.8, 20.0], ["void splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)", "No", 2, 8, 4, 4, 4, 1.6, 40.0], ["void at::native::(anonymous namespace)::nll_loss_forward_reduce_cuda_kernel_2d<float, float, long>(float*, float*, float const*, long const*, float const*, bool, long, long, long, long)", "No", 2, 7, 3, 3, 3, 0.01, 0.0], ["void at::native::(anonymous namespace)::nll_loss_backward_reduce_cuda_kernel_2d<float, long>(float*, float const*, long const*, float const*, float const*, bool, int, int, long, long)", "No", 2, 6, 3, 3, 3, 0.01, 0.0]]}}
{"total": {"columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["volta_sgemm_64x64_nt", 11043.791], ["void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)", 10121.009], ["void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 128, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)", 9276.215], ["void cudnn::detail::dgrad_engine<float, 512, 6, 5, 3, 3, 3, false>(int, int, int, float const*, int, float const*, int, float*, kernel_grad_params, unsigned long long, int, unsigned long long, int, float, int, int, int)", 9195.865], ["void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 32, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)", 8826.107], ["void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)", 8803.476], ["volta_sgemm_32x128_nt", 8536.283], ["_5x_cudnn_volta_scudnn_128x128_stridedB_splitK_medium_nn_v1", 7886.141], ["_5x_cudnn_volta_scudnn_128x64_stridedB_small_nn_v1", 7098.6539999999995], ["volta_sgemm_32x128_tn", 7091.62], ["_5x_cudnn_volta_scudnn_128x64_relu_medium_nn_v1", 7021.534], ["volta_sgemm_64x64_nn", 6950.819], ["void wgrad_alg0_engine<float, 128, 6, 7, 3, 3, 5, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 6376.07], ["void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)", 5917.358], ["void wgrad_alg0_engine<float, 128, 6, 8, 3, 3, 5, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 5460.651], ["_5x_cudnn_volta_scudnn_128x128_relu_small_nn_v1", 5366.439], ["void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 128, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)", 4916.874], ["_5x_cudnn_volta_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1", 4006.191], ["volta_sgemm_32x128_nn", 3529.617], ["_5x_cudnn_volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1", 3395.986], ["void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 32, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)", 3059.666], ["void precomputed_convolve_sgemm<float, 512, 6, 7, 4, 3, 5, 1, false>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, bool, bool, float const*, float const*, int*)", 3050.868], ["volta_sgemm_128x32_nt", 2680.406], ["_5x_cudnn_volta_scudnn_128x64_stridedB_splitK_xregs_large_nn_v1", 2459.6369999999997], ["void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)", 2349.364], ["volta_sgemm_64x64_tn", 2310.2619999999997], ["void at::native::(anonymous namespace)::max_pool_backward_nchw<float, float>(float const*, long const*, int, long, long, long, int, int, int, int, int, int, int, int, int, int, float*)", 2179.3500000000004], ["void wgrad_alg0_engine<float, 128, 5, 5, 3, 3, 3, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 1873.304], ["void cudnn::winograd_nonfused::winogradForwardOutput4x4<float, float>(cudnn::winograd_nonfused::WinogradOutputParams<float, float>)", 1713.749], ["void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 512, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)", 1679.194], ["void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float)", 1557.4], ["void cudnn::winograd_nonfused::winogradForwardData4x4<float, float>(cudnn::winograd_nonfused::WinogradDataParams<float, float>)", 1552.248], ["void cudnn::winograd_nonfused::winogradWgradDelta4x4<float, float>(cudnn::winograd_nonfused::WinogradDeltaParams<float, float>)", 1550.585], ["void cudnn::winograd_nonfused::winogradWgradData4x4<float, float>(cudnn::winograd_nonfused::WinogradDataParams<float, float>)", 1543.256], ["_5x_cudnn_volta_scudnn_128x32_relu_medium_nn_v1", 1233.051], ["void cudnn::cnn::reduce_wgrad_nchw_helper<float, float>(void*, void const*, float, int, int)", 1090.426], ["void cudnn::bn_bw_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_bw_1C11_args<float>)", 939.292], ["void cudnn::winograd_nonfused::winogradForwardFilter4x4<float, float>(cudnn::winograd_nonfused::WinogradFilterParams<float, float>)", 882.107], ["void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 512, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)", 881.205], ["void cudnn::engines_precompiled::scalePackedTensor_kernel<float, float>(long, float*, float)", 772.35], ["void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<1>, at::native::(anonymous namespace)::BinaryOpScalarFunctor<float, 1, 1, 0>, std::multiplies<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<1>, at::native::(anonymous namespace)::BinaryOpScalarFunctor<float, 1, 1, 0>, std::multiplies<float>, float)", 561.854], ["void cudnn::winograd_nonfused::winogradWgradOutput4x4<float, float>(cudnn::winograd_nonfused::WinogradWgradOutputParams<float, float>)", 484.381], ["void at::native::(anonymous namespace)::max_pool_forward_nchw<float, float>(int, float const*, long, long, long, int, int, int, int, int, int, int, int, int, int, float*, long*)", 458.654], ["void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)", 412.99], ["void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)", 237.087], ["void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)", 209.018], ["void cask__5x_cudnn::computeOffsetsKernel<false, false>(cask__5x_cudnn::ComputeOffsetsParams)", 102.812], ["void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)", 90.816], ["void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MeanOps<float, float, float, float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MeanOps<float, float, float, float>, unsigned int, float, 4>)", 73.344], ["void cask__5x_cudnn::computeOffsetsKernel<true, false>(cask__5x_cudnn::ComputeOffsetsParams)", 62.496], ["cask__5x_cudnn::computeBOffsetsKernel(cask__5x_cudnn::ComputeBOffsetsParams)", 53.696], ["void cudnn::winograd::generateWinogradTilesKernel<0, float, float>(cudnn::winograd::GenerateWinogradTilesParams<float, float>)", 50.944], ["volta_sgemm_64x32_sliced1x4_tn", 49.088], ["cask__5x_cudnn::computeWgradSplitKOffsetsKernel(cask__5x_cudnn::ComputeSplitKOffsetsParams)", 43.459], ["cask__5x_cudnn::computeWgradBOffsetsKernel(cask__5x_cudnn::ComputeWgradBOffsetsParams)", 42.493], ["volta_sgemm_64x32_sliced1x4_nn", 41.400000000000006], ["void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})", 39.727000000000004], ["void cudnn::cnn::kern_precompute_indices<false>(int*, int, int, int, int, int)", 20.381999999999998], ["void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)", 17.951999999999998], ["void (anonymous namespace)::softmax_warp_backward<float, float, float, 10, true, false>(float*, float const*, float const*, int, int, int, bool const*)", 17.056], ["void (anonymous namespace)::softmax_warp_forward<float, float, float, 10, true, false>(float*, float const*, int, int, int, bool const*, int, bool)", 15.488], ["void splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)", 9.408], ["void splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)", 8.16], ["void at::native::(anonymous namespace)::nll_loss_forward_reduce_cuda_kernel_2d<float, float, long>(float*, float*, float const*, long const*, float const*, bool, long, long, long, long)", 6.56], ["void at::native::(anonymous namespace)::nll_loss_backward_reduce_cuda_kernel_2d<float, long>(float*, float const*, long const*, float const*, float const*, bool, int, int, long, long)", 5.536]]}}
{"steps": {"columns": [{"type": "string", "name": "Step"}, {"type": "number", "name": "Kernel"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Memcpy"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Memset"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Runtime"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "DataLoader"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "CPU Exec"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Other"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}], "rows": [["2", 28286.59130859375, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 277323.27099609375us<br><b>Kernel: 28286.59130859375us</b><br>Percentage: 10.2%</div>", 924.2568359375, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 277323.27099609375us<br><b>Memcpy: 924.2568359375us</b><br>Percentage: 0.33%</div>", 0, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 277323.27099609375us<br><b>Memset: 0us</b><br>Percentage: 0.0%</div>", 4029.53564453125, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 277323.27099609375us<br><b>Runtime: 4029.53564453125us</b><br>Percentage: 1.45%</div>", 189608.14794921875, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 277323.27099609375us<br><b>DataLoader: 189608.14794921875us</b><br>Percentage: 68.37%</div>", 34669.44775390625, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 277323.27099609375us<br><b>CPU Exec: 34669.44775390625us</b><br>Percentage: 12.5%</div>", 19805.29150390625, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 277323.27099609375us<br><b>Other: 19805.29150390625us</b><br>Percentage: 7.14%</div>"], ["2", 0, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 0.0us<br><b>Kernel: 0us</b><br>Percentage: 0.0%</div>", 0, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 0.0us<br><b>Memcpy: 0us</b><br>Percentage: 0.0%</div>", 0, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 0.0us<br><b>Memset: 0us</b><br>Percentage: 0.0%</div>", 0, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 0.0us<br><b>Runtime: 0us</b><br>Percentage: 0.0%</div>", 0, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 0.0us<br><b>DataLoader: 0us</b><br>Percentage: 0.0%</div>", 0, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 0.0us<br><b>CPU Exec: 0us</b><br>Percentage: 0.0%</div>", 0, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 0.0us<br><b>Other: 0us</b><br>Percentage: 0.0%</div>"], ["3", 28273.80224609375, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 3<br>Total: 94005.59619140625us<br><b>Kernel: 28273.80224609375us</b><br>Percentage: 30.08%</div>", 1259.30078125, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 3<br>Total: 94005.59619140625us<br><b>Memcpy: 1259.30078125us</b><br>Percentage: 1.34%</div>", 0, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 3<br>Total: 94005.59619140625us<br><b>Memset: 0us</b><br>Percentage: 0.0%</div>", 2895.59716796875, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 3<br>Total: 94005.59619140625us<br><b>Runtime: 2895.59716796875us</b><br>Percentage: 3.08%</div>", 44901.34814453125, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 3<br>Total: 94005.59619140625us<br><b>DataLoader: 44901.34814453125us</b><br>Percentage: 47.76%</div>", 10916.18212890625, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 3<br>Total: 94005.59619140625us<br><b>CPU Exec: 10916.18212890625us</b><br>Percentage: 11.61%</div>", 5759.36572265625, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 3<br>Total: 94005.59619140625us<br><b>Other: 5759.36572265625us</b><br>Percentage: 6.13%</div>"]]}, "performance": [{"name": "Average Step Time", "description": "", "value": 123776, "extra": 100, "children": [{"name": "Kernel", "description": "", "value": 18853, "extra": 15.23}, {"name": "Memcpy", "description": "", "value": 728, "extra": 0.59}, {"name": "Memset", "description": "", "value": 0, "extra": 0.0}, {"name": "Runtime", "description": "", "value": 2308, "extra": 1.86}, {"name": "DataLoader", "description": "", "value": 78170, "extra": 63.15}, {"name": "CPU Exec", "description": "", "value": 15195, "extra": 12.28}, {"name": "Other", "description": "", "value": 8522, "extra": 6.88}]}], "recommendations": "<ul><li>This run has high time cost on input data loading. 63.2% of the step time is in DataLoader. You could try to set num_workers on DataLoader's construction and <a href=\"https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading\" target=\"_blank\">enable multi-processes on data loading</a>.</li><li>MLU 0 has low utilization. You could try to increase batch size to improve. Note: Increasing batch size may affect the speed and stability of model convergence.</li></ul>", "environments": [{"title": "Number of Worker(s)", "value": "1"}, {"title": "Device Type", "value": "MLU"}], "gpu_metrics": {"title": "MLU Summary", "data": [{"title": "MLU 0:", "value": ""}, {"title": "MLU Utilization", "value": "15.23 %"}], "tooltip": "The MLU usage metrics:\n\nMLU Utilization:\nMLU busy time / All steps time. The higher, the better. MLU busy time is the time during which there is at least one MLU kernel running on it. All steps time is the total time of all profiler steps(or called as iterations).\n"}}
{"device_total_time": {"title": "Device Total Time (us)", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["autograd::engine::evaluate_function: ConvolutionBackward0", 19930.80126953125], ["aten::convolution_backward", 17361.1630859375], ["ConvolutionBackward0", 17361.1630859375], ["aten::native_batch_norm_backward", 8411.0390625], ["NativeBatchNormBackward0", 8411.0390625], ["autograd::engine::evaluate_function: NativeBatchNormBackward0", 8411.0390625], ["aten::_convolution", 7842.99658203125], ["aten::convolution", 7842.99658203125], ["aten::conv2d", 7842.99658203125], ["aten::threshold_backward", 7603.9970703125], ["aten::add_", 7528.732421875], ["aten::native_batch_norm", 5785.9169921875], ["aten::_batch_norm_impl_index", 5785.9169921875], ["aten::batch_norm", 5785.9169921875], ["ReluBackward0", 3801.99853515625], ["autograd::engine::evaluate_function: ReluBackward0", 3801.99853515625], ["aten::clamp_min_", 3237.96337890625], ["aten::relu_", 3237.96337890625], ["aten::copy_", 2416.51806640625], ["aten::_to_copy", 2183.5576171875], ["aten::to", 2183.5576171875], ["aten::mul_", 965.98046875], ["aten::max_pool2d_with_indices_backward", 588.56005859375], ["MaxPool2DWithIndicesBackward0", 588.56005859375], ["autograd::engine::evaluate_function: MaxPool2DWithIndicesBackward0", 588.56005859375], ["aten::max_pool2d_with_indices", 487.76025390625], ["aten::max_pool2d", 487.76025390625], ["torch::autograd::AccumulateGrad", 226.96044921875], ["autograd::engine::evaluate_function: torch::autograd::AccumulateGrad", 226.96044921875], ["aten::mean", 69.56005859375], ["aten::adaptive_avg_pool2d", 69.56005859375], ["aten::div", 60.48046875], ["MeanBackward1", 60.48046875], ["autograd::engine::evaluate_function: MeanBackward1", 60.48046875], ["autograd::engine::evaluate_function: AddmmBackward0", 57.88037109375], ["aten::cross_entropy_loss", 47.2802734375], ["aten::mm", 46.56005859375], ["AddmmBackward0", 46.56005859375], ["aten::nll_loss_forward", 28.9599609375], ["aten::nll_loss", 28.9599609375], ["aten::nll_loss_nd", 28.9599609375], ["aten::_log_softmax_backward_data", 28.080078125], ["LogSoftmaxBackward0", 28.080078125], ["autograd::engine::evaluate_function: LogSoftmaxBackward0", 28.080078125], ["aten::addmm", 27.16015625], ["aten::linear", 27.16015625], ["aten::_log_softmax", 18.3203125], ["aten::log_softmax", 18.3203125], ["aten::nll_loss_backward", 18.24072265625], ["NllLossBackward0", 18.24072265625], ["autograd::engine::evaluate_function: NllLossBackward0", 18.24072265625], ["aten::sum", 11.3203125], ["AsStridedBackward1", 10.080078125], ["autograd::engine::evaluate_function: AsStridedBackward1", 10.080078125], ["aten::zero_", 7.4404296875], ["aten::new_zeros", 4.080078125], ["aten::fill_", 2.56005859375], ["aten::ones_like", 2.56005859375]]}, "device_self_time": {"title": "Device Self Time (us)", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::convolution_backward", 17361.1630859375], ["aten::native_batch_norm_backward", 8411.0390625], ["aten::_convolution", 7842.99658203125], ["aten::add_", 7528.732421875], ["aten::native_batch_norm", 5785.9169921875], ["aten::threshold_backward", 3801.99853515625], ["aten::clamp_min_", 3237.96337890625], ["aten::copy_", 2416.51806640625], ["aten::mul_", 965.98046875], ["aten::max_pool2d_with_indices_backward", 588.56005859375], ["aten::max_pool2d_with_indices", 487.76025390625], ["aten::mean", 69.56005859375], ["aten::div", 60.48046875], ["aten::mm", 46.56005859375], ["aten::nll_loss_forward", 28.9599609375], ["aten::_log_softmax_backward_data", 28.080078125], ["aten::addmm", 27.16015625], ["aten::_log_softmax", 18.3203125], ["aten::nll_loss_backward", 14.88037109375], ["aten::sum", 11.3203125], ["aten::zero_", 7.4404296875], ["aten::fill_", 2.56005859375]]}, "host_total_time": {"title": "Host Total Time (us)", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::copy_", 73647.21435546875], ["aten::contiguous", 56833.20654296875], ["aten::clone", 47689.93994140625], ["aten::to", 37703.6669921875], ["aten::_to_copy", 37401.15087890625], ["aten::div", 31028.4482421875], ["aten::stack", 22304.9697265625], ["aten::cat", 22241.2080078125], ["autograd::engine::evaluate_function: MeanBackward1", 16676.4521484375], ["MeanBackward1", 16645.65234375], ["autograd::engine::evaluate_function: ConvolutionBackward0", 15631.041015625], ["ConvolutionBackward0", 13790.26806640625], ["aten::empty_like", 13414.17822265625], ["aten::convolution_backward", 13017.3818359375], ["aten::narrow", 12902.005859375], ["aten::add_", 12292.1943359375], ["autograd::engine::evaluate_function: MaxPool2DWithIndicesBackward0", 8196.42431640625], ["MaxPool2DWithIndicesBackward0", 8177.669921875], ["aten::conv2d", 7266.9716796875], ["aten::convolution", 6802.046875], ["aten::_convolution", 5856.416015625], ["autograd::engine::evaluate_function: NativeBatchNormBackward0", 5735.53271484375], ["aten::batch_norm", 5096.54833984375], ["aten::_batch_norm_impl_index", 4795.0869140625], ["NativeBatchNormBackward0", 4355.966796875], ["autograd::engine::evaluate_function: torch::autograd::AccumulateGrad", 4186.3544921875], ["aten::native_batch_norm", 4083.201171875], ["aten::threshold_backward", 3942.32568359375], ["aten::native_batch_norm_backward", 3679.73388671875], ["autograd::engine::evaluate_function: ReluBackward0", 3677.966796875], ["aten::empty", 3389.41357421875], ["aten::mul_", 3331.65087890625], ["torch::autograd::AccumulateGrad", 3177.400390625], ["aten::relu_", 3076.40283203125], ["ReluBackward0", 2956.18212890625], ["aten::clamp_min_", 1618.61474609375], ["aten::detach", 1012.1572265625], ["aten::empty_strided", 902.41845703125], ["aten::adaptive_avg_pool2d", 882.34619140625], ["aten::cross_entropy_loss", 824.02099609375], ["autograd::engine::evaluate_function: AddmmBackward0", 701.76904296875], ["detach", 611.1904296875], ["aten::permute", 482.23828125], ["autograd::engine::evaluate_function: NllLossBackward0", 444.84130859375], ["AddmmBackward0", 401.70703125], ["NllLossBackward0", 368.2861328125], ["aten::item", 325.4130859375], ["autograd::engine::evaluate_function: AsStridedBackward1", 323.958984375], ["aten::view", 317.7958984375], ["aten::nll_loss_backward", 312.94287109375], ["AsStridedBackward1", 302.93896484375], ["aten::mm", 254.734375], ["autograd::engine::evaluate_function: AddBackward0", 226.521484375], ["aten::mean", 211.27294921875], ["aten::new_empty_strided", 209.7783203125], ["aten::linear", 198.52880859375], ["aten::t", 198.44140625], ["aten::zero_", 176.3974609375], ["aten::sum", 170.39794921875], ["aten::as_strided", 165.33154296875], ["aten::ones_like", 149.88818359375], ["aten::addmm", 147.212890625], ["aten::nll_loss_nd", 126.01025390625], ["aten::nll_loss", 116.43359375], ["aten::nll_loss_forward", 107.5791015625], ["autograd::engine::evaluate_function: LogSoftmaxBackward0", 106.69091796875], ["aten::transpose", 106.32861328125], ["aten::new_zeros", 91.4658203125], ["aten::max_pool2d", 91.2373046875], ["aten::lift_fresh", 90.6689453125], ["LogSoftmaxBackward0", 84.1357421875], ["aten::max_pool2d_with_indices", 80.302734375], ["aten::max_pool2d_with_indices_backward", 79.30615234375], ["aten::log_softmax", 78.2451171875], ["aten::fill_", 67.93310546875], ["aten::_log_softmax_backward_data", 62.7236328125], ["aten::expand", 60.078125], ["aten::_log_softmax", 60.04638671875], ["aten::_local_scalar_dense", 53.34912109375], ["AddBackward0", 38.18603515625], ["autograd::engine::evaluate_function: TBackward0", 35.708984375], ["autograd::engine::evaluate_function: ViewBackward0", 34.8759765625], ["aten::squeeze", 31.42333984375], ["aten::flatten", 30.06103515625], ["ViewBackward0", 24.50927734375], ["TBackward0", 24.40380859375], ["aten::as_strided_", 23.1923828125], ["aten::new_empty", 23.06396484375], ["aten::slice", 17.345703125], ["aten::detach_", 14.5029296875], ["aten::reshape", 13.666015625], ["aten::unsqueeze", 8.04248046875], ["detach_", 5.18896484375]]}, "host_self_time": {"title": "Host Self Time (us)", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::copy_", 70822.62060546875], ["aten::div", 30276.31884765625], ["MeanBackward1", 15993.7939453125], ["aten::narrow", 12884.66015625], ["aten::empty_like", 11846.625], ["aten::cat", 9339.2021484375], ["aten::contiguous", 9143.2666015625], ["aten::add_", 8581.07861328125], ["MaxPool2DWithIndicesBackward0", 8098.36376953125], ["aten::convolution_backward", 7444.14697265625], ["aten::empty", 3389.41357421875], ["aten::_convolution", 3141.1669921875], ["aten::native_batch_norm", 2214.9697265625], ["aten::native_batch_norm_backward", 1750.14990234375], ["aten::mul_", 1725.26806640625], ["aten::threshold_backward", 1532.4501953125], ["aten::relu_", 1457.7880859375], ["autograd::engine::evaluate_function: NativeBatchNormBackward0", 1379.56591796875], ["autograd::engine::evaluate_function: ConvolutionBackward0", 1186.7587890625], ["autograd::engine::evaluate_function: torch::autograd::AccumulateGrad", 1008.9541015625], ["aten::convolution", 945.630859375], ["aten::clamp_min_", 916.5361328125], ["aten::empty_strided", 902.41845703125], ["torch::autograd::AccumulateGrad", 872.0625], ["ConvolutionBackward0", 772.88623046875], ["aten::_to_copy", 769.6904296875], ["autograd::engine::evaluate_function: ReluBackward0", 721.78466796875], ["NativeBatchNormBackward0", 676.23291015625], ["aten::adaptive_avg_pool2d", 647.880859375], ["aten::cross_entropy_loss", 619.765625], ["detach", 611.1904296875], ["aten::_batch_norm_impl_index", 549.654296875], ["aten::conv2d", 464.9248046875], ["ReluBackward0", 402.8056640625], ["aten::detach", 400.966796875], ["aten::permute", 386.970703125], ["aten::clone", 355.84521484375], ["aten::view", 317.7958984375], ["aten::to", 302.51611328125], ["aten::batch_norm", 301.46142578125], ["aten::item", 272.06396484375], ["autograd::engine::evaluate_function: AddBackward0", 188.33544921875], ["aten::as_strided", 165.33154296875], ["aten::mm", 136.40380859375], ["aten::mean", 135.7255859375], ["aten::sum", 129.89599609375], ["autograd::engine::evaluate_function: AddmmBackward0", 113.47802734375], ["aten::new_empty_strided", 111.80517578125], ["aten::nll_loss_backward", 102.8662109375], ["aten::addmm", 92.25830078125], ["aten::t", 92.11279296875], ["aten::lift_fresh", 90.6689453125], ["aten::transpose", 78.75830078125], ["autograd::engine::evaluate_function: NllLossBackward0", 76.55517578125], ["AsStridedBackward1", 70.12451171875], ["aten::max_pool2d_with_indices", 57.376953125], ["aten::stack", 56.8427734375], ["aten::zero_", 56.40966796875], ["NllLossBackward0", 55.34326171875], ["aten::_local_scalar_dense", 53.34912109375], ["aten::max_pool2d_with_indices_backward", 48.52685546875], ["aten::nll_loss_forward", 48.05712890625], ["aten::expand", 47.83740234375], ["AddmmBackward0", 43.89208984375], ["AddBackward0", 38.18603515625], ["aten::_log_softmax_backward_data", 35.9189453125], ["aten::_log_softmax", 35.30859375], ["autograd::engine::evaluate_function: MeanBackward1", 30.7998046875], ["aten::squeeze", 26.74755859375], ["aten::as_strided_", 23.1923828125], ["autograd::engine::evaluate_function: LogSoftmaxBackward0", 22.55517578125], ["LogSoftmaxBackward0", 21.412109375], ["autograd::engine::evaluate_function: AsStridedBackward1", 21.02001953125], ["aten::new_zeros", 20.82666015625], ["aten::fill_", 20.69091796875], ["autograd::engine::evaluate_function: MaxPool2DWithIndicesBackward0", 18.75439453125], ["aten::linear", 18.3388671875], ["aten::log_softmax", 16.60302734375], ["aten::ones_like", 14.44140625], ["aten::slice", 13.07568359375], ["autograd::engine::evaluate_function: TBackward0", 11.30517578125], ["aten::max_pool2d", 10.9345703125], ["ViewBackward0", 10.84326171875], ["autograd::engine::evaluate_function: ViewBackward0", 10.36669921875], ["aten::nll_loss_nd", 9.57666015625], ["aten::detach_", 9.31396484375], ["aten::flatten", 9.29150390625], ["aten::new_empty", 9.15380859375], ["aten::nll_loss", 8.8544921875], ["aten::unsqueeze", 6.8955078125], ["aten::reshape", 6.462890625], ["detach_", 5.18896484375], ["TBackward0", 5.146484375]]}}
{"metadata": {"sort": "device_self_duration", "tooltips": {"tc_eligible": "Whether this operator is eligible to use Tensor Cores.", "tc_self_ratio": "Time of self-kernels with Tensor Cores / Time of self-kernels.", "tc_total_ratio": "Time of kernels with Tensor Cores / Time of kernels."}}, "data": [{"name": "aten::convolution_backward", "calls": 106, "device_self_duration": 17361, "device_total_duration": 17361, "host_self_duration": 7444, "host_total_duration": 13017, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::native_batch_norm_backward", "calls": 106, "device_self_duration": 8411, "device_total_duration": 8411, "host_self_duration": 1750, "host_total_duration": 3680, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_convolution", "calls": 106, "device_self_duration": 7843, "device_total_duration": 7843, "host_self_duration": 3141, "host_total_duration": 5856, "tc_eligible": "Yes", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::add_", "calls": 814, "device_self_duration": 7529, "device_total_duration": 7529, "host_self_duration": 8581, "host_total_duration": 12292, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::native_batch_norm", "calls": 106, "device_self_duration": 5786, "device_total_duration": 5786, "host_self_duration": 2215, "host_total_duration": 4083, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::threshold_backward", "calls": 196, "device_self_duration": 3802, "device_total_duration": 7604, "host_self_duration": 1532, "host_total_duration": 3942, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::clamp_min_", "calls": 98, "device_self_duration": 3238, "device_total_duration": 3238, "host_self_duration": 917, "host_total_duration": 1619, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::copy_", "calls": 232, "device_self_duration": 2417, "device_total_duration": 2417, "host_self_duration": 70823, "host_total_duration": 73647, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::mul_", "calls": 322, "device_self_duration": 966, "device_total_duration": 966, "host_self_duration": 1725, "host_total_duration": 3332, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::max_pool2d_with_indices_backward", "calls": 2, "device_self_duration": 589, "device_total_duration": 589, "host_self_duration": 49, "host_total_duration": 79, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::max_pool2d_with_indices", "calls": 2, "device_self_duration": 488, "device_total_duration": 488, "host_self_duration": 57, "host_total_duration": 80, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::mean", "calls": 2, "device_self_duration": 70, "device_total_duration": 70, "host_self_duration": 136, "host_total_duration": 211, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::div", "calls": 66, "device_self_duration": 60, "device_total_duration": 60, "host_self_duration": 30276, "host_total_duration": 31028, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::mm", "calls": 4, "device_self_duration": 47, "device_total_duration": 47, "host_self_duration": 136, "host_total_duration": 255, "tc_eligible": "Yes", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::nll_loss_forward", "calls": 2, "device_self_duration": 29, "device_total_duration": 29, "host_self_duration": 48, "host_total_duration": 108, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_log_softmax_backward_data", "calls": 2, "device_self_duration": 28, "device_total_duration": 28, "host_self_duration": 36, "host_total_duration": 63, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::addmm", "calls": 2, "device_self_duration": 27, "device_total_duration": 27, "host_self_duration": 92, "host_total_duration": 147, "tc_eligible": "Yes", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_log_softmax", "calls": 2, "device_self_duration": 18, "device_total_duration": 18, "host_self_duration": 35, "host_total_duration": 60, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::nll_loss_backward", "calls": 2, "device_self_duration": 15, "device_total_duration": 18, "host_self_duration": 103, "host_total_duration": 313, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::sum", "calls": 2, "device_self_duration": 11, "device_total_duration": 11, "host_self_duration": 130, "host_total_duration": 170, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::zero_", "calls": 4, "device_self_duration": 7, "device_total_duration": 7, "host_self_duration": 56, "host_total_duration": 176, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::fill_", "calls": 2, "device_self_duration": 3, "device_total_duration": 3, "host_self_duration": 21, "host_total_duration": 68, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::lift_fresh", "calls": 66, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 91, "host_total_duration": 91, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::view", "calls": 72, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 318, "host_total_duration": 318, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::as_strided", "calls": 96, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 165, "host_total_duration": 165, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::permute", "calls": 64, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 387, "host_total_duration": 482, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::empty", "calls": 1196, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 3389, "host_total_duration": 3389, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::empty_like", "calls": 488, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 11847, "host_total_duration": 13414, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::clone", "calls": 64, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 356, "host_total_duration": 47690, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::contiguous", "calls": 64, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 9143, "host_total_duration": 56833, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::empty_strided", "calls": 274, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 902, "host_total_duration": 902, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::_to_copy", "calls": 132, "device_self_duration": 0, "device_total_duration": 2184, "host_self_duration": 770, "host_total_duration": 37401, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::to", "calls": 136, "device_self_duration": 0, "device_total_duration": 2184, "host_self_duration": 303, "host_total_duration": 37704, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::slice", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 13, "host_total_duration": 17, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::narrow", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 12885, "host_total_duration": 12902, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::cat", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 9339, "host_total_duration": 22241, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::stack", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 57, "host_total_duration": 22305, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "detach_", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 5, "host_total_duration": 5, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::detach_", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 9, "host_total_duration": 15, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::convolution", "calls": 106, "device_self_duration": 0, "device_total_duration": 7843, "host_self_duration": 946, "host_total_duration": 6802, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::conv2d", "calls": 106, "device_self_duration": 0, "device_total_duration": 7843, "host_self_duration": 465, "host_total_duration": 7267, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_batch_norm_impl_index", "calls": 106, "device_self_duration": 0, "device_total_duration": 5786, "host_self_duration": 550, "host_total_duration": 4795, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::batch_norm", "calls": 106, "device_self_duration": 0, "device_total_duration": 5786, "host_self_duration": 301, "host_total_duration": 5097, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::relu_", "calls": 98, "device_self_duration": 0, "device_total_duration": 3238, "host_self_duration": 1458, "host_total_duration": 3076, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::max_pool2d", "calls": 2, "device_self_duration": 0, "device_total_duration": 488, "host_self_duration": 11, "host_total_duration": 91, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::as_strided_", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 23, "host_total_duration": 23, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::adaptive_avg_pool2d", "calls": 2, "device_self_duration": 0, "device_total_duration": 70, "host_self_duration": 648, "host_total_duration": 882, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::flatten", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 9, "host_total_duration": 30, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::transpose", "calls": 18, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 79, "host_total_duration": 106, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::t", "calls": 18, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 92, "host_total_duration": 198, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::unsqueeze", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 7, "host_total_duration": 8, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::linear", "calls": 2, "device_self_duration": 0, "device_total_duration": 27, "host_self_duration": 18, "host_total_duration": 199, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::log_softmax", "calls": 2, "device_self_duration": 0, "device_total_duration": 18, "host_self_duration": 17, "host_total_duration": 78, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::nll_loss", "calls": 2, "device_self_duration": 0, "device_total_duration": 29, "host_self_duration": 9, "host_total_duration": 116, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::nll_loss_nd", "calls": 2, "device_self_duration": 0, "device_total_duration": 29, "host_self_duration": 10, "host_total_duration": 126, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::cross_entropy_loss", "calls": 2, "device_self_duration": 0, "device_total_duration": 47, "host_self_duration": 620, "host_total_duration": 824, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::ones_like", "calls": 2, "device_self_duration": 0, "device_total_duration": 3, "host_self_duration": 14, "host_total_duration": 150, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_local_scalar_dense", "calls": 324, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 53, "host_total_duration": 53, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::item", "calls": 324, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 272, "host_total_duration": 325, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "NllLossBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 18, "host_self_duration": 55, "host_total_duration": 368, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: NllLossBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 18, "host_self_duration": 77, "host_total_duration": 445, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "LogSoftmaxBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 28, "host_self_duration": 21, "host_total_duration": 84, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: LogSoftmaxBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 28, "host_self_duration": 23, "host_total_duration": 107, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "AddmmBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 47, "host_self_duration": 44, "host_total_duration": 402, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: AddmmBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 58, "host_self_duration": 113, "host_total_duration": 702, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "detach", "calls": 288, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 611, "host_total_duration": 611, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::detach", "calls": 288, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 401, "host_total_duration": 1012, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "torch::autograd::AccumulateGrad", "calls": 322, "device_self_duration": 0, "device_total_duration": 227, "host_self_duration": 872, "host_total_duration": 3177, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: torch::autograd::AccumulateGrad", "calls": 322, "device_self_duration": 0, "device_total_duration": 227, "host_self_duration": 1009, "host_total_duration": 4186, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "TBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 5, "host_total_duration": 24, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: TBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 11, "host_total_duration": 36, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::reshape", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 6, "host_total_duration": 14, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "ViewBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 11, "host_total_duration": 25, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: ViewBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 10, "host_total_duration": 35, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::squeeze", "calls": 4, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 27, "host_total_duration": 31, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::new_empty", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 9, "host_total_duration": 23, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::new_zeros", "calls": 2, "device_self_duration": 0, "device_total_duration": 4, "host_self_duration": 21, "host_total_duration": 91, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "AsStridedBackward1", "calls": 2, "device_self_duration": 0, "device_total_duration": 10, "host_self_duration": 70, "host_total_duration": 303, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: AsStridedBackward1", "calls": 2, "device_self_duration": 0, "device_total_duration": 10, "host_self_duration": 21, "host_total_duration": 324, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::expand", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 48, "host_total_duration": 60, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "MeanBackward1", "calls": 2, "device_self_duration": 0, "device_total_duration": 60, "host_self_duration": 15994, "host_total_duration": 16646, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: MeanBackward1", "calls": 2, "device_self_duration": 0, "device_total_duration": 60, "host_self_duration": 31, "host_total_duration": 16676, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "ReluBackward0", "calls": 98, "device_self_duration": 0, "device_total_duration": 3802, "host_self_duration": 403, "host_total_duration": 2956, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: ReluBackward0", "calls": 98, "device_self_duration": 0, "device_total_duration": 3802, "host_self_duration": 722, "host_total_duration": 3678, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "AddBackward0", "calls": 32, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 38, "host_total_duration": 38, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: AddBackward0", "calls": 32, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 188, "host_total_duration": 227, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "NativeBatchNormBackward0", "calls": 106, "device_self_duration": 0, "device_total_duration": 8411, "host_self_duration": 676, "host_total_duration": 4356, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: NativeBatchNormBackward0", "calls": 106, "device_self_duration": 0, "device_total_duration": 8411, "host_self_duration": 1380, "host_total_duration": 5736, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "ConvolutionBackward0", "calls": 106, "device_self_duration": 0, "device_total_duration": 17361, "host_self_duration": 773, "host_total_duration": 13790, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: ConvolutionBackward0", "calls": 106, "device_self_duration": 0, "device_total_duration": 19931, "host_self_duration": 1187, "host_total_duration": 15631, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::new_empty_strided", "calls": 34, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 112, "host_total_duration": 210, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "MaxPool2DWithIndicesBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 589, "host_self_duration": 8098, "host_total_duration": 8178, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: MaxPool2DWithIndicesBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 589, "host_self_duration": 19, "host_total_duration": 8196, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}]}
{"metadata": {"sort": "Total Duration (us)"}, "data": {"columns": [{"type": "string", "name": "Name"}, {"type": "string", "name": "Tensor Cores Used", "tooltip": "Whether this kernel uses Tensor Cores."}, {"type": "number", "name": "Calls"}, {"type": "number", "name": "Total Duration (us)"}, {"type": "number", "name": "Mean Duration (us)"}, {"type": "number", "name": "Max Duration (us)"}, {"type": "number", "name": "Min Duration (us)"}], "rows": [["MLUUnionXKernelBatchNormBackward(void*, void*, void*, void*, void*, void*, void*, void*, void*, void*, void*, float, unsigned long, int, int, int, cnnlDataType_t, cnnlActivationMode_t)", "No", 106, 8411, 79, 300, 18], ["MLUUnionXKernelBatchNormForward(void*, void*, void*, void*, void*, void*, void*, void*, void*, void*, float, float, unsigned long, int, int, int, int, int, cnnlDataType_t)", "No", 106, 5786, 55, 190, 16], ["void MLUOpTensorElement110Pipe3<float, float, float, float>(float const*, float const*, float*, float, float, float, DataInfo, cnnlOpTensorDesc_t)", "No", 70, 5132, 73, 186, 18], ["void MLUUnion1KernelGemmRb<float, tfloat32_t, float, tfloat32_t, float, float>(void*, void*, void*, void*, float, float, int, int, int, int, int, int, int, int, int, int, float, float, bool, bool, bool)", "No", 64, 3903, 61, 101, 23], ["void MLUBlockKernel3StagePipelineV2ThresholdBackwardFast<float, float, float, float>(char*, char*, char*, unsigned long, unsigned long, float)", "No", 98, 3802, 39, 186, 7], ["void MLUUnionXKernelGepdot<float, tfloat32_t, float, tfloat32_t, float, float>(void*, void*, void*, void*, void*, float, float, int, int, int, int, int, int, int, int, int, int, bool, bool, int, int, float, float)", "No", 54, 3410, 63, 110, 41], ["void MLUBlockKernel3StagePipelineClipFast<float, float, float, float, OpStyle>(char*, char*, unsigned long, float, float, OpStyle)", "No", 98, 3238, 33, 134, 6], ["void MLUUnion1KernelGepp<float, tfloat32_t, float, float>(void*, void*, void*, void*, int, float, float, int, int, int, int, int, int, bool, bool, int, int, int, int)", "No", 58, 2799, 48, 83, 41], ["MLUUnionXKernelConvbpfIm2ColIn_float_float(void const*, void const*, void*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, bool)", "No", 26, 2398, 92, 127, 79], ["MLUDeconv3dDefault_true_float_float_float_float_float_float(void*, void const*, void const*, void const*, void*, void const*, void const*, void const*, void const*, void const*, void const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, bool, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, cnnlQuantizeRoundMode_t, bool, int, void const*, float, void const*, int, void const*, int, void const*, float, void const*, int, void const*, int, void const*, float, void const*, int, void const*)", "No", 14, 2102, 150, 308, 99], ["void MLUConvCoCi<0, 0, 0, float, float, float, float, float, float>(char*, char*, char*, char*, char*, char*, char*, char*, char*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, float, int, int, int, cnnlConvolutionCastMode_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, FuseParamHost, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlReorderType_t, cnnlReorderType_t, cnnlQuantizeRoundMode_t)", "No", 18, 2016, 112, 144, 96], ["void MLUOpTensorElement1x0Block<float, float, float, float>(float const*, float const*, float*, float, float, float, DataInfo, cnnlOpTensorDesc_t)", "No", 322, 1101, 3, 21, 2], ["void MLUBlockKernel3StagePipelineTransformFast<float, float, float, float>(char*, char*, unsigned long, float, float)", "No", 324, 1003, 3, 19, 2], ["MLUUnionXKernelConvbpfIm2ColInStride_float_float(void const*, void const*, void*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, bool)", "No", 8, 996, 125, 190, 86], ["void MLUOpTensorElement110Block<float, float, float, float>(float const*, float const*, float*, float, float, float, DataInfo, cnnlOpTensorDesc_t)", "No", 316, 952, 3, 18, 2], ["co_4_split_entry_CONVBPDATA_NO_STRIDE_F32_TF32_TF32_F32(void const*, void const*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, cnnlDataType_t, int, int, int, int, float, float, float, int, int, int, int, int, int, void*, void*, void*, void*, void*, void*, void*, void*, void*, int, int, cnnlQuantizeRoundMode_t)", "No", 10, 865, 87, 89, 84], ["void MLUAntsKernelGemmU1Ex<float, tfloat32_t, float, tfloat32_t, float, float, false>(void*, void*, void*, void*, int*, float*, int*, float*, float, float, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, bool, bool, bool)", "No", 18, 850, 47, 76, 38], ["void MLUConvCoCiSplit<0, 0, 0, float, float, float, float, float, float>(char const*, char*, char*, char*, char*, char*, char*, char*, char*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, float, int, int, int, cnnlConvolutionCastMode_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, FuseParamHost, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlReorderType_t, cnnlReorderType_t, cnnlQuantizeRoundMode_t)", "No", 8, 779, 97, 112, 92], ["void MLUTransposeKernel3DCSmallBlock<int, (Trans3DMode)2>(int const*, int*, DimSplit, DimSplit, DimSplit)", "No", 102, 697, 7, 16, 3], ["MLUUnionXKernelStrideGepdotIn_float_float_float_float(void const*, void const*, void*, void*, float, float, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, void const*, void const*, void const*, void const*, bool, bool, bool)", "No", 6, 657, 109, 123, 100], ["MLUKernelBlockPoolBackward_opt(void*, void*, void*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, long, long, cnnlPoolingMode_t, bool, bool, bool, bool, cnnlDataType_t, cnnlDataType_t)", "No", 2, 589, 294, 294, 294], ["nosplit_hw_entry_F32_F32_F32_F32_F32_F32_false_false(void*, void const*, void const*, void const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, nosplitPartitionMode_t, int, float, void const*, void const*, int, float, void const*, void const*, int, float, int, int, int, void const*, void const*, int, int, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, bool, int, int, int, bool, cnnlQuantizeRoundMode_t)", "No", 6, 558, 93, 94, 92], ["void MLUConvCo1<0, 0, 0, float, float, float, float, float, float>(char const*, char*, char*, char*, char*, char*, char*, char*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, float, int, int, int, cnnlConvolutionCastMode_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, FuseParamHost, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlReorderType_t, cnnlReorderType_t, cnnlQuantizeRoundMode_t)", "No", 6, 504, 84, 85, 83], ["MLUUnion1PoolingForwardWithIndex(void*, void*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, cnnlDataType_t, cnnlDataType_t)", "No", 2, 488, 244, 244, 244], ["co_4_split_entry_CONVBPDATA_STRIDE_F32_TF32_TF32_F32(void const*, void const*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, cnnlDataType_t, int, int, int, int, float, float, float, int, int, int, int, int, int, void*, void*, void*, void*, void*, void*, void*, void*, void*, int, int, cnnlQuantizeRoundMode_t)", "No", 4, 470, 118, 120, 114], ["MLUUnion1KernelGepdotReduce(void*, void*, int, int, int, int, int, cnnlDataType_t, cnnlDataType_t)", "No", 54, 380, 7, 9, 4], ["void MLUConvbpfDiffwJobReduceBlock<float, float>(float*, float*, unsigned long, int)", "No", 40, 347, 9, 17, 4], ["void MLUBlockKernel3StagePipelineTransformInt64Fast<long, long, long, long>(char*, char*, unsigned long, long, long)", "No", 106, 344, 3, 5, 3], ["MLUUnion1ConvForwardGeppAnt_float_tfloat32_t_float_float(void const*, void const*, void const*, void*, int, int, int, int, int, int, int, int, int, float, float, int, int, float, float, void const*, void const*, void const*, void const*, bool)", "No", 4, 308, 77, 82, 72], ["void MLUConvImplicitGEMM<0, 0, 0>(char*, char*, char*, char*, char*, char*, char*, char*, char*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, float, int, int, int, cnnlConvolutionCastMode_t, int, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, FuseParamHost, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlQuantizeRoundMode_t)", "No", 2, 268, 134, 134, 134], ["MLUUnion1ConvBackwardDataGeppAnt_float_tfloat32_t_float_float(void const*, void const*, void*, int, int, int, int, int, int, int, int, int, float, float, int, int, float, float, void const*, void const*, void const*, void const*, bool)", "No", 2, 200, 100, 101, 99], ["MLUUnion1ConvBackwardDataGEPB_float_tfloat32_t_float_float(void const*, void const*, void*, int, int, int, int, int, int, int, int, int, float, float, int, int, float, float, void const*, void const*, void const*, void const*, bool, bool)", "No", 2, 194, 97, 102, 91], ["void MLUAntsKernelGemmU1Ex<float, tfloat32_t, float, tfloat32_t, float, float, true>(void*, void*, void*, void*, int*, float*, int*, float*, float, float, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, bool, bool, bool)", "No", 4, 171, 43, 44, 41], ["MLUUnion1ConvForwardGEPB_float_tfloat32_t_float_float(void const*, void const*, void const*, void*, int, int, int, int, int, int, int, int, int, float, float, int, int, float, float, void const*, void const*, void const*, void const*, bool, bool)", "No", 2, 145, 72, 73, 72], ["void MLUTranspose2DDefaultBlock<int>(int const*, int*, bool, DimSplit, DimSplit)", "No", 14, 114, 8, 14, 4], ["void MLUTransposeKernel3DDefaultBlock<int, (Trans3DMode)2>(int const*, int*, DimSplit, DimSplit, DimSplit)", "No", 4, 110, 27, 30, 25], ["void reformatWeightNoStrideMLUUnion1<float, unsigned int>(float const*, unsigned int*, int, int, int, int, cnnlTensorLayout_t, int, int, float, int, int, void*, void*, void*, cnnlQuantizeRoundMode_t)", "No", 10, 86, 9, 9, 7], ["void reformatWeightStrideMLUUnion1<float, unsigned int>(float const*, unsigned int*, int, int, int, int, int, int, int, int, cnnlTensorLayout_t, int, int, float, int, int, void*, void*, void*, cnnlQuantizeRoundMode_t)", "No", 4, 84, 21, 35, 7], ["void MLUUnion1Reduce2DPipelineMeanValue<float, float>(void*, void*, void*, void*, unsigned long, unsigned long, unsigned int, unsigned int, unsigned int, bool, float, float, float, cnnlReduceOp_t, cnnlReduceIndices_t, cnnlDataType_t, cnnlIndicesType_t)", "No", 2, 36, 18, 18, 18], ["void MLUTransposeKernel3DDefaultPipe3Union1<int, (Trans3DMode)2>(int const*, int*, DimSplit, DimSplit, DimSplit)", "No", 2, 33, 17, 17, 17], ["mluUnion1KernelSoftmaxBackwardFp32(void*, void*, void*, long, long, long, cnnlSoftmaxAlgorithm_t, cnnlSoftmaxMode_t, cnnlDataType_t)", "No", 2, 28, 14, 14, 14], ["void MLUUnion1KernelGepmEx<float, float, float, float, float>(void*, void*, void*, void*, float, float, int, int, int, int, int, int, bool, bool, int, int, int, int, int, int, void*, matmul_ex::FuseParamHost, float, int, int, cnnlQuantizeRoundMode_t)", "No", 2, 27, 14, 14, 14], ["void MLUUnion1KernelGepm<float, float, float, float>(void*, void*, void*, void*, float, float, int, int, int, int, int, int, bool, bool, int, int, int, int, int, int)", "No", 2, 24, 12, 12, 12], ["void MLUBlockKernelExpandC1AtoCBA<int, unsigned int>(void*, void*, Shapes<unsigned int>)", "No", 2, 23, 12, 12, 11], ["void MLUAntsKernelGemmU1Ex<float, float, float, float, float, float, false>(void*, void*, void*, void*, int*, float*, int*, float*, float, float, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, bool, bool, bool)", "No", 2, 22, 11, 11, 11], ["void MLUTranspose2DSmallBlock<int>(int const*, int*, bool, DimSplit, DimSplit)", "No", 6, 19, 3, 3, 3], ["void MLUBlockKernelSoftmaxForwardLowTransposeSmallTarget<float, float, (cnnlSoftmaxAlgorithm_t)2>(float*, float*, unsigned long, unsigned int, unsigned int, unsigned int, unsigned int)", "No", 2, 18, 9, 9, 9], ["void MLUUnion1KernelFillHostValue<unsigned int>(void*, unsigned long, unsigned int)", "No", 10, 16, 2, 2, 1], ["void MLUUnion1NlllossForwardVaa<float, long, (deviceMode_t)1>(float*, float*, float*, float*, float*, long*, long, unsigned long, unsigned long, unsigned int, bool)", "No", 2, 14, 7, 7, 7], ["void MLUUnion1NlllossBackwardVaa<float, long, 2u>(float*, float*, float*, float*, long*, long, unsigned long, unsigned long, unsigned int)", "No", 2, 12, 6, 6, 6], ["void MLUUnion1Reduce2DPipelineSumValue<float, float>(void*, void*, void*, void*, unsigned long, unsigned long, unsigned int, unsigned int, unsigned int, bool, float, float, float, cnnlReduceOp_t, cnnlReduceIndices_t, cnnlDataType_t, cnnlIndicesType_t)", "No", 2, 11, 6, 6, 6], ["void MLUBlockNlllossTargetCheck<long>(long*, unsigned long, unsigned long, long)", "No", 2, 8, 4, 4, 4], ["MLUUnion1KernelCopy(void*, void*, unsigned long, int)", "No", 2, 6, 3, 3, 3], ["MLUBlockNlllossForwardPostprocess(void*, void*, float*, int, int, int)", "No", 2, 4, 2, 2, 2]]}}
{"total": {"columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["MLUUnionXKernelBatchNormBackward(void*, void*, void*, void*, void*, void*, void*, void*, void*, void*, void*, float, unsigned long, int, int, int, cnnlDataType_t, cnnlActivationMode_t)", 8411.04], ["MLUUnionXKernelBatchNormForward(void*, void*, void*, void*, void*, void*, void*, void*, void*, void*, float, float, unsigned long, int, int, int, int, int, cnnlDataType_t)", 5785.92], ["void MLUOpTensorElement110Pipe3<float, float, float, float>(float const*, float const*, float*, float, float, float, DataInfo, cnnlOpTensorDesc_t)", 5131.76], ["void MLUUnion1KernelGemmRb<float, tfloat32_t, float, tfloat32_t, float, float>(void*, void*, void*, void*, float, float, int, int, int, int, int, int, int, int, int, int, float, float, bool, bool, bool)", 3903.24], ["void MLUBlockKernel3StagePipelineV2ThresholdBackwardFast<float, float, float, float>(char*, char*, char*, unsigned long, unsigned long, float)", 3802.0], ["void MLUUnionXKernelGepdot<float, tfloat32_t, float, tfloat32_t, float, float>(void*, void*, void*, void*, void*, float, float, int, int, int, int, int, int, int, int, int, int, bool, bool, int, int, float, float)", 3410.2799999999997], ["void MLUBlockKernel3StagePipelineClipFast<float, float, float, float, OpStyle>(char*, char*, unsigned long, float, float, OpStyle)", 3237.96], ["void MLUUnion1KernelGepp<float, tfloat32_t, float, float>(void*, void*, void*, void*, int, float, float, int, int, int, int, int, int, bool, bool, int, int, int, int)", 2799.08], ["MLUUnionXKernelConvbpfIm2ColIn_float_float(void const*, void const*, void*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, bool)", 2398.04], ["MLUDeconv3dDefault_true_float_float_float_float_float_float(void*, void const*, void const*, void const*, void*, void const*, void const*, void const*, void const*, void const*, void const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, bool, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, cnnlQuantizeRoundMode_t, bool, int, void const*, float, void const*, int, void const*, int, void const*, float, void const*, int, void const*, int, void const*, float, void const*, int, void const*)", 2101.96], ["void MLUConvCoCi<0, 0, 0, float, float, float, float, float, float>(char*, char*, char*, char*, char*, char*, char*, char*, char*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, float, int, int, int, cnnlConvolutionCastMode_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, FuseParamHost, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlReorderType_t, cnnlReorderType_t, cnnlQuantizeRoundMode_t)", 2016.1200000000001], ["void MLUOpTensorElement1x0Block<float, float, float, float>(float const*, float const*, float*, float, float, float, DataInfo, cnnlOpTensorDesc_t)", 1101.24], ["void MLUBlockKernel3StagePipelineTransformFast<float, float, float, float>(char*, char*, unsigned long, float, float)", 1003.32], ["MLUUnionXKernelConvbpfIm2ColInStride_float_float(void const*, void const*, void*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, bool)", 996.12], ["void MLUOpTensorElement110Block<float, float, float, float>(float const*, float const*, float*, float, float, float, DataInfo, cnnlOpTensorDesc_t)", 951.6], ["co_4_split_entry_CONVBPDATA_NO_STRIDE_F32_TF32_TF32_F32(void const*, void const*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, cnnlDataType_t, int, int, int, int, float, float, float, int, int, int, int, int, int, void*, void*, void*, void*, void*, void*, void*, void*, void*, int, int, cnnlQuantizeRoundMode_t)", 865.16], ["void MLUAntsKernelGemmU1Ex<float, tfloat32_t, float, tfloat32_t, float, float, false>(void*, void*, void*, void*, int*, float*, int*, float*, float, float, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, bool, bool, bool)", 850.4], ["void MLUConvCoCiSplit<0, 0, 0, float, float, float, float, float, float>(char const*, char*, char*, char*, char*, char*, char*, char*, char*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, float, int, int, int, cnnlConvolutionCastMode_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, FuseParamHost, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlReorderType_t, cnnlReorderType_t, cnnlQuantizeRoundMode_t)", 778.64], ["void MLUTransposeKernel3DCSmallBlock<int, (Trans3DMode)2>(int const*, int*, DimSplit, DimSplit, DimSplit)", 696.56], ["MLUUnionXKernelStrideGepdotIn_float_float_float_float(void const*, void const*, void*, void*, float, float, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, void const*, void const*, void const*, void const*, bool, bool, bool)", 656.96], ["MLUKernelBlockPoolBackward_opt(void*, void*, void*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, long, long, cnnlPoolingMode_t, bool, bool, bool, bool, cnnlDataType_t, cnnlDataType_t)", 588.56], ["nosplit_hw_entry_F32_F32_F32_F32_F32_F32_false_false(void*, void const*, void const*, void const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, nosplitPartitionMode_t, int, float, void const*, void const*, int, float, void const*, void const*, int, float, int, int, int, void const*, void const*, int, int, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, bool, int, int, int, bool, cnnlQuantizeRoundMode_t)", 558.16], ["void MLUConvCo1<0, 0, 0, float, float, float, float, float, float>(char const*, char*, char*, char*, char*, char*, char*, char*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, float, int, int, int, cnnlConvolutionCastMode_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, FuseParamHost, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlReorderType_t, cnnlReorderType_t, cnnlQuantizeRoundMode_t)", 504.44], ["MLUUnion1PoolingForwardWithIndex(void*, void*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, cnnlDataType_t, cnnlDataType_t)", 487.76], ["co_4_split_entry_CONVBPDATA_STRIDE_F32_TF32_TF32_F32(void const*, void const*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, cnnlDataType_t, int, int, int, int, float, float, float, int, int, int, int, int, int, void*, void*, void*, void*, void*, void*, void*, void*, void*, int, int, cnnlQuantizeRoundMode_t)", 470.24], ["MLUUnion1KernelGepdotReduce(void*, void*, int, int, int, int, int, cnnlDataType_t, cnnlDataType_t)", 379.92], ["void MLUConvbpfDiffwJobReduceBlock<float, float>(float*, float*, unsigned long, int)", 347.36], ["void MLUBlockKernel3StagePipelineTransformInt64Fast<long, long, long, long>(char*, char*, unsigned long, long, long)", 344.12], ["MLUUnion1ConvForwardGeppAnt_float_tfloat32_t_float_float(void const*, void const*, void const*, void*, int, int, int, int, int, int, int, int, int, float, float, int, int, float, float, void const*, void const*, void const*, void const*, bool)", 308.48], ["void MLUConvImplicitGEMM<0, 0, 0>(char*, char*, char*, char*, char*, char*, char*, char*, char*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, float, int, int, int, cnnlConvolutionCastMode_t, int, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, FuseParamHost, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlQuantizeRoundMode_t)", 268.4], ["MLUUnion1ConvBackwardDataGeppAnt_float_tfloat32_t_float_float(void const*, void const*, void*, int, int, int, int, int, int, int, int, int, float, float, int, int, float, float, void const*, void const*, void const*, void const*, bool)", 200.44], ["MLUUnion1ConvBackwardDataGEPB_float_tfloat32_t_float_float(void const*, void const*, void*, int, int, int, int, int, int, int, int, int, float, float, int, int, float, float, void const*, void const*, void const*, void const*, bool, bool)", 193.68], ["void MLUAntsKernelGemmU1Ex<float, tfloat32_t, float, tfloat32_t, float, float, true>(void*, void*, void*, void*, int*, float*, int*, float*, float, float, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, bool, bool, bool)", 170.8], ["MLUUnion1ConvForwardGEPB_float_tfloat32_t_float_float(void const*, void const*, void const*, void*, int, int, int, int, int, int, int, int, int, float, float, int, int, float, float, void const*, void const*, void const*, void const*, bool, bool)", 144.68], ["void MLUTranspose2DDefaultBlock<int>(int const*, int*, bool, DimSplit, DimSplit)", 113.96000000000001], ["void MLUTransposeKernel3DDefaultBlock<int, (Trans3DMode)2>(int const*, int*, DimSplit, DimSplit, DimSplit)", 109.52], ["void reformatWeightNoStrideMLUUnion1<float, unsigned int>(float const*, unsigned int*, int, int, int, int, cnnlTensorLayout_t, int, int, float, int, int, void*, void*, void*, cnnlQuantizeRoundMode_t)", 85.60000000000001], ["void reformatWeightStrideMLUUnion1<float, unsigned int>(float const*, unsigned int*, int, int, int, int, int, int, int, int, cnnlTensorLayout_t, int, int, float, int, int, void*, void*, void*, cnnlQuantizeRoundMode_t)", 83.92], ["void MLUUnion1Reduce2DPipelineMeanValue<float, float>(void*, void*, void*, void*, unsigned long, unsigned long, unsigned int, unsigned int, unsigned int, bool, float, float, float, cnnlReduceOp_t, cnnlReduceIndices_t, cnnlDataType_t, cnnlIndicesType_t)", 36.08], ["void MLUTransposeKernel3DDefaultPipe3Union1<int, (Trans3DMode)2>(int const*, int*, DimSplit, DimSplit, DimSplit)", 33.480000000000004], ["mluUnion1KernelSoftmaxBackwardFp32(void*, void*, void*, long, long, long, cnnlSoftmaxAlgorithm_t, cnnlSoftmaxMode_t, cnnlDataType_t)", 28.08], ["void MLUUnion1KernelGepmEx<float, float, float, float, float>(void*, void*, void*, void*, float, float, int, int, int, int, int, int, bool, bool, int, int, int, int, int, int, void*, matmul_ex::FuseParamHost, float, int, int, cnnlQuantizeRoundMode_t)", 27.16], ["void MLUUnion1KernelGepm<float, float, float, float>(void*, void*, void*, void*, float, float, int, int, int, int, int, int, bool, bool, int, int, int, int, int, int)", 24.159999999999997], ["void MLUBlockKernelExpandC1AtoCBA<int, unsigned int>(void*, void*, Shapes<unsigned int>)", 23.16], ["void MLUAntsKernelGemmU1Ex<float, float, float, float, float, float, false>(void*, void*, void*, void*, int*, float*, int*, float*, float, float, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, bool, bool, bool)", 22.4], ["void MLUTranspose2DSmallBlock<int>(int const*, int*, bool, DimSplit, DimSplit)", 18.96], ["void MLUBlockKernelSoftmaxForwardLowTransposeSmallTarget<float, float, (cnnlSoftmaxAlgorithm_t)2>(float*, float*, unsigned long, unsigned int, unsigned int, unsigned int, unsigned int)", 18.32], ["void MLUUnion1KernelFillHostValue<unsigned int>(void*, unsigned long, unsigned int)", 16.36], ["void MLUUnion1NlllossForwardVaa<float, long, (deviceMode_t)1>(float*, float*, float*, float*, float*, long*, long, unsigned long, unsigned long, unsigned int, bool)", 14.0], ["void MLUUnion1NlllossBackwardVaa<float, long, 2u>(float*, float*, float*, float*, long*, long, unsigned long, unsigned long, unsigned int)", 11.52], ["void MLUUnion1Reduce2DPipelineSumValue<float, float>(void*, void*, void*, void*, unsigned long, unsigned long, unsigned int, unsigned int, unsigned int, bool, float, float, float, cnnlReduceOp_t, cnnlReduceIndices_t, cnnlDataType_t, cnnlIndicesType_t)", 11.32], ["void MLUBlockNlllossTargetCheck<long>(long*, unsigned long, unsigned long, long)", 8.2], ["MLUUnion1KernelCopy(void*, void*, unsigned long, int)", 6.0], ["MLUBlockNlllossForwardPostprocess(void*, void*, float*, int, int, int)", 3.76]]}}
