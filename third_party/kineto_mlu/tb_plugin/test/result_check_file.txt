{"steps": {"columns": [{"type": "string", "name": "Step"}, {"type": "number", "name": "Kernel"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Memcpy"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Memset"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Runtime"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "DataLoader"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "CPU Exec"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Other"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}], "rows": [["1", 89172, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 1<br>Total: 152302us<br><b>Kernel: 89172us</b><br>Percentage: 58.55%</div>", 4153, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 1<br>Total: 152302us<br><b>Memcpy: 4153us</b><br>Percentage: 2.73%</div>", 82, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 1<br>Total: 152302us<br><b>Memset: 82us</b><br>Percentage: 0.05%</div>", 745, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 1<br>Total: 152302us<br><b>Runtime: 745us</b><br>Percentage: 0.49%</div>", 56524, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 1<br>Total: 152302us<br><b>DataLoader: 56524us</b><br>Percentage: 37.11%</div>", 1062, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 1<br>Total: 152302us<br><b>CPU Exec: 1062us</b><br>Percentage: 0.7%</div>", 564, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 1<br>Total: 152302us<br><b>Other: 564us</b><br>Percentage: 0.37%</div>"], ["2", 88967, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 101591us<br><b>Kernel: 88967us</b><br>Percentage: 87.57%</div>", 4108, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 101591us<br><b>Memcpy: 4108us</b><br>Percentage: 4.04%</div>", 83, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 101591us<br><b>Memset: 83us</b><br>Percentage: 0.08%</div>", 738, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 101591us<br><b>Runtime: 738us</b><br>Percentage: 0.73%</div>", 6759, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 101591us<br><b>DataLoader: 6759us</b><br>Percentage: 6.65%</div>", 686, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 101591us<br><b>CPU Exec: 686us</b><br>Percentage: 0.68%</div>", 250, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 101591us<br><b>Other: 250us</b><br>Percentage: 0.25%</div>"]]}, "performance": [{"name": "Average Step Time", "description": "", "value": 126946, "extra": 100, "children": [{"name": "Kernel", "description": "", "value": 89070, "extra": 70.16}, {"name": "Memcpy", "description": "", "value": 4130, "extra": 3.25}, {"name": "Memset", "description": "", "value": 82, "extra": 0.06}, {"name": "Runtime", "description": "", "value": 742, "extra": 0.58}, {"name": "DataLoader", "description": "", "value": 31642, "extra": 24.93}, {"name": "CPU Exec", "description": "", "value": 874, "extra": 0.69}, {"name": "Other", "description": "", "value": 407, "extra": 0.32}]}], "recommendations": "<ul><li>This run has high time cost on input data loading. 24.9% of the step time is in DataLoader. You could try to set num_workers on DataLoader's construction and <a href=\"https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading\" target=\"_blank\">enable multi-processes on data loading</a>.</li><li>Kernels with 20% time are launched by Tensor Cores eligible operators. You could enable <a href=\"https://pytorch.org/docs/stable/amp.html\" target=\"_blank\">Automatic Mixed Precision</a> to speedup by using FP16.</li></ul>", "environments": [{"title": "Number of Worker(s)", "value": "1"}, {"title": "Device Type", "value": "GPU"}], "gpu_metrics": {"title": "GPU Summary", "data": [{"title": "GPU 0:", "value": ""}, {"title": "Name", "value": "Tesla V100-SXM2-16GB"}, {"title": "Memory", "value": "15.77 GB"}, {"title": "Compute Capability", "value": "7.0"}, {"title": "GPU Utilization", "value": "70.16 %"}, {"title": "Est. SM Efficiency", "value": "69.35 %"}, {"title": "Est. Achieved Occupancy", "value": "46.75 %"}, {"title": "Kernel Time using Tensor Cores", "value": "0.0 %"}], "tooltip": "The GPU usage metrics:\n\nGPU Utilization:\nGPU busy time / All steps time. The higher, the better. GPU busy time is the time during which there is at least one GPU kernel running on it. All steps time is the total time of all profiler steps(or called as iterations).\n\nEst. SM Efficiency:\nEstimated Stream Multiprocessor Efficiency. The higher, the better. This metric of a kernel, SM_Eff_K = min(blocks of this kernel / SM number of this GPU, 100%). This overall number is the sum of all kernels' SM_Eff_K weighted by kernel's execution duration, divided by all steps time.\n\nEst. Achieved Occupancy:\nFor most cases such as memory bandwidth bounded kernels, the higher the better. Occupancy is the ratio of active warps on an SM to the maximum number of active warps supported by the SM. The theoretical occupancy of a kernel is upper limit occupancy of this kernel, limited by multiple factors such as kernel shape, kernel used resource, and the GPU compute capability.\nEst. Achieved Occupancy of a kernel, OCC_K = min(threads of the kernel / SM number / max threads per SM, theoretical occupancy of the kernel). This overall number is the weighted average of all kernels' OCC_K using kernel's execution duration as weight. It shows fine-grained low-level GPU utilization.\n\nKernel using Tensor Cores:\nTotal GPU Time for Tensor Core kernels / Total GPU Time for all kernels.\n"}}
{"device_total_time": {"title": "Device Total Time (us)", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["autograd::engine::evaluate_function: ConvolutionBackward0", 86882], ["aten::convolution_backward", 81825], ["ConvolutionBackward0", 81825], ["aten::cudnn_convolution", 35039], ["aten::_convolution", 35039], ["aten::convolution", 35039], ["aten::conv2d", 35039], ["aten::cudnn_batch_norm_backward", 20418], ["CudnnBatchNormBackward0", 20418], ["autograd::engine::evaluate_function: CudnnBatchNormBackward0", 20418], ["aten::cudnn_batch_norm", 10933], ["aten::_batch_norm_impl_index", 10933], ["aten::batch_norm", 10933], ["aten::add_", 10215], ["aten::threshold_backward", 8737], ["ReluBackward0", 8737], ["autograd::engine::evaluate_function: ReluBackward0", 8737], ["aten::copy_", 8261], ["aten::_to_copy", 8261], ["aten::to", 8261], ["aten::clamp_min_", 5822], ["aten::relu_", 5822], ["aten::max_pool2d_with_indices_backward", 2434], ["MaxPool2DWithIndicesBackward0", 2434], ["autograd::engine::evaluate_function: MaxPool2DWithIndicesBackward0", 2434], ["aten::_foreach_add_", 1558], ["aten::_foreach_mul_", 554], ["aten::max_pool2d_with_indices", 455], ["aten::max_pool2d", 455], ["aten::fill_", 234], ["aten::zero_", 232], ["autograd::engine::evaluate_function: AddmmBackward0", 103], ["aten::mm", 87], ["AddmmBackward0", 87], ["aten::mean", 71], ["aten::adaptive_avg_pool2d", 71], ["aten::addmm", 54], ["aten::linear", 54], ["aten::div", 40], ["MeanBackward1", 40], ["autograd::engine::evaluate_function: MeanBackward1", 40], ["aten::cross_entropy_loss", 21], ["aten::_log_softmax_backward_data", 17], ["LogSoftmaxBackward0", 17], ["autograd::engine::evaluate_function: LogSoftmaxBackward0", 17], ["aten::sum", 16], ["aten::_log_softmax", 15], ["aten::log_softmax", 15], ["aten::nll_loss_forward", 6], ["aten::nll_loss", 6], ["aten::nll_loss_nd", 6], ["aten::nll_loss_backward", 6], ["NllLossBackward0", 6], ["autograd::engine::evaluate_function: NllLossBackward0", 6], ["aten::ones_like", 2]]}, "device_self_time": {"title": "Device Self Time (us)", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::convolution_backward", 81825], ["aten::cudnn_convolution", 35039], ["aten::cudnn_batch_norm_backward", 20418], ["aten::cudnn_batch_norm", 10933], ["aten::add_", 10215], ["aten::threshold_backward", 8737], ["aten::copy_", 8261], ["aten::clamp_min_", 5822], ["aten::max_pool2d_with_indices_backward", 2204], ["aten::_foreach_add_", 1558], ["aten::_foreach_mul_", 554], ["aten::max_pool2d_with_indices", 455], ["aten::fill_", 234], ["aten::mm", 87], ["aten::mean", 71], ["aten::addmm", 54], ["aten::div", 40], ["aten::_log_softmax_backward_data", 17], ["aten::sum", 16], ["aten::_log_softmax", 15], ["aten::nll_loss_forward", 6], ["aten::nll_loss_backward", 4]]}, "host_total_time": {"title": "Host Total Time (us)", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["autograd::engine::evaluate_function: ConvolutionBackward0", 16779], ["ConvolutionBackward0", 14689], ["aten::copy_", 14626], ["aten::convolution_backward", 14045], ["aten::to", 13246], ["aten::_to_copy", 12853], ["aten::batch_norm", 11861], ["aten::_batch_norm_impl_index", 11600], ["aten::conv2d", 11553], ["aten::convolution", 11109], ["aten::cudnn_batch_norm", 11062], ["aten::stack", 10937], ["aten::cat", 10755], ["aten::_convolution", 9995], ["autograd::engine::evaluate_function: CudnnBatchNormBackward0", 9591], ["aten::cudnn_convolution", 9306], ["aten::empty", 8250], ["CudnnBatchNormBackward0", 8144], ["aten::cudnn_batch_norm_backward", 7588], ["aten::div", 4705], ["aten::add_", 3878], ["aten::contiguous", 3857], ["autograd::engine::evaluate_function: ReluBackward0", 3790], ["aten::clone", 3753], ["autograd::engine::evaluate_function: torch::autograd::AccumulateGrad", 3529], ["aten::relu_", 3208], ["ReluBackward0", 3040], ["aten::threshold_backward", 2657], ["aten::empty_like", 2125], ["aten::_foreach_add_", 2118], ["aten::clamp_min_", 1869], ["torch::autograd::AccumulateGrad", 1699], ["aten::_foreach_mul_", 1112], ["aten::detach", 973], ["aten::view", 739], ["detach", 645], ["aten::permute", 536], ["aten::empty_strided", 484], ["autograd::engine::evaluate_function: AddmmBackward0", 391], ["aten::result_type", 275], ["AddmmBackward0", 271], ["aten::linear", 242], ["autograd::engine::evaluate_function: NllLossBackward0", 223], ["autograd::engine::evaluate_function: AddBackward0", 223], ["aten::cross_entropy_loss", 209], ["aten::mm", 193], ["aten::addmm", 188], ["NllLossBackward0", 185], ["aten::fill_", 171], ["autograd::engine::evaluate_function: MeanBackward1", 156], ["aten::nll_loss_backward", 155], ["MeanBackward1", 140], ["aten::ones_like", 135], ["aten::as_strided", 133], ["autograd::engine::evaluate_function: MaxPool2DWithIndicesBackward0", 130], ["aten::max_pool2d", 124], ["aten::zero_", 112], ["MaxPool2DWithIndicesBackward0", 112], ["aten::max_pool2d_with_indices", 109], ["aten::adaptive_avg_pool2d", 106], ["aten::lift_fresh", 104], ["aten::nll_loss_nd", 99], ["aten::t", 90], ["aten::max_pool2d_with_indices_backward", 89], ["aten::nll_loss", 88], ["autograd::engine::evaluate_function: LogSoftmaxBackward0", 86], ["aten::mean", 85], ["aten::log_softmax", 85], ["aten::nll_loss_forward", 77], ["LogSoftmaxBackward0", 70], ["aten::_log_softmax", 69], ["aten::sum", 66], ["aten::_log_softmax_backward_data", 53], ["aten::transpose", 50], ["aten::narrow", 40], ["autograd::engine::evaluate_function: ViewBackward0", 37], ["AddBackward0", 27], ["autograd::engine::evaluate_function: TBackward0", 24], ["aten::flatten", 19], ["ViewBackward0", 17], ["aten::expand", 16], ["aten::slice", 14], ["TBackward0", 14], ["aten::detach_", 9], ["aten::reshape", 8], ["detach_", 3], ["aten::resize_", 2]]}, "host_self_time": {"title": "Host Self Time (us)", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::cat", 10715], ["aten::convolution_backward", 9471], ["aten::empty", 8250], ["aten::cudnn_convolution", 7054], ["aten::copy_", 5595], ["aten::cudnn_batch_norm", 5059], ["aten::div", 4119], ["aten::cudnn_batch_norm_backward", 3728], ["aten::add_", 2719], ["autograd::engine::evaluate_function: torch::autograd::AccumulateGrad", 1830], ["aten::_foreach_add_", 1822], ["aten::threshold_backward", 1817], ["autograd::engine::evaluate_function: CudnnBatchNormBackward0", 1447], ["autograd::engine::evaluate_function: ConvolutionBackward0", 1443], ["aten::relu_", 1339], ["aten::convolution", 1114], ["aten::clamp_min_", 1113], ["aten::_foreach_mul_", 849], ["autograd::engine::evaluate_function: ReluBackward0", 750], ["aten::view", 739], ["torch::autograd::AccumulateGrad", 726], ["aten::empty_like", 716], ["aten::_to_copy", 712], ["aten::_convolution", 689], ["detach", 645], ["ConvolutionBackward0", 644], ["CudnnBatchNormBackward0", 556], ["aten::_batch_norm_impl_index", 538], ["aten::empty_strided", 484], ["aten::conv2d", 444], ["aten::permute", 424], ["aten::to", 393], ["ReluBackward0", 383], ["aten::detach", 328], ["aten::clone", 320], ["aten::result_type", 275], ["aten::batch_norm", 261], ["autograd::engine::evaluate_function: AddBackward0", 196], ["aten::stack", 177], ["aten::addmm", 152], ["aten::as_strided", 133], ["aten::mm", 133], ["aten::lift_fresh", 104], ["aten::contiguous", 104], ["aten::max_pool2d_with_indices", 91], ["aten::fill_", 73], ["aten::mean", 68], ["aten::nll_loss_backward", 63], ["aten::nll_loss_forward", 55], ["aten::_log_softmax", 52], ["aten::sum", 51], ["autograd::engine::evaluate_function: AddmmBackward0", 49], ["aten::max_pool2d_with_indices_backward", 41], ["aten::t", 40], ["autograd::engine::evaluate_function: NllLossBackward0", 38], ["aten::_log_softmax_backward_data", 38], ["aten::transpose", 34], ["AddmmBackward0", 31], ["NllLossBackward0", 30], ["MeanBackward1", 27], ["AddBackward0", 27], ["aten::narrow", 26], ["aten::cross_entropy_loss", 25], ["MaxPool2DWithIndicesBackward0", 23], ["aten::linear", 22], ["aten::adaptive_avg_pool2d", 21], ["autograd::engine::evaluate_function: ViewBackward0", 20], ["autograd::engine::evaluate_function: MaxPool2DWithIndicesBackward0", 18], ["aten::zero_", 17], ["LogSoftmaxBackward0", 17], ["aten::log_softmax", 16], ["autograd::engine::evaluate_function: LogSoftmaxBackward0", 16], ["autograd::engine::evaluate_function: MeanBackward1", 16], ["aten::max_pool2d", 15], ["aten::ones_like", 14], ["aten::expand", 13], ["aten::slice", 12], ["aten::nll_loss", 11], ["aten::nll_loss_nd", 11], ["autograd::engine::evaluate_function: TBackward0", 10], ["ViewBackward0", 9], ["aten::flatten", 7], ["aten::detach_", 6], ["aten::reshape", 5], ["detach_", 3], ["TBackward0", 3], ["aten::resize_", 2]]}}
{"metadata": {"sort": "device_self_duration", "tooltips": {"tc_eligible": "Whether this operator is eligible to use Tensor Cores.", "tc_self_ratio": "Time of self-kernels with Tensor Cores / Time of self-kernels.", "tc_total_ratio": "Time of kernels with Tensor Cores / Time of kernels."}}, "data": [{"name": "aten::convolution_backward", "calls": 106, "device_self_duration": 81825, "device_total_duration": 81825, "host_self_duration": 9471, "host_total_duration": 14045, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::cudnn_convolution", "calls": 106, "device_self_duration": 35039, "device_total_duration": 35039, "host_self_duration": 7054, "host_total_duration": 9306, "tc_eligible": "Yes", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::cudnn_batch_norm_backward", "calls": 106, "device_self_duration": 20418, "device_total_duration": 20418, "host_self_duration": 3728, "host_total_duration": 7588, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::cudnn_batch_norm", "calls": 106, "device_self_duration": 10933, "device_total_duration": 10933, "host_self_duration": 5059, "host_total_duration": 11062, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::add_", "calls": 170, "device_self_duration": 10215, "device_total_duration": 10215, "host_self_duration": 2719, "host_total_duration": 3878, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::threshold_backward", "calls": 98, "device_self_duration": 8737, "device_total_duration": 8737, "host_self_duration": 1817, "host_total_duration": 2657, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::copy_", "calls": 196, "device_self_duration": 8261, "device_total_duration": 8261, "host_self_duration": 5595, "host_total_duration": 14626, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::clamp_min_", "calls": 98, "device_self_duration": 5822, "device_total_duration": 5822, "host_self_duration": 1113, "host_total_duration": 1869, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::max_pool2d_with_indices_backward", "calls": 2, "device_self_duration": 2204, "device_total_duration": 2434, "host_self_duration": 41, "host_total_duration": 89, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_foreach_add_", "calls": 4, "device_self_duration": 1558, "device_total_duration": 1558, "host_self_duration": 1822, "host_total_duration": 2118, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_foreach_mul_", "calls": 2, "device_self_duration": 554, "device_total_duration": 554, "host_self_duration": 849, "host_total_duration": 1112, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::max_pool2d_with_indices", "calls": 2, "device_self_duration": 455, "device_total_duration": 455, "host_self_duration": 91, "host_total_duration": 109, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::fill_", "calls": 6, "device_self_duration": 234, "device_total_duration": 234, "host_self_duration": 73, "host_total_duration": 171, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::mm", "calls": 4, "device_self_duration": 87, "device_total_duration": 87, "host_self_duration": 133, "host_total_duration": 193, "tc_eligible": "Yes", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::mean", "calls": 2, "device_self_duration": 71, "device_total_duration": 71, "host_self_duration": 68, "host_total_duration": 85, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::addmm", "calls": 2, "device_self_duration": 54, "device_total_duration": 54, "host_self_duration": 152, "host_total_duration": 188, "tc_eligible": "Yes", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::div", "calls": 66, "device_self_duration": 40, "device_total_duration": 40, "host_self_duration": 4119, "host_total_duration": 4705, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_log_softmax_backward_data", "calls": 2, "device_self_duration": 17, "device_total_duration": 17, "host_self_duration": 38, "host_total_duration": 53, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::sum", "calls": 2, "device_self_duration": 16, "device_total_duration": 16, "host_self_duration": 51, "host_total_duration": 66, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_log_softmax", "calls": 2, "device_self_duration": 15, "device_total_duration": 15, "host_self_duration": 52, "host_total_duration": 69, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::nll_loss_forward", "calls": 2, "device_self_duration": 6, "device_total_duration": 6, "host_self_duration": 55, "host_total_duration": 77, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::nll_loss_backward", "calls": 2, "device_self_duration": 4, "device_total_duration": 6, "host_self_duration": 63, "host_total_duration": 155, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::lift_fresh", "calls": 66, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 104, "host_total_duration": 104, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::view", "calls": 284, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 739, "host_total_duration": 739, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::as_strided", "calls": 78, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 133, "host_total_duration": 133, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::permute", "calls": 64, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 424, "host_total_duration": 536, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::empty", "calls": 1126, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 8250, "host_total_duration": 8250, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::empty_like", "calls": 172, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 716, "host_total_duration": 2125, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::clone", "calls": 64, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 320, "host_total_duration": 3753, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::contiguous", "calls": 64, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 104, "host_total_duration": 3857, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::empty_strided", "calls": 134, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 484, "host_total_duration": 484, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::_to_copy", "calls": 132, "device_self_duration": 0, "device_total_duration": 8261, "host_self_duration": 712, "host_total_duration": 12853, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::to", "calls": 242, "device_self_duration": 0, "device_total_duration": 8261, "host_self_duration": 393, "host_total_duration": 13246, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::slice", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 12, "host_total_duration": 14, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::narrow", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 26, "host_total_duration": 40, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::cat", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 10715, "host_total_duration": 10755, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::stack", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 177, "host_total_duration": 10937, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "detach_", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 3, "host_total_duration": 3, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::detach_", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 6, "host_total_duration": 9, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::_convolution", "calls": 106, "device_self_duration": 0, "device_total_duration": 35039, "host_self_duration": 689, "host_total_duration": 9995, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::convolution", "calls": 106, "device_self_duration": 0, "device_total_duration": 35039, "host_self_duration": 1114, "host_total_duration": 11109, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::conv2d", "calls": 106, "device_self_duration": 0, "device_total_duration": 35039, "host_self_duration": 444, "host_total_duration": 11553, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_batch_norm_impl_index", "calls": 106, "device_self_duration": 0, "device_total_duration": 10933, "host_self_duration": 538, "host_total_duration": 11600, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::batch_norm", "calls": 106, "device_self_duration": 0, "device_total_duration": 10933, "host_self_duration": 261, "host_total_duration": 11861, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::relu_", "calls": 98, "device_self_duration": 0, "device_total_duration": 5822, "host_self_duration": 1339, "host_total_duration": 3208, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::max_pool2d", "calls": 2, "device_self_duration": 0, "device_total_duration": 455, "host_self_duration": 15, "host_total_duration": 124, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::adaptive_avg_pool2d", "calls": 2, "device_self_duration": 0, "device_total_duration": 71, "host_self_duration": 21, "host_total_duration": 106, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::flatten", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 7, "host_total_duration": 19, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::transpose", "calls": 10, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 34, "host_total_duration": 50, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::t", "calls": 10, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 40, "host_total_duration": 90, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::linear", "calls": 2, "device_self_duration": 0, "device_total_duration": 54, "host_self_duration": 22, "host_total_duration": 242, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::log_softmax", "calls": 2, "device_self_duration": 0, "device_total_duration": 15, "host_self_duration": 16, "host_total_duration": 85, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::resize_", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 2, "host_total_duration": 2, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::nll_loss", "calls": 2, "device_self_duration": 0, "device_total_duration": 6, "host_self_duration": 11, "host_total_duration": 88, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::nll_loss_nd", "calls": 2, "device_self_duration": 0, "device_total_duration": 6, "host_self_duration": 11, "host_total_duration": 99, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::cross_entropy_loss", "calls": 2, "device_self_duration": 0, "device_total_duration": 21, "host_self_duration": 25, "host_total_duration": 209, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::ones_like", "calls": 2, "device_self_duration": 0, "device_total_duration": 2, "host_self_duration": 14, "host_total_duration": 135, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::zero_", "calls": 4, "device_self_duration": 0, "device_total_duration": 232, "host_self_duration": 17, "host_total_duration": 112, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "NllLossBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 6, "host_self_duration": 30, "host_total_duration": 185, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: NllLossBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 6, "host_self_duration": 38, "host_total_duration": 223, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "LogSoftmaxBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 17, "host_self_duration": 17, "host_total_duration": 70, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: LogSoftmaxBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 17, "host_self_duration": 16, "host_total_duration": 86, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "AddmmBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 87, "host_self_duration": 31, "host_total_duration": 271, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: AddmmBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 103, "host_self_duration": 49, "host_total_duration": 391, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "detach", "calls": 322, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 645, "host_total_duration": 645, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::detach", "calls": 322, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 328, "host_total_duration": 973, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "torch::autograd::AccumulateGrad", "calls": 322, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 726, "host_total_duration": 1699, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: torch::autograd::AccumulateGrad", "calls": 322, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 1830, "host_total_duration": 3529, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "TBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 3, "host_total_duration": 14, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: TBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 10, "host_total_duration": 24, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::reshape", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 5, "host_total_duration": 8, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "ViewBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 9, "host_total_duration": 17, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: ViewBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 20, "host_total_duration": 37, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::expand", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 13, "host_total_duration": 16, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "MeanBackward1", "calls": 2, "device_self_duration": 0, "device_total_duration": 40, "host_self_duration": 27, "host_total_duration": 140, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: MeanBackward1", "calls": 2, "device_self_duration": 0, "device_total_duration": 40, "host_self_duration": 16, "host_total_duration": 156, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "ReluBackward0", "calls": 98, "device_self_duration": 0, "device_total_duration": 8737, "host_self_duration": 383, "host_total_duration": 3040, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: ReluBackward0", "calls": 98, "device_self_duration": 0, "device_total_duration": 8737, "host_self_duration": 750, "host_total_duration": 3790, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "AddBackward0", "calls": 32, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 27, "host_total_duration": 27, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: AddBackward0", "calls": 32, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 196, "host_total_duration": 223, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "CudnnBatchNormBackward0", "calls": 106, "device_self_duration": 0, "device_total_duration": 20418, "host_self_duration": 556, "host_total_duration": 8144, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: CudnnBatchNormBackward0", "calls": 106, "device_self_duration": 0, "device_total_duration": 20418, "host_self_duration": 1447, "host_total_duration": 9591, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "ConvolutionBackward0", "calls": 106, "device_self_duration": 0, "device_total_duration": 81825, "host_self_duration": 644, "host_total_duration": 14689, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: ConvolutionBackward0", "calls": 106, "device_self_duration": 0, "device_total_duration": 86882, "host_self_duration": 1443, "host_total_duration": 16779, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "MaxPool2DWithIndicesBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 2434, "host_self_duration": 23, "host_total_duration": 112, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: MaxPool2DWithIndicesBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 2434, "host_self_duration": 18, "host_total_duration": 130, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::result_type", "calls": 966, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 275, "host_total_duration": 275, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}]}
{"metadata": {"sort": "Total Duration (us)"}, "data": {"columns": [{"type": "string", "name": "Name"}, {"type": "string", "name": "Tensor Cores Used", "tooltip": "Whether this kernel uses Tensor Cores."}, {"type": "number", "name": "Calls"}, {"type": "number", "name": "Total Duration (us)"}, {"type": "number", "name": "Mean Duration (us)"}, {"type": "number", "name": "Max Duration (us)"}, {"type": "number", "name": "Min Duration (us)"}, {"type": "number", "name": "Mean Blocks Per SM", "tooltip": "Blocks Per SM = blocks of this kernel / SM number of this GPU.\nIf this number is less than 1, it indicates the GPU multiprocessors are not fully utilized.\n\"Mean Blocks per SM\" is the weighted average of all calls of this kernel, using each call's execution duration as weight."}, {"type": "number", "name": "Mean Est. Achieved Occupancy (%)", "tooltip": "Est. Achieved Occupancy:\nFor most cases such as memory bandwidth bounded kernels, the higher the better. Occupancy is the ratio of active warps on an SM to the maximum number of active warps supported by the SM. The theoretical occupancy of a kernel is upper limit occupancy of this kernel, limited by multiple factors such as kernel shape, kernel used resource, and the GPU compute capability.\nEst. Achieved Occupancy of a kernel, OCC_K = min(threads of the kernel / SM number / max threads per SM, theoretical occupancy of the kernel). This \"Mean\" number is the weighted average of all calls' OCC_K of the kernel, using each call's execution duration as weight. It shows fine-grained low-level GPU utilization."}], "rows": [["volta_sgemm_64x64_nt", "No", 50, 10998, 220, 470, 78, 24.13, 21.44], ["void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)", "No", 64, 10085, 158, 365, 46, 401.38, 100.0], ["void cudnn::detail::dgrad_engine<float, 512, 6, 5, 3, 3, 3, false>(int, int, int, float const*, int, float const*, int, float*, kernel_grad_params, unsigned long long, int, unsigned long long, int, float, int, int, int)", "No", 12, 9543, 795, 972, 600, 73.49, 31.0], ["void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 128, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)", "No", 22, 9076, 413, 750, 208, 2.42, 57.15], ["void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 32, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)", "No", 64, 8754, 137, 357, 46, 6.97, 70.03], ["void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)", "No", 98, 8737, 89, 364, 12, 328.45, 100.0], ["volta_sgemm_32x128_nt", "No", 30, 8546, 285, 520, 145, 19.67, 47.5], ["volta_sgemm_32x128_tn", "No", 24, 7101, 296, 624, 264, 21.69, 48.26], ["void wgrad_alg0_engine<float, 128, 6, 7, 3, 3, 5, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", "No", 16, 6291, 393, 468, 189, 5.71, 37.82], ["cudnn_infer_volta_scudnn_128x64_relu_medium_nn_v1", "No", 20, 6099, 305, 578, 246, 13.93, 25.0], ["volta_sgemm_64x64_nn", "No", 26, 5908, 227, 477, 85, 40.33, 24.61], ["void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)", "No", 98, 5822, 59, 249, 3, 336.32, 100.0], ["void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 128, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)", "No", 22, 4846, 220, 403, 103, 2.44, 41.39], ["cudnn_infer_volta_scudnn_128x64_stridedB_interior_nn_v1", "No", 16, 4388, 274, 289, 269, 3.17, 16.05], ["cudnn_train_volta_scudnn_128x128_stridedB_splitK_interior_nn_v1", "No", 10, 4213, 421, 608, 305, 5.6, 25.0], ["void wgrad_alg0_engine<float, 128, 6, 8, 3, 3, 5, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", "No", 10, 4134, 413, 649, 352, 4.19, 25.0], ["cudnn_infer_volta_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1", "No", 12, 3995, 333, 342, 329, 22.4, 25.0], ["cudnn_infer_volta_scudnn_128x64_relu_small_nn_v1", "No", 14, 3830, 274, 282, 260, 4.47, 17.75], ["cudnn_train_volta_scudnn_128x128_stridedB_splitK_medium_nn_v1", "No", 8, 3746, 468, 658, 311, 7.78, 25.0], ["volta_sgemm_64x64_tn", "No", 6, 3566, 594, 650, 569, 5.57, 12.08], ["volta_sgemm_32x128_nn", "No", 14, 3538, 253, 505, 154, 16.0, 50.0], ["cudnn_infer_volta_scudnn_128x128_relu_interior_nn_v1", "No", 10, 3199, 320, 571, 245, 8.06, 25.0], ["void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 32, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)", "No", 26, 2957, 114, 207, 49, 5.04, 47.62], ["volta_sgemm_128x32_nt", "No", 8, 2671, 334, 441, 18, 0.98, 11.54], ["cudnn_infer_volta_scudnn_128x64_stridedB_small_nn_v1", "No", 10, 2559, 256, 267, 250, 9.8, 19.0], ["cudnn_train_volta_scudnn_128x64_stridedB_splitK_xregs_large_nn_v1", "No", 4, 2468, 617, 619, 615, 15.77, 19.0], ["cudnn_infer_volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1", "No", 4, 2369, 592, 620, 519, 3.71, 25.0], ["void precomputed_convolve_sgemm<float, 512, 6, 7, 4, 3, 5, 1, false>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, bool, float const*, float const*, int*)", "No", 4, 2309, 577, 623, 534, 4.95, 25.0], ["void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)", "No", 56, 2276, 41, 83, 12, 12.58, 75.0], ["void at::native::(anonymous namespace)::max_pool_backward_nchw<float, float>(float const*, long const*, int, long, long, long, int, int, int, int, int, int, int, int, int, int, float*)", "No", 2, 2204, 1102, 1105, 1099, 1254.4, 100.0], ["void wgrad_alg0_engine<float, 128, 5, 5, 3, 3, 3, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", "No", 2, 1866, 933, 936, 930, 100.0, 31.0], ["void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float)", "No", 13, 1808, 139, 253, 28, 2.97, 74.16], ["void cudnn::winograd_nonfused::winogradForwardOutput4x4<float, float>(cudnn::winograd_nonfused::WinogradOutputParams<float, float>)", "No", 40, 1669, 42, 68, 19, 10.09, 63.0], ["void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 512, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)", "No", 2, 1659, 830, 832, 827, 0.8, 20.0], ["void cudnn::winograd_nonfused::winogradWgradDelta4x4<float, float>(cudnn::winograd_nonfused::WinogradDeltaParams<float, float>)", "No", 26, 1520, 58, 124, 18, 15.65, 50.0], ["void cudnn::winograd_nonfused::winogradForwardData4x4<float, float>(cudnn::winograd_nonfused::WinogradDataParams<float, float>)", "No", 40, 1518, 38, 61, 18, 10.04, 50.0], ["void cudnn::winograd_nonfused::winogradWgradData4x4<float, float>(cudnn::winograd_nonfused::WinogradDataParams<float, float>)", "No", 26, 1515, 58, 122, 21, 15.48, 38.0], ["void cudnn::cnn::reduce_wgrad_nchw_helper<float, float>(void*, void const*, float, int, int)", "No", 30, 1132, 38, 48, 3, 24.47, 97.06], ["cudnn_infer_volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1", "No", 4, 1113, 278, 280, 276, 2.6, 25.0], ["cudnn_infer_volta_scudnn_128x64_relu_xregs_large_nn_v1", "No", 2, 1031, 516, 517, 514, 4.9, 19.0], ["cudnn_infer_volta_scudnn_128x128_relu_small_nn_v1", "No", 2, 1001, 500, 502, 499, 9.8, 25.0], ["void cudnn::bn_bw_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_bw_1C11_args<float>)", "No", 18, 929, 52, 89, 23, 20.72, 50.0], ["void cudnn::winograd_nonfused::winogradForwardFilter4x4<float, float>(cudnn::winograd_nonfused::WinogradFilterParams<float, float>)", "No", 40, 877, 22, 67, 4, 8.68, 73.25], ["void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 512, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)", "No", 2, 854, 427, 429, 425, 0.8, 20.0], ["void cudnn::ops::scalePackedTensor_kernel<float, float>(long, float*, float)", "No", 12, 766, 64, 126, 21, 599.06, 100.0], ["void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<1>, at::native::(anonymous namespace)::BinaryOpScalarFunctor<float, 1, 1, 0>, std::multiplies<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<1>, at::native::(anonymous namespace)::BinaryOpScalarFunctor<float, 1, 1, 0>, std::multiplies<float>, float)", "No", 6, 554, 92, 194, 20, 3.31, 82.83], ["void cudnn::winograd_nonfused::winogradWgradOutput4x4<float, float>(cudnn::winograd_nonfused::WinogradWgradOutputParams<float, float>)", "No", 26, 464, 18, 63, 4, 7.99, 41.14], ["void at::native::(anonymous namespace)::max_pool_forward_nchw<float, float>(int, float const*, long, long, long, int, int, int, int, int, int, int, int, int, int, float*, long*)", "No", 2, 455, 228, 228, 227, 313.6, 100.0], ["void cudnn::ops::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)0>(cudnn::ops::nchw2nhwc_params_t<float>, float const*, float*)", "No", 12, 402, 34, 65, 9, 65.36, 100.0], ["void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)", "No", 6, 234, 39, 115, 1, 616.49, 98.33], ["void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)", "No", 106, 130, 1, 2, 1, 0.01, 0.0], ["void cask_cudnn_infer::computeOffsetsKernel<false, false>(cask_cudnn_infer::ComputeOffsetsParams)", "No", 56, 100, 2, 2, 1, 0.06, 0.72], ["void cudnn::ops::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::ops::nhwc2nchw_params_t<float>, float const*, float*)", "No", 8, 80, 10, 16, 7, 16.64, 100.0], ["void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MeanOps<float, float, float, float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MeanOps<float, float, float, float>, unsigned int, float, 4>)", "No", 2, 71, 36, 36, 35, 51.2, 100.0], ["void cask_cudnn_infer::computeOffsetsKernel<true, false>(cask_cudnn_infer::ComputeOffsetsParams)", "No", 26, 52, 2, 2, 2, 0.01, 0.0], ["void cudnn::winograd::generateWinogradTilesKernel<0, float, float>(cudnn::winograd::GenerateWinogradTilesParams<float, float>)", "No", 12, 48, 4, 5, 3, 0.4, 3.0], ["volta_sgemm_64x32_sliced1x4_tn", "No", 2, 47, 24, 24, 23, 1.0, 13.0], ["volta_sgemm_64x32_sliced1x4_nn", "No", 2, 42, 21, 21, 21, 2.0, 25.0], ["void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})", "No", 2, 40, 20, 20, 20, 156.8, 100.0], ["cask_cudnn_infer::computeBOffsetsKernel(cask_cudnn_infer::ComputeBOffsetsParams)", "No", 26, 34, 1, 2, 1, 0.03, 0.0], ["cask_cudnn_train::computeWgradBOffsetsKernel(cask_cudnn_train::ComputeWgradBOffsetsParams)", "No", 22, 33, 2, 2, 1, 0.03, 0.0], ["cask_cudnn_train::computeWgradSplitKOffsetsKernel(cask_cudnn_train::ComputeSplitKOffsetsParams)", "No", 22, 32, 1, 2, 1, 0.15, 1.69], ["void (anonymous namespace)::softmax_warp_backward<float, float, float, 10, true, false>(float*, float const*, float const*, int, int, int, bool const*)", "No", 2, 17, 8, 9, 8, 0.1, 1.0], ["void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)", "No", 2, 16, 8, 8, 8, 0.03, 0.0], ["void (anonymous namespace)::softmax_warp_forward<float, float, float, 10, true, false>(float*, float const*, int, int, int, bool const*, int, bool)", "No", 2, 15, 8, 8, 7, 0.1, 1.0], ["void cudnn::cnn::kern_precompute_indices<false>(int*, int, int, int, int, int)", "No", 4, 12, 3, 3, 3, 12.8, 1.0], ["void splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)", "No", 2, 8, 4, 4, 4, 1.6, 40.0], ["void splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)", "No", 2, 7, 4, 4, 3, 0.8, 20.0], ["void at::native::(anonymous namespace)::nll_loss_forward_reduce_cuda_kernel_2d<float, float, long>(float*, float*, float const*, long const*, float const*, bool, long, long, long, long)", "No", 2, 6, 3, 3, 3, 0.01, 0.0], ["void at::native::(anonymous namespace)::nll_loss_backward_reduce_cuda_kernel_2d<float, long>(float*, float const*, long const*, float const*, float const*, bool, int, int, long, long)", "No", 2, 4, 2, 2, 2, 0.01, 0.0]]}}
{"total": {"columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["volta_sgemm_64x64_nt", 10998], ["void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)", 10085], ["void cudnn::detail::dgrad_engine<float, 512, 6, 5, 3, 3, 3, false>(int, int, int, float const*, int, float const*, int, float*, kernel_grad_params, unsigned long long, int, unsigned long long, int, float, int, int, int)", 9543], ["void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 128, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)", 9076], ["void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 32, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)", 8754], ["void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)", 8737], ["volta_sgemm_32x128_nt", 8546], ["volta_sgemm_32x128_tn", 7101], ["void wgrad_alg0_engine<float, 128, 6, 7, 3, 3, 5, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 6291], ["cudnn_infer_volta_scudnn_128x64_relu_medium_nn_v1", 6099], ["volta_sgemm_64x64_nn", 5908], ["void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)", 5822], ["void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 128, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)", 4846], ["cudnn_infer_volta_scudnn_128x64_stridedB_interior_nn_v1", 4388], ["cudnn_train_volta_scudnn_128x128_stridedB_splitK_interior_nn_v1", 4213], ["void wgrad_alg0_engine<float, 128, 6, 8, 3, 3, 5, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 4134], ["cudnn_infer_volta_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1", 3995], ["cudnn_infer_volta_scudnn_128x64_relu_small_nn_v1", 3830], ["cudnn_train_volta_scudnn_128x128_stridedB_splitK_medium_nn_v1", 3746], ["volta_sgemm_64x64_tn", 3566], ["volta_sgemm_32x128_nn", 3538], ["cudnn_infer_volta_scudnn_128x128_relu_interior_nn_v1", 3199], ["void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 32, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)", 2957], ["volta_sgemm_128x32_nt", 2671], ["cudnn_infer_volta_scudnn_128x64_stridedB_small_nn_v1", 2559], ["cudnn_train_volta_scudnn_128x64_stridedB_splitK_xregs_large_nn_v1", 2468], ["cudnn_infer_volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1", 2369], ["void precomputed_convolve_sgemm<float, 512, 6, 7, 4, 3, 5, 1, false>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, bool, float const*, float const*, int*)", 2309], ["void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)", 2276], ["void at::native::(anonymous namespace)::max_pool_backward_nchw<float, float>(float const*, long const*, int, long, long, long, int, int, int, int, int, int, int, int, int, int, float*)", 2204], ["void wgrad_alg0_engine<float, 128, 5, 5, 3, 3, 3, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 1866], ["void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float)", 1808], ["void cudnn::winograd_nonfused::winogradForwardOutput4x4<float, float>(cudnn::winograd_nonfused::WinogradOutputParams<float, float>)", 1669], ["void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 512, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)", 1659], ["void cudnn::winograd_nonfused::winogradWgradDelta4x4<float, float>(cudnn::winograd_nonfused::WinogradDeltaParams<float, float>)", 1520], ["void cudnn::winograd_nonfused::winogradForwardData4x4<float, float>(cudnn::winograd_nonfused::WinogradDataParams<float, float>)", 1518], ["void cudnn::winograd_nonfused::winogradWgradData4x4<float, float>(cudnn::winograd_nonfused::WinogradDataParams<float, float>)", 1515], ["void cudnn::cnn::reduce_wgrad_nchw_helper<float, float>(void*, void const*, float, int, int)", 1132], ["cudnn_infer_volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1", 1113], ["cudnn_infer_volta_scudnn_128x64_relu_xregs_large_nn_v1", 1031], ["cudnn_infer_volta_scudnn_128x128_relu_small_nn_v1", 1001], ["void cudnn::bn_bw_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_bw_1C11_args<float>)", 929], ["void cudnn::winograd_nonfused::winogradForwardFilter4x4<float, float>(cudnn::winograd_nonfused::WinogradFilterParams<float, float>)", 877], ["void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 512, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)", 854], ["void cudnn::ops::scalePackedTensor_kernel<float, float>(long, float*, float)", 766], ["void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<1>, at::native::(anonymous namespace)::BinaryOpScalarFunctor<float, 1, 1, 0>, std::multiplies<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<1>, at::native::(anonymous namespace)::BinaryOpScalarFunctor<float, 1, 1, 0>, std::multiplies<float>, float)", 554], ["void cudnn::winograd_nonfused::winogradWgradOutput4x4<float, float>(cudnn::winograd_nonfused::WinogradWgradOutputParams<float, float>)", 464], ["void at::native::(anonymous namespace)::max_pool_forward_nchw<float, float>(int, float const*, long, long, long, int, int, int, int, int, int, int, int, int, int, float*, long*)", 455], ["void cudnn::ops::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)0>(cudnn::ops::nchw2nhwc_params_t<float>, float const*, float*)", 402], ["void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)", 234], ["void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)", 130], ["void cask_cudnn_infer::computeOffsetsKernel<false, false>(cask_cudnn_infer::ComputeOffsetsParams)", 100], ["void cudnn::ops::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::ops::nhwc2nchw_params_t<float>, float const*, float*)", 80], ["void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MeanOps<float, float, float, float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MeanOps<float, float, float, float>, unsigned int, float, 4>)", 71], ["void cask_cudnn_infer::computeOffsetsKernel<true, false>(cask_cudnn_infer::ComputeOffsetsParams)", 52], ["void cudnn::winograd::generateWinogradTilesKernel<0, float, float>(cudnn::winograd::GenerateWinogradTilesParams<float, float>)", 48], ["volta_sgemm_64x32_sliced1x4_tn", 47], ["volta_sgemm_64x32_sliced1x4_nn", 42], ["void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})", 40], ["cask_cudnn_infer::computeBOffsetsKernel(cask_cudnn_infer::ComputeBOffsetsParams)", 34], ["cask_cudnn_train::computeWgradBOffsetsKernel(cask_cudnn_train::ComputeWgradBOffsetsParams)", 33], ["cask_cudnn_train::computeWgradSplitKOffsetsKernel(cask_cudnn_train::ComputeSplitKOffsetsParams)", 32], ["void (anonymous namespace)::softmax_warp_backward<float, float, float, 10, true, false>(float*, float const*, float const*, int, int, int, bool const*)", 17], ["void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)", 16], ["void (anonymous namespace)::softmax_warp_forward<float, float, float, 10, true, false>(float*, float const*, int, int, int, bool const*, int, bool)", 15], ["void cudnn::cnn::kern_precompute_indices<false>(int*, int, int, int, int, int)", 12], ["void splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)", 8], ["void splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)", 7], ["void at::native::(anonymous namespace)::nll_loss_forward_reduce_cuda_kernel_2d<float, float, long>(float*, float*, float const*, long const*, float const*, bool, long, long, long, long)", 6], ["void at::native::(anonymous namespace)::nll_loss_backward_reduce_cuda_kernel_2d<float, long>(float*, float const*, long const*, float const*, float const*, bool, int, int, long, long)", 4]]}}
{"steps": {"columns": [{"type": "string", "name": "Step"}, {"type": "number", "name": "Kernel"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Memcpy"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Memset"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Runtime"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "DataLoader"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "CPU Exec"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Other"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}], "rows": [["1", 28197, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 1<br>Total: 98905us<br><b>Kernel: 28197us</b><br>Percentage: 28.51%</div>", 780, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 1<br>Total: 98905us<br><b>Memcpy: 780us</b><br>Percentage: 0.79%</div>", 0, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 1<br>Total: 98905us<br><b>Memset: 0us</b><br>Percentage: 0.0%</div>", 6559, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 1<br>Total: 98905us<br><b>Runtime: 6559us</b><br>Percentage: 6.63%</div>", 44881, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 1<br>Total: 98905us<br><b>DataLoader: 44881us</b><br>Percentage: 45.38%</div>", 14720, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 1<br>Total: 98905us<br><b>CPU Exec: 14720us</b><br>Percentage: 14.88%</div>", 3768, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 1<br>Total: 98905us<br><b>Other: 3768us</b><br>Percentage: 3.81%</div>"], ["2", 28204, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 93609us<br><b>Kernel: 28204us</b><br>Percentage: 30.13%</div>", 751, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 93609us<br><b>Memcpy: 751us</b><br>Percentage: 0.8%</div>", 0, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 93609us<br><b>Memset: 0us</b><br>Percentage: 0.0%</div>", 6119, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 93609us<br><b>Runtime: 6119us</b><br>Percentage: 6.54%</div>", 46532, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 93609us<br><b>DataLoader: 46532us</b><br>Percentage: 49.71%</div>", 9910, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 93609us<br><b>CPU Exec: 9910us</b><br>Percentage: 10.59%</div>", 2093, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 2<br>Total: 93609us<br><b>Other: 2093us</b><br>Percentage: 2.24%</div>"]]}, "performance": [{"name": "Average Step Time", "description": "", "value": 96257, "extra": 100, "children": [{"name": "Kernel", "description": "", "value": 28200, "extra": 29.3}, {"name": "Memcpy", "description": "", "value": 766, "extra": 0.8}, {"name": "Memset", "description": "", "value": 0, "extra": 0.0}, {"name": "Runtime", "description": "", "value": 6339, "extra": 6.59}, {"name": "DataLoader", "description": "", "value": 45706, "extra": 47.48}, {"name": "CPU Exec", "description": "", "value": 12315, "extra": 12.79}, {"name": "Other", "description": "", "value": 2930, "extra": 3.04}]}], "recommendations": "<ul><li>This run has high time cost on input data loading. 47.5% of the step time is in DataLoader. You could try to set num_workers on DataLoader's construction and <a href=\"https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading\" target=\"_blank\">enable multi-processes on data loading</a>.</li><li>MLU 0 has low utilization. You could try to increase batch size to improve. Note: Increasing batch size may affect the speed and stability of model convergence.</li></ul>", "environments": [{"title": "Number of Worker(s)", "value": "1"}, {"title": "Device Type", "value": "MLU"}], "gpu_metrics": {"title": "MLU Summary", "data": [{"title": "MLU 0:", "value": ""}, {"title": "Name", "value": "MLU590-H8"}, {"title": "Memory", "value": "80.0 GB"}, {"title": "Compute Capability", "value": "5.0"}, {"title": "MLU Utilization", "value": "29.29 %"}], "tooltip": "The MLU usage metrics:\n\nMLU Utilization:\nMLU busy time / All steps time. The higher, the better. MLU busy time is the time during which there is at least one MLU kernel running on it. All steps time is the total time of all profiler steps(or called as iterations).\n"}}
{"device_total_time": {"title": "Device Total Time (us)", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["autograd::engine::evaluate_function: ConvolutionBackward0", 20123], ["aten::convolution_backward", 17551], ["ConvolutionBackward0", 17551], ["aten::native_batch_norm_backward", 8489], ["NativeBatchNormBackward0", 8489], ["autograd::engine::evaluate_function: NativeBatchNormBackward0", 8489], ["aten::convolution", 7867], ["aten::conv2d", 7867], ["aten::threshold_backward", 7524], ["aten::add_", 7129], ["aten::native_batch_norm", 5907], ["aten::_batch_norm_impl_index", 5907], ["aten::batch_norm", 5907], ["ReluBackward0", 3762], ["autograd::engine::evaluate_function: ReluBackward0", 3762], ["aten::clamp_min_", 3206], ["aten::relu_", 3206], ["aten::copy_", 1762], ["aten::_to_copy", 1531], ["aten::to", 1531], ["aten::mul_", 921], ["aten::max_pool2d_with_indices_backward", 628], ["MaxPool2DWithIndicesBackward0", 628], ["autograd::engine::evaluate_function: MaxPool2DWithIndicesBackward0", 628], ["aten::max_pool2d_with_indices", 370], ["aten::max_pool2d", 370], ["torch::autograd::AccumulateGrad", 224], ["autograd::engine::evaluate_function: torch::autograd::AccumulateGrad", 224], ["aten::div", 102], ["MeanBackward1", 102], ["autograd::engine::evaluate_function: MeanBackward1", 102], ["aten::mean", 72], ["aten::adaptive_avg_pool2d", 72], ["autograd::engine::evaluate_function: AddmmBackward0", 55], ["aten::mm", 46], ["AddmmBackward0", 46], ["aten::cross_entropy_loss", 35], ["aten::addmm", 32], ["aten::linear", 32], ["aten::_log_softmax_backward_data", 28], ["LogSoftmaxBackward0", 28], ["autograd::engine::evaluate_function: LogSoftmaxBackward0", 28], ["aten::_log_softmax", 18], ["aten::log_softmax", 18], ["aten::nll_loss_forward", 17], ["aten::nll_loss", 17], ["aten::nll_loss_nd", 17], ["AsStridedBackward1", 11], ["autograd::engine::evaluate_function: AsStridedBackward1", 11], ["aten::nll_loss_backward", 10], ["NllLossBackward0", 10], ["autograd::engine::evaluate_function: NllLossBackward0", 10], ["aten::sum", 9], ["aten::zero_", 6], ["aten::new_zeros", 4], ["aten::fill_", 2], ["aten::ones_like", 2]]}, "device_self_time": {"title": "Device Self Time (us)", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::convolution_backward", 17551], ["aten::native_batch_norm_backward", 8489], ["aten::convolution", 7867], ["aten::add_", 7129], ["aten::native_batch_norm", 5907], ["aten::threshold_backward", 3762], ["aten::clamp_min_", 3206], ["aten::copy_", 1762], ["aten::mul_", 921], ["aten::max_pool2d_with_indices_backward", 628], ["aten::max_pool2d_with_indices", 370], ["aten::div", 102], ["aten::mean", 72], ["aten::mm", 46], ["aten::addmm", 32], ["aten::_log_softmax_backward_data", 28], ["aten::_log_softmax", 18], ["aten::nll_loss_forward", 17], ["aten::sum", 9], ["aten::nll_loss_backward", 8], ["aten::zero_", 6], ["aten::fill_", 2]]}, "host_total_time": {"title": "Host Total Time (us)", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::conv2d", 17029], ["aten::convolution", 16583], ["aten::add_", 14169], ["autograd::engine::evaluate_function: ConvolutionBackward0", 13382], ["ConvolutionBackward0", 11824], ["aten::convolution_backward", 11216], ["aten::copy_", 10540], ["aten::batch_norm", 6845], ["aten::_batch_norm_impl_index", 6576], ["aten::to", 6434], ["aten::_to_copy", 6159], ["aten::native_batch_norm", 5663], ["aten::stack", 5608], ["aten::cat", 5558], ["aten::mul_", 5407], ["autograd::engine::evaluate_function: NativeBatchNormBackward0", 5178], ["aten::contiguous", 5089], ["aten::clone", 4970], ["aten::div", 4631], ["aten::empty", 4422], ["NativeBatchNormBackward0", 4319], ["aten::threshold_backward", 3992], ["autograd::engine::evaluate_function: torch::autograd::AccumulateGrad", 3967], ["aten::native_batch_norm_backward", 3759], ["aten::relu_", 3337], ["autograd::engine::evaluate_function: ReluBackward0", 3275], ["torch::autograd::AccumulateGrad", 3107], ["ReluBackward0", 2797], ["aten::empty_like", 2608], ["aten::clamp_min_", 2327], ["aten::detach", 896], ["aten::empty_strided", 833], ["detach", 625], ["aten::permute", 498], ["aten::item", 420], ["aten::as_strided", 398], ["autograd::engine::evaluate_function: AddmmBackward0", 370], ["aten::view", 296], ["aten::linear", 269], ["aten::cross_entropy_loss", 266], ["aten::adaptive_avg_pool2d", 263], ["autograd::engine::evaluate_function: MeanBackward1", 262], ["MeanBackward1", 249], ["autograd::engine::evaluate_function: NllLossBackward0", 245], ["AddmmBackward0", 238], ["aten::addmm", 224], ["aten::mean", 216], ["autograd::engine::evaluate_function: AsStridedBackward1", 208], ["NllLossBackward0", 200], ["AsStridedBackward1", 199], ["aten::new_empty_strided", 184], ["aten::mm", 168], ["aten::nll_loss_backward", 165], ["aten::nll_loss_nd", 163], ["aten::nll_loss", 157], ["aten::nll_loss_forward", 147], ["aten::zero_", 141], ["autograd::engine::evaluate_function: AddBackward0", 126], ["aten::t", 124], ["aten::ones_like", 123], ["aten::max_pool2d", 115], ["autograd::engine::evaluate_function: MaxPool2DWithIndicesBackward0", 106], ["aten::max_pool2d_with_indices", 101], ["aten::lift_fresh", 97], ["MaxPool2DWithIndicesBackward0", 94], ["aten::fill_", 87], ["aten::transpose", 86], ["aten::sum", 85], ["aten::new_zeros", 84], ["aten::log_softmax", 81], ["aten::max_pool2d_with_indices_backward", 76], ["autograd::engine::evaluate_function: LogSoftmaxBackward0", 73], ["aten::_local_scalar_dense", 72], ["aten::_log_softmax", 63], ["LogSoftmaxBackward0", 63], ["aten::_log_softmax_backward_data", 48], ["aten::narrow", 36], ["aten::flatten", 36], ["aten::expand", 24], ["AddBackward0", 23], ["aten::as_strided_", 21], ["autograd::engine::evaluate_function: TBackward0", 21], ["autograd::engine::evaluate_function: ViewBackward0", 21], ["aten::squeeze", 20], ["TBackward0", 15], ["ViewBackward0", 15], ["aten::slice", 13], ["aten::new_empty", 10], ["aten::detach_", 9], ["aten::reshape", 8], ["detach_", 4], ["aten::resize_", 2]]}, "host_self_time": {"title": "Host Self Time (us)", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::convolution", 10471], ["aten::copy_", 8351], ["aten::add_", 6947], ["aten::cat", 5522], ["aten::convolution_backward", 4507], ["aten::empty", 4422], ["aten::div", 4001], ["aten::native_batch_norm", 3044], ["aten::mul_", 2423], ["aten::native_batch_norm_backward", 1658], ["aten::threshold_backward", 1388], ["aten::clamp_min_", 1144], ["aten::empty_like", 1106], ["aten::relu_", 1010], ["autograd::engine::evaluate_function: torch::autograd::AccumulateGrad", 860], ["autograd::engine::evaluate_function: NativeBatchNormBackward0", 859], ["aten::empty_strided", 833], ["autograd::engine::evaluate_function: ConvolutionBackward0", 801], ["torch::autograd::AccumulateGrad", 767], ["aten::_batch_norm_impl_index", 642], ["detach", 625], ["aten::_to_copy", 619], ["ConvolutionBackward0", 608], ["NativeBatchNormBackward0", 560], ["autograd::engine::evaluate_function: ReluBackward0", 478], ["aten::conv2d", 446], ["aten::as_strided", 398], ["aten::clone", 390], ["aten::permute", 385], ["aten::item", 348], ["ReluBackward0", 312], ["aten::view", 296], ["aten::to", 275], ["aten::detach", 271], ["aten::batch_norm", 269], ["aten::mean", 132], ["aten::contiguous", 119], ["aten::addmm", 112], ["autograd::engine::evaluate_function: AddBackward0", 103], ["aten::lift_fresh", 97], ["aten::new_empty_strided", 86], ["aten::zero_", 74], ["aten::_local_scalar_dense", 72], ["aten::mm", 69], ["aten::max_pool2d_with_indices", 64], ["aten::transpose", 57], ["aten::nll_loss_forward", 56], ["aten::sum", 53], ["aten::nll_loss_backward", 48], ["aten::stack", 45], ["autograd::engine::evaluate_function: NllLossBackward0", 45], ["AsStridedBackward1", 42], ["autograd::engine::evaluate_function: AddmmBackward0", 39], ["aten::t", 38], ["aten::fill_", 38], ["aten::max_pool2d_with_indices_backward", 36], ["NllLossBackward0", 35], ["aten::_log_softmax", 33], ["aten::adaptive_avg_pool2d", 26], ["AddmmBackward0", 26], ["aten::_log_softmax_backward_data", 24], ["aten::narrow", 23], ["AddBackward0", 23], ["aten::cross_entropy_loss", 22], ["aten::as_strided_", 21], ["aten::expand", 20], ["aten::flatten", 19], ["MaxPool2DWithIndicesBackward0", 18], ["aten::linear", 17], ["aten::squeeze", 17], ["aten::log_softmax", 16], ["MeanBackward1", 16], ["LogSoftmaxBackward0", 15], ["aten::max_pool2d", 14], ["aten::new_zeros", 13], ["autograd::engine::evaluate_function: MeanBackward1", 13], ["autograd::engine::evaluate_function: MaxPool2DWithIndicesBackward0", 12], ["aten::slice", 10], ["aten::nll_loss", 10], ["aten::ones_like", 10], ["autograd::engine::evaluate_function: LogSoftmaxBackward0", 10], ["autograd::engine::evaluate_function: AsStridedBackward1", 9], ["ViewBackward0", 7], ["aten::nll_loss_nd", 6], ["autograd::engine::evaluate_function: TBackward0", 6], ["autograd::engine::evaluate_function: ViewBackward0", 6], ["aten::detach_", 5], ["detach_", 4], ["TBackward0", 4], ["aten::new_empty", 4], ["aten::reshape", 2], ["aten::resize_", 2]]}}
{"metadata": {"sort": "device_self_duration", "tooltips": {"tc_eligible": "Whether this operator is eligible to use Tensor Cores.", "tc_self_ratio": "Time of self-kernels with Tensor Cores / Time of self-kernels.", "tc_total_ratio": "Time of kernels with Tensor Cores / Time of kernels."}}, "data": [{"name": "aten::convolution_backward", "calls": 106, "device_self_duration": 17551, "device_total_duration": 17551, "host_self_duration": 4507, "host_total_duration": 11216, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::native_batch_norm_backward", "calls": 106, "device_self_duration": 8489, "device_total_duration": 8489, "host_self_duration": 1658, "host_total_duration": 3759, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::convolution", "calls": 106, "device_self_duration": 7867, "device_total_duration": 7867, "host_self_duration": 10471, "host_total_duration": 16583, "tc_eligible": "Yes", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::add_", "calls": 814, "device_self_duration": 7129, "device_total_duration": 7129, "host_self_duration": 6947, "host_total_duration": 14169, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::native_batch_norm", "calls": 106, "device_self_duration": 5907, "device_total_duration": 5907, "host_self_duration": 3044, "host_total_duration": 5663, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::threshold_backward", "calls": 196, "device_self_duration": 3762, "device_total_duration": 7524, "host_self_duration": 1388, "host_total_duration": 3992, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::clamp_min_", "calls": 98, "device_self_duration": 3206, "device_total_duration": 3206, "host_self_duration": 1144, "host_total_duration": 2327, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::copy_", "calls": 232, "device_self_duration": 1762, "device_total_duration": 1762, "host_self_duration": 8351, "host_total_duration": 10540, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::mul_", "calls": 322, "device_self_duration": 921, "device_total_duration": 921, "host_self_duration": 2423, "host_total_duration": 5407, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::max_pool2d_with_indices_backward", "calls": 2, "device_self_duration": 628, "device_total_duration": 628, "host_self_duration": 36, "host_total_duration": 76, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::max_pool2d_with_indices", "calls": 2, "device_self_duration": 370, "device_total_duration": 370, "host_self_duration": 64, "host_total_duration": 101, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::div", "calls": 66, "device_self_duration": 102, "device_total_duration": 102, "host_self_duration": 4001, "host_total_duration": 4631, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::mean", "calls": 2, "device_self_duration": 72, "device_total_duration": 72, "host_self_duration": 132, "host_total_duration": 216, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::mm", "calls": 4, "device_self_duration": 46, "device_total_duration": 46, "host_self_duration": 69, "host_total_duration": 168, "tc_eligible": "Yes", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::addmm", "calls": 2, "device_self_duration": 32, "device_total_duration": 32, "host_self_duration": 112, "host_total_duration": 224, "tc_eligible": "Yes", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_log_softmax_backward_data", "calls": 2, "device_self_duration": 28, "device_total_duration": 28, "host_self_duration": 24, "host_total_duration": 48, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_log_softmax", "calls": 2, "device_self_duration": 18, "device_total_duration": 18, "host_self_duration": 33, "host_total_duration": 63, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::nll_loss_forward", "calls": 2, "device_self_duration": 17, "device_total_duration": 17, "host_self_duration": 56, "host_total_duration": 147, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::sum", "calls": 2, "device_self_duration": 9, "device_total_duration": 9, "host_self_duration": 53, "host_total_duration": 85, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::nll_loss_backward", "calls": 2, "device_self_duration": 8, "device_total_duration": 10, "host_self_duration": 48, "host_total_duration": 165, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::zero_", "calls": 4, "device_self_duration": 6, "device_total_duration": 6, "host_self_duration": 74, "host_total_duration": 141, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::fill_", "calls": 2, "device_self_duration": 2, "device_total_duration": 2, "host_self_duration": 38, "host_total_duration": 87, "tc_eligible": "No", "tc_self_ratio": 0.0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::lift_fresh", "calls": 66, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 97, "host_total_duration": 97, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::view", "calls": 72, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 296, "host_total_duration": 296, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::as_strided", "calls": 240, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 398, "host_total_duration": 398, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::permute", "calls": 64, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 385, "host_total_duration": 498, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::empty", "calls": 1634, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 4422, "host_total_duration": 4422, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::empty_like", "calls": 490, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 1106, "host_total_duration": 2608, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::clone", "calls": 64, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 390, "host_total_duration": 4970, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::contiguous", "calls": 64, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 119, "host_total_duration": 5089, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::empty_strided", "calls": 276, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 833, "host_total_duration": 833, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::_to_copy", "calls": 132, "device_self_duration": 0, "device_total_duration": 1531, "host_self_duration": 619, "host_total_duration": 6159, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::to", "calls": 142, "device_self_duration": 0, "device_total_duration": 1531, "host_self_duration": 275, "host_total_duration": 6434, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::slice", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 10, "host_total_duration": 13, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::narrow", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 23, "host_total_duration": 36, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::cat", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 5522, "host_total_duration": 5558, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::stack", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 45, "host_total_duration": 5608, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "detach_", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 4, "host_total_duration": 4, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::detach_", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 5, "host_total_duration": 9, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::conv2d", "calls": 106, "device_self_duration": 0, "device_total_duration": 7867, "host_self_duration": 446, "host_total_duration": 17029, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_batch_norm_impl_index", "calls": 106, "device_self_duration": 0, "device_total_duration": 5907, "host_self_duration": 642, "host_total_duration": 6576, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::batch_norm", "calls": 106, "device_self_duration": 0, "device_total_duration": 5907, "host_self_duration": 269, "host_total_duration": 6845, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::relu_", "calls": 98, "device_self_duration": 0, "device_total_duration": 3206, "host_self_duration": 1010, "host_total_duration": 3337, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::max_pool2d", "calls": 2, "device_self_duration": 0, "device_total_duration": 370, "host_self_duration": 14, "host_total_duration": 115, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::as_strided_", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 21, "host_total_duration": 21, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::adaptive_avg_pool2d", "calls": 2, "device_self_duration": 0, "device_total_duration": 72, "host_self_duration": 26, "host_total_duration": 263, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::flatten", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 19, "host_total_duration": 36, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::transpose", "calls": 18, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 57, "host_total_duration": 86, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::t", "calls": 18, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 38, "host_total_duration": 124, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::expand", "calls": 4, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 20, "host_total_duration": 24, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::linear", "calls": 2, "device_self_duration": 0, "device_total_duration": 32, "host_self_duration": 17, "host_total_duration": 269, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::log_softmax", "calls": 2, "device_self_duration": 0, "device_total_duration": 18, "host_self_duration": 16, "host_total_duration": 81, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::nll_loss", "calls": 2, "device_self_duration": 0, "device_total_duration": 17, "host_self_duration": 10, "host_total_duration": 157, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::nll_loss_nd", "calls": 2, "device_self_duration": 0, "device_total_duration": 17, "host_self_duration": 6, "host_total_duration": 163, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::cross_entropy_loss", "calls": 2, "device_self_duration": 0, "device_total_duration": 35, "host_self_duration": 22, "host_total_duration": 266, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::_local_scalar_dense", "calls": 330, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 72, "host_total_duration": 72, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::item", "calls": 330, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 348, "host_total_duration": 420, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::ones_like", "calls": 2, "device_self_duration": 0, "device_total_duration": 2, "host_self_duration": 10, "host_total_duration": 123, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "NllLossBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 10, "host_self_duration": 35, "host_total_duration": 200, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: NllLossBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 10, "host_self_duration": 45, "host_total_duration": 245, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "LogSoftmaxBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 28, "host_self_duration": 15, "host_total_duration": 63, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: LogSoftmaxBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 28, "host_self_duration": 10, "host_total_duration": 73, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "AddmmBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 46, "host_self_duration": 26, "host_total_duration": 238, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: AddmmBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 55, "host_self_duration": 39, "host_total_duration": 370, "tc_eligible": "Yes", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "detach", "calls": 288, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 625, "host_total_duration": 625, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::detach", "calls": 288, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 271, "host_total_duration": 896, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "torch::autograd::AccumulateGrad", "calls": 322, "device_self_duration": 0, "device_total_duration": 224, "host_self_duration": 767, "host_total_duration": 3107, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: torch::autograd::AccumulateGrad", "calls": 322, "device_self_duration": 0, "device_total_duration": 224, "host_self_duration": 860, "host_total_duration": 3967, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "TBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 4, "host_total_duration": 15, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: TBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 6, "host_total_duration": 21, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::reshape", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 2, "host_total_duration": 8, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "ViewBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 7, "host_total_duration": 15, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: ViewBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 6, "host_total_duration": 21, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::squeeze", "calls": 4, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 17, "host_total_duration": 20, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::new_empty", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 4, "host_total_duration": 10, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "aten::new_zeros", "calls": 2, "device_self_duration": 0, "device_total_duration": 4, "host_self_duration": 13, "host_total_duration": 84, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "AsStridedBackward1", "calls": 2, "device_self_duration": 0, "device_total_duration": 11, "host_self_duration": 42, "host_total_duration": 199, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: AsStridedBackward1", "calls": 2, "device_self_duration": 0, "device_total_duration": 11, "host_self_duration": 9, "host_total_duration": 208, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::resize_", "calls": 2, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 2, "host_total_duration": 2, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "MeanBackward1", "calls": 2, "device_self_duration": 0, "device_total_duration": 102, "host_self_duration": 16, "host_total_duration": 249, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: MeanBackward1", "calls": 2, "device_self_duration": 0, "device_total_duration": 102, "host_self_duration": 13, "host_total_duration": 262, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "ReluBackward0", "calls": 98, "device_self_duration": 0, "device_total_duration": 3762, "host_self_duration": 312, "host_total_duration": 2797, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: ReluBackward0", "calls": 98, "device_self_duration": 0, "device_total_duration": 3762, "host_self_duration": 478, "host_total_duration": 3275, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "AddBackward0", "calls": 32, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 23, "host_total_duration": 23, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: AddBackward0", "calls": 32, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 103, "host_total_duration": 126, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "NativeBatchNormBackward0", "calls": 106, "device_self_duration": 0, "device_total_duration": 8489, "host_self_duration": 560, "host_total_duration": 4319, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: NativeBatchNormBackward0", "calls": 106, "device_self_duration": 0, "device_total_duration": 8489, "host_self_duration": 859, "host_total_duration": 5178, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "ConvolutionBackward0", "calls": 106, "device_self_duration": 0, "device_total_duration": 17551, "host_self_duration": 608, "host_total_duration": 11824, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: ConvolutionBackward0", "calls": 106, "device_self_duration": 0, "device_total_duration": 20123, "host_self_duration": 801, "host_total_duration": 13382, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "aten::new_empty_strided", "calls": 34, "device_self_duration": 0, "device_total_duration": 0, "host_self_duration": 86, "host_total_duration": 184, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0, "has_call_stack": false}, {"name": "MaxPool2DWithIndicesBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 628, "host_self_duration": 18, "host_total_duration": 94, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}, {"name": "autograd::engine::evaluate_function: MaxPool2DWithIndicesBackward0", "calls": 2, "device_self_duration": 0, "device_total_duration": 628, "host_self_duration": 12, "host_total_duration": 106, "tc_eligible": "No", "tc_self_ratio": 0, "tc_total_ratio": 0.0, "has_call_stack": false}]}
{"metadata": {"sort": "Total Duration (us)"}, "data": {"columns": [{"type": "string", "name": "Name"}, {"type": "string", "name": "Tensor Cores Used", "tooltip": "Whether this kernel uses Tensor Cores."}, {"type": "number", "name": "Calls"}, {"type": "number", "name": "Total Duration (us)"}, {"type": "number", "name": "Mean Duration (us)"}, {"type": "number", "name": "Max Duration (us)"}, {"type": "number", "name": "Min Duration (us)"}], "rows": [["MLUUnionXKernelBatchNormBackward(void*, void*, void*, void*, void*, void*, void*, void*, void*, void*, void*, float, unsigned long, int, int, int, cnnlDataType_t, cnnlActivationMode_t)", "No", 106, 8489, 80, 305, 18], ["MLUUnionXKernelBatchNormForward(void*, void*, void*, void*, void*, void*, void*, void*, void*, void*, float, float, unsigned long, int, int, int, int, int, cnnlDataType_t)", "No", 106, 5907, 56, 194, 17], ["void MLUOpTensorElementParam110ThreePipeline<float, float, float, float>(float const*, float const*, float*, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, cnnlOpTensorDesc_t, bool)", "No", 73, 5190, 71, 187, 18], ["void MLUUnion1KernelGemmRb<float, tfloat32_t, float, tfloat32_t, float, float>(void*, void*, void*, void*, float, float, int, int, int, int, int, int, int, int, int, int, float, float, bool, bool, bool)", "No", 64, 3905, 61, 100, 24], ["void MLUBlockKernel3StagePipelineV2ThresholdBackwardFast<float, float, float, float>(char*, char*, char*, unsigned long, unsigned long, float)", "No", 98, 3762, 38, 187, 7], ["void MLUUnionXKernelGepdot<float, tfloat32_t, float, tfloat32_t, float, float>(void*, void*, void*, void*, void*, float, float, int, int, int, int, int, int, int, bool, bool, int, int, float, float)", "No", 54, 3471, 64, 112, 40], ["void MLUBlockKernel3StagePipelineClipFast<float, float, float, float, OpStyle>(char*, char*, unsigned long, float, float, OpStyle)", "No", 98, 3206, 33, 129, 6], ["void MLUUnion1KernelGepp<float, tfloat32_t, float, float>(void*, void*, void*, void*, float, float, int, int, int, int, int, int, bool, bool, int, int, int, int)", "No", 58, 2739, 47, 83, 39], ["MLUUnionXKernelConvbpfIm2ColIn_float_float(void const*, void const*, void*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, bool)", "No", 26, 2395, 92, 123, 79], ["MLUDeconv3dDefault_true_float_float_float_float_float_float(void*, void const*, void const*, void const*, void*, void const*, void const*, void const*, void const*, void const*, void const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, bool, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, cnnlQuantizeRoundMode_t, bool, int, void const*, float, void const*, int, void const*, int, void const*, float, void const*, int, void const*, int, void const*, float, void const*, int, void const*)", "No", 14, 2263, 162, 340, 102], ["void MLUConvCoCi<0, 0, 0, float, float, float, float, float, float>(char*, char*, char*, char*, char*, char*, char*, char*, char*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, float, int, int, int, cnnlConvolutionCastMode_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, FuseParamHost, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlReorderType_t, cnnlReorderType_t, cnnlQuantizeRoundMode_t)", "No", 18, 2040, 113, 144, 97], ["void MLUOpTensorElementParam1x0<float, float, float, float>(float const*, float const*, float*, float, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, cnnlOpTensorDesc_t)", "No", 436, 1364, 3, 20, 1], ["void MLUBlockKernel3StagePipelineTransformFast<float, float, float, float>(char*, char*, unsigned long, float, float)", "No", 438, 1318, 3, 19, 1], ["void MLUOpTensorElementParam110<float, float, float, float>(float const*, float const*, float*, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, cnnlOpTensorDesc_t, bool)", "No", 427, 1259, 3, 17, 2], ["MLUUnionXKernelConvbpfIm2ColInStride_float_float(void const*, void const*, void*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, bool)", "No", 8, 997, 125, 192, 86], ["co_4_split_entry_CONVBPDATA_NO_STRIDE_F32_TF32_TF32_F32(void const*, void const*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, cnnlDataType_t, int, int, int, int, float, float, float, int, int, int, int, int, int, void*, void*, void*, void*, void*, void*, void*, void*, void*, int, int, cnnlQuantizeRoundMode_t)", "No", 10, 866, 87, 89, 85], ["void MLUAntsKernelGemmU1Ex<float, tfloat32_t, float, tfloat32_t, float, float, false>(void*, void*, void*, void*, int*, float*, int*, float*, float, float, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, bool, bool, bool)", "No", 18, 823, 46, 77, 37], ["void MLUConvCoCiSplit<0, 0, 0, float, float, float, float, float, float>(char const*, char*, char*, char*, char*, char*, char*, char*, char*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, float, int, int, int, cnnlConvolutionCastMode_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, FuseParamHost, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlReorderType_t, cnnlReorderType_t, cnnlQuantizeRoundMode_t)", "No", 8, 792, 99, 114, 93], ["void MLUTransposeKernel3DCSmallBlock<int, (Trans3DMode)2>(int const*, int*, DimSplit, DimSplit, DimSplit)", "No", 102, 683, 7, 16, 3], ["MLUUnionXKernelStrideGepdotIn_float_float_float_float(void const*, void const*, void*, void*, float, float, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, void const*, void const*, void const*, void const*, bool, bool, bool)", "No", 6, 656, 109, 122, 100], ["MLUKernelBlockPoolBackward_opt(void*, void*, void*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, cnnlPoolingMode_t, bool, bool, bool, bool, cnnlDataType_t)", "No", 2, 628, 314, 314, 314], ["nosplit_hw_entry_CONVBPDATA_WITHOUT_STRIDE_F32_F32_F32_F32_F32(void*, void const*, void const*, void const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, nosplitPartitionMode_t, int, float, void const*, void const*, int, float, void const*, void const*, int, float, int, int, int, void const*, void const*, int, int, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, bool, int, int, int, bool, cnnlQuantizeRoundMode_t)", "No", 6, 557, 93, 95, 89], ["void MLUConvCo1<0, 0, 0, float, float, float, float, float, float>(char const*, char*, char*, char*, char*, char*, char*, char*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, float, int, int, int, cnnlConvolutionCastMode_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, FuseParamHost, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlReorderType_t, cnnlReorderType_t, cnnlQuantizeRoundMode_t)", "No", 6, 508, 85, 87, 83], ["co_4_split_entry_CONVBPDATA_STRIDE_F32_TF32_TF32_F32(void const*, void const*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, cnnlDataType_t, int, int, int, int, float, float, float, int, int, int, int, int, int, void*, void*, void*, void*, void*, void*, void*, void*, void*, int, int, cnnlQuantizeRoundMode_t)", "No", 4, 464, 116, 118, 113], ["MLUUnion1KernelGepdotReduce(void*, void*, void*, float, int, int, int, int, cnnlDataType_t, cnnlDataType_t)", "No", 54, 419, 8, 12, 4], ["MLUUnion1PoolingForwardWithIndex(void*, void*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, cnnlDataType_t)", "No", 2, 370, 185, 186, 184], ["MLUUnion1KernelConvbpfReduce_float_float(void*, void*, unsigned long, int)", "No", 40, 333, 8, 17, 3], ["MLUUnion1ConvForwardGeppAnt_float_tfloat32_t_float_float(void const*, void const*, void const*, void*, int, int, int, int, int, int, int, int, int, float, float, int, int, float, float, void const*, void const*, void const*, void const*, bool)", "No", 4, 316, 79, 81, 77], ["void MLUConvImplicitGEMM<0, 0, 0>(char*, char*, char*, char*, char*, char*, char*, char*, char*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, float, int, int, int, cnnlConvolutionCastMode_t, int, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, FuseParamHost, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlQuantizeRoundMode_t)", "No", 2, 280, 140, 140, 140], ["MLUUnion1ConvBackwardDataGEPB_float_tfloat32_t_float_tfloat32_t_float(void const*, void const*, void*, int, int, int, int, int, int, int, int, int, float, float, int, int, float, float, void const*, void const*, void const*, void const*, bool, bool)", "No", 2, 203, 102, 106, 97], ["MLUUnion1ConvBackwardDataGeppAnt_float_tfloat32_t_float_tfloat32_t_float(void const*, void const*, void*, int, int, int, int, int, int, int, int, int, float, float, int, int, float, float, void const*, void const*, void const*, void const*, bool)", "No", 2, 175, 88, 88, 87], ["void MLUAntsKernelGemmU1Ex<float, tfloat32_t, float, tfloat32_t, float, float, true>(void*, void*, void*, void*, int*, float*, int*, float*, float, float, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, bool, bool, bool)", "No", 4, 166, 42, 43, 40], ["MLUUnion1ConvForwardGEPB_float_tfloat32_t_float_tfloat32_t_float(void const*, void const*, void const*, void*, int, int, int, int, int, int, int, int, int, float, float, int, int, float, float, void const*, void const*, void const*, void const*, bool, bool)", "No", 2, 153, 76, 77, 76], ["void smallCompute<int>(int*, int*, float, float)", "No", 106, 116, 1, 2, 1], ["void MLUTransposeKernel3DDefaultBlock<int, (Trans3DMode)2>(int const*, int*, DimSplit, DimSplit, DimSplit)", "No", 4, 115, 29, 31, 27], ["void MLUTranspose2DDefaultBlock<int>(int const*, int*, bool, DimSplit, DimSplit)", "No", 14, 110, 8, 14, 4], ["void reformatWeightStrideMLUUnion1<float, unsigned int>(float const*, unsigned int*, int, int, int, int, int, int, int, int, cnnlTensorLayout_t, int, int, float, int, int, void*, void*, void*, cnnlQuantizeRoundMode_t)", "No", 4, 84, 21, 34, 7], ["void reformatWeightNoStrideMLUUnion1<float, unsigned int>(float const*, unsigned int*, int, int, int, int, cnnlTensorLayout_t, int, int, float, int, int, void*, void*, void*, cnnlQuantizeRoundMode_t)", "No", 10, 78, 8, 9, 7], ["void MLUTransposeKernel3DDefaultPipe3Union1<int, (Trans3DMode)2>(int const*, int*, DimSplit, DimSplit, DimSplit)", "No", 4, 68, 17, 18, 16], ["void MLUUnion1Reduce2DPipelineMeanValue<float, float>(void*, void*, void*, void*, unsigned long, unsigned long, unsigned int, unsigned int, unsigned int, bool, float, float, float, cnnlReduceOp_t, cnnlReduceIndices_t, cnnlDataType_t, cnnlIndicesType_t)", "No", 2, 36, 18, 18, 18], ["MLUUnion1gemm4convGdramSetZeroNH(void*, int, int, int, int, int, int, int, int, cnnlDataType_t)", "No", 2, 34, 17, 17, 17], ["void MLUBlockKernelExpandDefault<int, unsigned int>(void*, void*, Shapes<unsigned int>)", "No", 2, 33, 16, 17, 16], ["mluUnion1KernelSoftmaxBackwardFp32(void*, void*, void*, long, long, long, cnnlSoftmaxAlgorithm_t, cnnlSoftmaxMode_t, cnnlDataType_t)", "No", 2, 28, 14, 14, 14], ["void MLUUnionXKernelGepdot<float, float, float, float, float, float>(void*, void*, void*, void*, void*, float, float, int, int, int, int, int, int, int, bool, bool, int, int, float, float)", "No", 2, 27, 14, 14, 13], ["void MLUUnion1KernelGepm<float, float, float, float>(void*, void*, void*, void*, float, float, int, int, int, int, int, int, bool, bool, int, int, int, int, int, int)", "No", 2, 24, 12, 12, 12], ["void MLUAntsKernelGemmU1Ex<float, float, float, float, float, float, false>(void*, void*, void*, void*, int*, float*, int*, float*, float, float, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, bool, bool, bool)", "No", 2, 22, 11, 11, 11], ["void MLUBlockKernelSoftmaxForwardLowTransposeSmallTarget<float, float, (cnnlSoftmaxAlgorithm_t)2>(float*, float*, unsigned long, unsigned int, unsigned int, unsigned int, unsigned int)", "No", 2, 18, 9, 9, 9], ["void MLUTranspose2DSmallBlock<int>(int const*, int*, bool, DimSplit, DimSplit)", "No", 6, 17, 3, 3, 2], ["void MLUUnion1KernelFillHostValue<unsigned int>(void*, unsigned long, unsigned int)", "No", 10, 12, 1, 2, 1], ["void MLUUnion1NlllossForwardVaa<float, int, (deviceMode_t)1>(float*, float*, float*, float*, float*, int*, long, unsigned long, unsigned long, unsigned int, bool)", "No", 2, 10, 5, 5, 5], ["void MLUUnion1Reduce2DPipelineSumValue<float, float>(void*, void*, void*, void*, unsigned long, unsigned long, unsigned int, unsigned int, unsigned int, bool, float, float, float, cnnlReduceOp_t, cnnlReduceIndices_t, cnnlDataType_t, cnnlIndicesType_t)", "No", 2, 9, 4, 5, 4], ["MLUUnion1KernelCopy(void*, void*, unsigned long, int)", "No", 2, 7, 4, 4, 3], ["void MLUUnion1NlllossBackwardVaa<float, int, 2u>(float*, float*, float*, float*, int*, long, unsigned long, unsigned long, unsigned int)", "No", 2, 6, 3, 3, 3], ["void MLUBlockKernelExpand1AtoBA<int, unsigned int>(void*, void*, Shapes<unsigned int>)", "No", 2, 5, 2, 3, 2], ["void MLUBlockNlllossTargetCheck<int>(int*, unsigned long, unsigned long, long)", "No", 2, 3, 2, 2, 1], ["MLUBlockNlllossForwardPostprocess(void*, void*, float*, int, int, int)", "No", 2, 2, 1, 1, 1]]}}
{"total": {"columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["MLUUnionXKernelBatchNormBackward(void*, void*, void*, void*, void*, void*, void*, void*, void*, void*, void*, float, unsigned long, int, int, int, cnnlDataType_t, cnnlActivationMode_t)", 8489], ["MLUUnionXKernelBatchNormForward(void*, void*, void*, void*, void*, void*, void*, void*, void*, void*, float, float, unsigned long, int, int, int, int, int, cnnlDataType_t)", 5907], ["void MLUOpTensorElementParam110ThreePipeline<float, float, float, float>(float const*, float const*, float*, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, cnnlOpTensorDesc_t, bool)", 5190], ["void MLUUnion1KernelGemmRb<float, tfloat32_t, float, tfloat32_t, float, float>(void*, void*, void*, void*, float, float, int, int, int, int, int, int, int, int, int, int, float, float, bool, bool, bool)", 3905], ["void MLUBlockKernel3StagePipelineV2ThresholdBackwardFast<float, float, float, float>(char*, char*, char*, unsigned long, unsigned long, float)", 3762], ["void MLUUnionXKernelGepdot<float, tfloat32_t, float, tfloat32_t, float, float>(void*, void*, void*, void*, void*, float, float, int, int, int, int, int, int, int, bool, bool, int, int, float, float)", 3471], ["void MLUBlockKernel3StagePipelineClipFast<float, float, float, float, OpStyle>(char*, char*, unsigned long, float, float, OpStyle)", 3206], ["void MLUUnion1KernelGepp<float, tfloat32_t, float, float>(void*, void*, void*, void*, float, float, int, int, int, int, int, int, bool, bool, int, int, int, int)", 2739], ["MLUUnionXKernelConvbpfIm2ColIn_float_float(void const*, void const*, void*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, bool)", 2395], ["MLUDeconv3dDefault_true_float_float_float_float_float_float(void*, void const*, void const*, void const*, void*, void const*, void const*, void const*, void const*, void const*, void const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, bool, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, cnnlQuantizeRoundMode_t, bool, int, void const*, float, void const*, int, void const*, int, void const*, float, void const*, int, void const*, int, void const*, float, void const*, int, void const*)", 2263], ["void MLUConvCoCi<0, 0, 0, float, float, float, float, float, float>(char*, char*, char*, char*, char*, char*, char*, char*, char*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, float, int, int, int, cnnlConvolutionCastMode_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, FuseParamHost, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlReorderType_t, cnnlReorderType_t, cnnlQuantizeRoundMode_t)", 2040], ["void MLUOpTensorElementParam1x0<float, float, float, float>(float const*, float const*, float*, float, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, cnnlOpTensorDesc_t)", 1364], ["void MLUBlockKernel3StagePipelineTransformFast<float, float, float, float>(char*, char*, unsigned long, float, float)", 1318], ["void MLUOpTensorElementParam110<float, float, float, float>(float const*, float const*, float*, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, cnnlOpTensorDesc_t, bool)", 1259], ["MLUUnionXKernelConvbpfIm2ColInStride_float_float(void const*, void const*, void*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, bool)", 997], ["co_4_split_entry_CONVBPDATA_NO_STRIDE_F32_TF32_TF32_F32(void const*, void const*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, cnnlDataType_t, int, int, int, int, float, float, float, int, int, int, int, int, int, void*, void*, void*, void*, void*, void*, void*, void*, void*, int, int, cnnlQuantizeRoundMode_t)", 866], ["void MLUAntsKernelGemmU1Ex<float, tfloat32_t, float, tfloat32_t, float, float, false>(void*, void*, void*, void*, int*, float*, int*, float*, float, float, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, bool, bool, bool)", 823], ["void MLUConvCoCiSplit<0, 0, 0, float, float, float, float, float, float>(char const*, char*, char*, char*, char*, char*, char*, char*, char*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, float, int, int, int, cnnlConvolutionCastMode_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, FuseParamHost, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlReorderType_t, cnnlReorderType_t, cnnlQuantizeRoundMode_t)", 792], ["void MLUTransposeKernel3DCSmallBlock<int, (Trans3DMode)2>(int const*, int*, DimSplit, DimSplit, DimSplit)", 683], ["MLUUnionXKernelStrideGepdotIn_float_float_float_float(void const*, void const*, void*, void*, float, float, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, void const*, void const*, void const*, void const*, bool, bool, bool)", 656], ["MLUKernelBlockPoolBackward_opt(void*, void*, void*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, cnnlPoolingMode_t, bool, bool, bool, bool, cnnlDataType_t)", 628], ["nosplit_hw_entry_CONVBPDATA_WITHOUT_STRIDE_F32_F32_F32_F32_F32(void*, void const*, void const*, void const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, nosplitPartitionMode_t, int, float, void const*, void const*, int, float, void const*, void const*, int, float, int, int, int, void const*, void const*, int, int, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, bool, int, int, int, bool, cnnlQuantizeRoundMode_t)", 557], ["void MLUConvCo1<0, 0, 0, float, float, float, float, float, float>(char const*, char*, char*, char*, char*, char*, char*, char*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, float, int, int, int, cnnlConvolutionCastMode_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, FuseParamHost, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlReorderType_t, cnnlReorderType_t, cnnlQuantizeRoundMode_t)", 508], ["co_4_split_entry_CONVBPDATA_STRIDE_F32_TF32_TF32_F32(void const*, void const*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, cnnlDataType_t, int, int, int, int, float, float, float, int, int, int, int, int, int, void*, void*, void*, void*, void*, void*, void*, void*, void*, int, int, cnnlQuantizeRoundMode_t)", 464], ["MLUUnion1KernelGepdotReduce(void*, void*, void*, float, int, int, int, int, cnnlDataType_t, cnnlDataType_t)", 419], ["MLUUnion1PoolingForwardWithIndex(void*, void*, void*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, bool, cnnlDataType_t)", 370], ["MLUUnion1KernelConvbpfReduce_float_float(void*, void*, unsigned long, int)", 333], ["MLUUnion1ConvForwardGeppAnt_float_tfloat32_t_float_float(void const*, void const*, void const*, void*, int, int, int, int, int, int, int, int, int, float, float, int, int, float, float, void const*, void const*, void const*, void const*, bool)", 316], ["void MLUConvImplicitGEMM<0, 0, 0>(char*, char*, char*, char*, char*, char*, char*, char*, char*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, float, int, int, int, cnnlConvolutionCastMode_t, int, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, cnnlDataType_t, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, FuseParamHost, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlTensorLayout_t, cnnlQuantizeRoundMode_t)", 280], ["MLUUnion1ConvBackwardDataGEPB_float_tfloat32_t_float_tfloat32_t_float(void const*, void const*, void*, int, int, int, int, int, int, int, int, int, float, float, int, int, float, float, void const*, void const*, void const*, void const*, bool, bool)", 203], ["MLUUnion1ConvBackwardDataGeppAnt_float_tfloat32_t_float_tfloat32_t_float(void const*, void const*, void*, int, int, int, int, int, int, int, int, int, float, float, int, int, float, float, void const*, void const*, void const*, void const*, bool)", 175], ["void MLUAntsKernelGemmU1Ex<float, tfloat32_t, float, tfloat32_t, float, float, true>(void*, void*, void*, void*, int*, float*, int*, float*, float, float, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, bool, bool, bool)", 166], ["MLUUnion1ConvForwardGEPB_float_tfloat32_t_float_tfloat32_t_float(void const*, void const*, void const*, void*, int, int, int, int, int, int, int, int, int, float, float, int, int, float, float, void const*, void const*, void const*, void const*, bool, bool)", 153], ["void smallCompute<int>(int*, int*, float, float)", 116], ["void MLUTransposeKernel3DDefaultBlock<int, (Trans3DMode)2>(int const*, int*, DimSplit, DimSplit, DimSplit)", 115], ["void MLUTranspose2DDefaultBlock<int>(int const*, int*, bool, DimSplit, DimSplit)", 110], ["void reformatWeightStrideMLUUnion1<float, unsigned int>(float const*, unsigned int*, int, int, int, int, int, int, int, int, cnnlTensorLayout_t, int, int, float, int, int, void*, void*, void*, cnnlQuantizeRoundMode_t)", 84], ["void reformatWeightNoStrideMLUUnion1<float, unsigned int>(float const*, unsigned int*, int, int, int, int, cnnlTensorLayout_t, int, int, float, int, int, void*, void*, void*, cnnlQuantizeRoundMode_t)", 78], ["void MLUTransposeKernel3DDefaultPipe3Union1<int, (Trans3DMode)2>(int const*, int*, DimSplit, DimSplit, DimSplit)", 68], ["void MLUUnion1Reduce2DPipelineMeanValue<float, float>(void*, void*, void*, void*, unsigned long, unsigned long, unsigned int, unsigned int, unsigned int, bool, float, float, float, cnnlReduceOp_t, cnnlReduceIndices_t, cnnlDataType_t, cnnlIndicesType_t)", 36], ["MLUUnion1gemm4convGdramSetZeroNH(void*, int, int, int, int, int, int, int, int, cnnlDataType_t)", 34], ["void MLUBlockKernelExpandDefault<int, unsigned int>(void*, void*, Shapes<unsigned int>)", 33], ["mluUnion1KernelSoftmaxBackwardFp32(void*, void*, void*, long, long, long, cnnlSoftmaxAlgorithm_t, cnnlSoftmaxMode_t, cnnlDataType_t)", 28], ["void MLUUnionXKernelGepdot<float, float, float, float, float, float>(void*, void*, void*, void*, void*, float, float, int, int, int, int, int, int, int, bool, bool, int, int, float, float)", 27], ["void MLUUnion1KernelGepm<float, float, float, float>(void*, void*, void*, void*, float, float, int, int, int, int, int, int, bool, bool, int, int, int, int, int, int)", 24], ["void MLUAntsKernelGemmU1Ex<float, float, float, float, float, float, false>(void*, void*, void*, void*, int*, float*, int*, float*, float, float, int, int, int, int, int, int, int, int, int, int, int, int, int, float, float, bool, bool, bool)", 22], ["void MLUBlockKernelSoftmaxForwardLowTransposeSmallTarget<float, float, (cnnlSoftmaxAlgorithm_t)2>(float*, float*, unsigned long, unsigned int, unsigned int, unsigned int, unsigned int)", 18], ["void MLUTranspose2DSmallBlock<int>(int const*, int*, bool, DimSplit, DimSplit)", 17], ["void MLUUnion1KernelFillHostValue<unsigned int>(void*, unsigned long, unsigned int)", 12], ["void MLUUnion1NlllossForwardVaa<float, int, (deviceMode_t)1>(float*, float*, float*, float*, float*, int*, long, unsigned long, unsigned long, unsigned int, bool)", 10], ["void MLUUnion1Reduce2DPipelineSumValue<float, float>(void*, void*, void*, void*, unsigned long, unsigned long, unsigned int, unsigned int, unsigned int, bool, float, float, float, cnnlReduceOp_t, cnnlReduceIndices_t, cnnlDataType_t, cnnlIndicesType_t)", 9], ["MLUUnion1KernelCopy(void*, void*, unsigned long, int)", 7], ["void MLUUnion1NlllossBackwardVaa<float, int, 2u>(float*, float*, float*, float*, int*, long, unsigned long, unsigned long, unsigned int)", 6], ["void MLUBlockKernelExpand1AtoBA<int, unsigned int>(void*, void*, Shapes<unsigned int>)", 5], ["void MLUBlockNlllossTargetCheck<int>(int*, unsigned long, unsigned long, long)", 3], ["MLUBlockNlllossForwardPostprocess(void*, void*, float*, int, int, int)", 2]]}}
